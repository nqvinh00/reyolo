{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reyolo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GD9M5U4ZMDn",
        "outputId": "bd8c478a-7322-49ae-a164-ae85e03092d8"
      },
      "source": [
        "pip install pytorch-lightning"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.7/dist-packages (1.4.9)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2021.10.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu111)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: torchmetrics>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.41.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OydlIuq8WD4I"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import cv2 \n",
        "import random"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJUwz7yebSo"
      },
      "source": [
        "class EmptyLayer(pl.LightningModule):\n",
        "  \"\"\"\n",
        "  Use for route module\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(EmptyLayer, self).__init__()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEHTy5XagBNO"
      },
      "source": [
        "class DetectionLayer(pl.LightningModule):\n",
        "  \"\"\"\n",
        "  Use for yolo module\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, anchors):\n",
        "    super(DetectionLayer, self).__init__()\n",
        "    self.anchors = anchors"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyz8LNnPK17e"
      },
      "source": [
        "class Darknet(pl.LightningModule):\n",
        "  def __init__(self, cfg_file):\n",
        "    super(Darknet, self).__init__()\n",
        "    self.blocks = parse_cfg(cfg_file)\n",
        "    self.net, self.module_list = create_modules(self.blocks)\n",
        "\n",
        "  def forward(self, x, CUDA):\n",
        "    \"\"\"\n",
        "    Calculate the output\n",
        "    Transform the output detection feature maps in a vay can be processed easier\n",
        "    \"\"\"\n",
        "    modules = self.blocks[1:] # skip first element of blocks, which is net info\n",
        "    outputs = {}\n",
        "    check = 0\n",
        "\n",
        "    for i, module in enumerate(modules):        \n",
        "      module_type = (module[\"type\"])\n",
        "      \n",
        "      if module_type == \"convolutional\" or module_type == \"upsample\" or module_type==\"maxpool\":\n",
        "        x = self.module_list[i](x)\n",
        "      elif module_type == \"route\":\n",
        "        layers = module[\"layers\"]\n",
        "        layers = [int(a) for a in layers]\n",
        "\n",
        "        if layers[0] > 0:\n",
        "            layers[0] -= i\n",
        "\n",
        "        if len(layers) == 1:\n",
        "            x = outputs[i + layers[0]]\n",
        "        else:\n",
        "            if layers[1] > 0:\n",
        "                layers[1] -= i\n",
        "\n",
        "            map1 = outputs[i + layers[0]]\n",
        "            map2 = outputs[i + layers[1]]\n",
        "            x = torch.cat((map1, map2), 1)\n",
        "      elif  module_type == \"shortcut\":\n",
        "          f = int(module[\"from\"])\n",
        "          x = outputs[i - 1] + outputs[i + f]\n",
        "      elif module_type == 'yolo':        \n",
        "          anchors = self.module_list[i][0].anchors   # anchors\n",
        "          input_dim = int (self.net[\"height\"])       # input dimension\n",
        "          num_classes = int (module[\"classes\"])      # number of classes\n",
        "  \n",
        "          # transform \n",
        "          x = x.data\n",
        "          x = predict_transform(x, input_dim, anchors, num_classes, CUDA)\n",
        "          if not check:\n",
        "              detections = x\n",
        "              check = 1\n",
        "          else:       \n",
        "              detections = torch.cat((detections, x), 1)\n",
        "  \n",
        "      outputs[i] = x\n",
        "    \n",
        "    return detections\n",
        "\n",
        "  def load_weight(self, file_path):\n",
        "    file = open(file_path, \"rb\")\n",
        "\n",
        "    # first 5 items in weight file are header information\n",
        "    # major ver, minor ver, subversion, images seen by the network\n",
        "    header = np.fromfile(file, dtype=np.int32, count=5)\n",
        "    self.header = torch.from_numpy(header)\n",
        "    self.network_seen = self.header[3]\n",
        "    weights = np.fromfile(file, dtype=np.float32)\n",
        "\n",
        "    n = 0\n",
        "    for i in range(len(self.module_list)):\n",
        "      module_type = self.blocks[i + 1][\"type\"]\n",
        "      # if not convolutional, ignore\n",
        "      if module_type == \"convolutional\":\n",
        "        module = self.module_list[i]\n",
        "        try:\n",
        "          batch_normalize = int(self.blocks[i + 1][\"batch_normalize\"])\n",
        "        except:\n",
        "          batch_normalize = 0\n",
        "        \n",
        "        convol_layer = module[0]\n",
        "\n",
        "        # batch normalize layer\n",
        "        if batch_normalize:\n",
        "          batch_norm_layer = module[1]\n",
        "          num_biases = batch_norm_layer.bias.numel()\n",
        "          \n",
        "          # load weights\n",
        "          bnl_biases = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          bnl_weights = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          bnl_running_mean = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          bnl_running_var = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          # cast weights into dimensions of model weights\n",
        "          bnl_biases = bnl_biases.view_as(batch_norm_layer.bias.data)\n",
        "          bnl_weights = bnl_weights.view_as(batch_norm_layer.weight.data)\n",
        "          bnl_running_mean = bnl_running_mean.view_as(batch_norm_layer.running_mean)\n",
        "          bnl_running_var = bnl_running_var.view_as(batch_norm_layer.running_var)\n",
        "\n",
        "          # copy data to model\n",
        "          batch_norm_layer.bias.data.copy_(bnl_biases)\n",
        "          batch_norm_layer.weight.data.copy_(bnl_weights)\n",
        "          batch_norm_layer.running_mean.copy_(bnl_running_mean)\n",
        "          batch_norm_layer.running_var.copy_(bnl_running_var)\n",
        "        else:     # convolutional layer\n",
        "          num_biases = convol_layer.bias.numel()\n",
        "\n",
        "          # load weights\n",
        "          convol_biases = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          # cast weights into dimensions of model weights\n",
        "          convol_biases = convol_biases.view_as(convol_layer.bias.data)\n",
        "\n",
        "          # copy data to model\n",
        "          convol_layer.bias.data.copy_(convol_biases)\n",
        "        \n",
        "        # weights of convolutional layerss\n",
        "        num_weights = convol_layer.weight.numel()\n",
        "        convol_weights = torch.from_numpy(weights[n: n + num_weights])\n",
        "        n += num_weights\n",
        "        convol_weights = convol_weights.view_as(convol_layer.weight.data)\n",
        "        convol_layer.weight.data.copy_(convol_weights)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCFJA6nVoHN5"
      },
      "source": [
        "def predict_transform(predict, input_dim, anchors, num_classes, CUDA = True):\n",
        "  \"\"\"\n",
        "  Transfer input (which is output of forward()) into 2d tensor.\n",
        "  Each row of the tensor corresponds to attributes of a bounding box.\n",
        "  \"\"\"\n",
        "\n",
        "  batch_size = predict.size(0)\n",
        "  stride = input_dim // predict.size(2)\n",
        "  grid_size = input_dim // stride\n",
        "  bounding_box_attrs = num_classes + 5\n",
        "\n",
        "  predict = predict.view(batch_size, bounding_box_attrs * len(anchors), grid_size ** 2)\n",
        "  predict = predict.transpose(1,2).contiguous()\n",
        "  predict = predict.view(batch_size, grid_size ** 2 * len(anchors), bounding_box_attrs)\n",
        "\n",
        "  # dimensions of anchors are in accordance to height and width attr of net block\n",
        "  anchors = [(a[0] / stride, a[1] / stride) for a in anchors]\n",
        "\n",
        "  # sigmoid x, y coordinates and objectness score\n",
        "  # center_x, center_y, object_confidence\n",
        "  predict[:, :, 0] = torch.sigmoid(predict[:, :, 0])\n",
        "  predict[:, :, 1] = torch.sigmoid(predict[:, :, 1])\n",
        "  predict[:, :, 4] = torch.sigmoid(predict[:, :, 4])\n",
        "\n",
        "  # add center offsets\n",
        "  grid = np.arange(grid_size)\n",
        "  x, y = np.meshgrid(grid, grid)\n",
        "  x_offset = torch.FloatTensor(x).view(-1, 1)\n",
        "  y_offset = torch.FloatTensor(y).view(-1, 1)\n",
        "\n",
        "\n",
        "  if CUDA:\n",
        "    x_offset = x_offset.cuda()\n",
        "    y_offset = y_offset.cuda()\n",
        "    anchors = anchors.cuda()\n",
        "  \n",
        "  xy_offset = torch.cat((x_offset, y_offset), 1).repeat(1, len(anchors)).view(-1, 2).unsqueeze(0)\n",
        "  predict[:, :, :2] += xy_offset\n",
        "\n",
        "  # apply anchors to dimensions of bounding box\n",
        "  anchors = torch.FloatTensor(anchors)\n",
        "  if CUDA:\n",
        "    anchors = anchors.cuda()\n",
        "\n",
        "  anchors = anchors.repeat(grid_size ** 2, 1).unsqueeze(0)\n",
        "\n",
        "  predict[:, :, 2: 4] = torch.exp(predict[:, :, 2: 4]) * anchors\n",
        "  # apply sigmoid to class scores\n",
        "  predict[:, :, 5: num_classes + 5] = torch.sigmoid(predict[:, :, 5: num_classes + 5])\n",
        "  # resize detections map to size of input image\n",
        "  predict[:, :, :4] *= stride\n",
        "\n",
        "  return predict"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLuHRI-uWlx9"
      },
      "source": [
        "def parse_cfg(file):\n",
        "  \"\"\"\n",
        "  Parse config from file. Returns a list of blocks.\n",
        "  Each blocks describes a block in neural network to be built.\n",
        "  \"\"\"\n",
        "\n",
        "  file = open(file, 'r')\n",
        "  lines = file.read().split('\\n')\n",
        "  lines = [l for l in lines if len(l) > 0]\n",
        "  lines = [l for l in lines if l[0] != '#']\n",
        "  lines = [l.rstrip().lstrip() for l in lines]\n",
        "\n",
        "  b = {}\n",
        "  blocks = []\n",
        "\n",
        "  for l in lines:\n",
        "    if l[0] == \"[\":                 # Check for new block\n",
        "      if len(b) != 0:               # Check if block not empty\n",
        "        blocks.append(b)\n",
        "        b = {}\n",
        "      b[\"type\"] = l[1:-1].rstrip()\n",
        "    else:\n",
        "      key, value = l.split(\"=\")     # get key-value from line\n",
        "      b[key.rstrip()] = value.lstrip()\n",
        "\n",
        "  blocks.append(b)\n",
        "  return blocks"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3FOSgxGWwJb"
      },
      "source": [
        "def create_modules(blocks):\n",
        "  net = blocks[0]                  # net info about the input and pre-processing\n",
        "  modules = nn.ModuleList()\n",
        "  in_channels = 3\n",
        "  output_filters = []\n",
        "\n",
        "  for i, x in enumerate(blocks[1:]):\n",
        "    module = nn.Sequential()\n",
        "    module_type = x[\"type\"]\n",
        "\n",
        "    # check type of block\n",
        "    # create new module for block\n",
        "    # append to module list (modules variable)\n",
        "    if module_type == \"convolutional\":\n",
        "      activation = x[\"activation\"]\n",
        "      try:\n",
        "          batch_normalize = int(x[\"batch_normalize\"])\n",
        "          bias = False\n",
        "      except:\n",
        "          batch_normalize = 0\n",
        "          bias = True\n",
        "      \n",
        "      filters = int(x[\"filters\"])\n",
        "      padding = int(x[\"pad\"])\n",
        "      kernel_size = int(x[\"size\"])\n",
        "      stride = int(x[\"stride\"])\n",
        "\n",
        "      if padding:\n",
        "        pad = (kernel_size - 1) // 2\n",
        "      else:\n",
        "        pad = 0\n",
        "      \n",
        "      # convolutional layer\n",
        "      convol_layer = nn.Conv2d(\n",
        "          in_channels=in_channels,\n",
        "          out_channels=filters,\n",
        "          kernel_size=kernel_size,\n",
        "          stride=stride,\n",
        "          padding=pad,\n",
        "          bias=bias\n",
        "      )\n",
        "      module.add_module(\"conv_{}\".format(i), convol_layer)\n",
        "\n",
        "      # batch norm layer\n",
        "      if batch_normalize:\n",
        "          batch_norm_layer = nn.BatchNorm2d(num_features=filters)\n",
        "          module.add_module(\"batch_norm_{}\".format(i), batch_norm_layer)\n",
        "      \n",
        "      if activation == \"leaky\":      # linear or leaky relu for yolo\n",
        "          leaky_layer = nn.LeakyReLU(0.1, inplace=True)\n",
        "          module.add_module(\"leaky_{}\".format(i), leaky_layer)\n",
        "    # maxpool layers\n",
        "    elif module_type == \"maxpool\":\n",
        "      kernel_size = int(x[\"size\"])\n",
        "      stride = int(x[\"stride\"])\n",
        "\n",
        "      maxpool = nn.MaxPool2d(\n",
        "          kernel_size=kernel_size,\n",
        "          stride=stride,\n",
        "          padding=int((kernel_size - 1) // 2)\n",
        "      )\n",
        "\n",
        "      if kernel_size == 2 and stride == 1:\n",
        "        module.add_module('ZeroPad2d',nn.ZeroPad2d((0, 1, 0, 1)))\n",
        "        module.add_module('MaxPool2d',maxpool)\n",
        "      else:\n",
        "        module = maxpool\n",
        "    # unsample layers\n",
        "    elif module_type == \"upsample\":\n",
        "      stride = int(x[\"stride\"])\n",
        "      upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
        "      module.add_module(\"upsample_{}\".format(i), upsample)\n",
        "    # route layer\n",
        "    elif module_type == \"route\":\n",
        "      x[\"layers\"] = x[\"layers\"].split(\",\")\n",
        "      start = int(x[\"layers\"][0])\n",
        "      try:\n",
        "        end = int(x[\"layers\"][1])\n",
        "      except:\n",
        "        end = 0\n",
        "      \n",
        "      if start > 0:\n",
        "        start -= i\n",
        "      if end > 0:\n",
        "        end -= i\n",
        "\n",
        "      route = EmptyLayer()\n",
        "      module.add_module(\"route_{}\".format(i), route)\n",
        "      if end < 0:\n",
        "        filters = output_filters[i + start] + output_filters[i + end]\n",
        "      else:\n",
        "        filters = output_filters[i + start]\n",
        "    # shortcut\n",
        "    elif module_type == \"shortcut\":\n",
        "      shortcut = EmptyLayer()\n",
        "      module.add_module(\"shortcut_{}\".format(i), shortcut)\n",
        "    # yolo: detection layer\n",
        "    elif module_type == \"yolo\":\n",
        "      mask = x[\"mask\"].split(\",\")\n",
        "      mask = [int(m) for m in mask]\n",
        "\n",
        "      anchors = x[\"anchors\"].split(\",\")\n",
        "      anchors = [int(a) for a in anchors]\n",
        "      anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
        "      anchors = [anchors[m] for m in mask]\n",
        "\n",
        "      detection = DetectionLayer(anchors)\n",
        "      module.add_module(\"Detection_{}\".format(i), detection)\n",
        "\n",
        "    modules.append(module)\n",
        "    in_channels = filters\n",
        "    output_filters.append(filters)\n",
        "  return (net, modules)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F7cDfhlY7mw"
      },
      "source": [
        "def test_input(file_path, img_size):\n",
        "    img = cv2.imread(file_path)\n",
        "    img = cv2.resize(img, img_size)\n",
        "    img_result = img[:, :, ::-1].transpose((2, 0, 1))     # BGR -> RGB\n",
        "    img_result = img_result[np.newaxis, :, :, :]/255.0    # Add a channel at 0\n",
        "    img_result = torch.from_numpy(img_result).float()     # Convert to float\n",
        "    img_result = Variable(img_result)                     # Convert to Variable\n",
        "    return img_result"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3IXXibifjgQ"
      },
      "source": [
        "def get_result(prediction, confidence, num_classes, nms_conf=0.4):\n",
        "  # object confidence thresholding\n",
        "  # each bounding box having objectness score below a threshold\n",
        "  # set the value of entrie row representing the bounding box to zero\n",
        "  conf_mask = (prediction[:, :, 4] > confidence).float().unsqueeze(2)\n",
        "  prediction *= conf_mask\n",
        "\n",
        "  # transform center_x, center_y, height, width of box\n",
        "  # to top_left_corner_x, top_right_corner_y, right_bottom_corner_x, right_bottom_corner_y \n",
        "  box = prediction.new(prediction.shape)\n",
        "  box[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
        "  box[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
        "  box[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
        "  box[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
        "  prediction[:, :, :4] = box[:, :, :4]\n",
        "\n",
        "  batch_size = prediction.size(0)\n",
        "  check = False\n",
        "\n",
        "  # the number of true detections in every image may be different\n",
        "  # confidence thresholding and nms has to be done for one image at conce\n",
        "  # must loop over the 1st dimension of prediction\n",
        "  for i in range(batch_size):\n",
        "    image_prediction = prediction[i]      # image tensor\n",
        "\n",
        "    # each bounding box has 85 attri\n",
        "    # 80 attri are class scores\n",
        "    max_confidence, max_confidence_score = torch.max(image_prediction[:, 5: num_classes + 5], 1)\n",
        "    max_confidence = max_confidence.float().unsqueeze(1)\n",
        "    max_confidence_score = max_confidence_score.float().unsqueeze(1)\n",
        "    image_prediction = torch.cat((image_prediction[:, :5], max_confidence, max_confidence_score), 1)\n",
        "\n",
        "    non_zero = torch.nonzero(image_prediction[:, 4])\n",
        "    try:\n",
        "      image_prediction_ = image_prediction[non_zero.squeeze(), :].view(-1, 7)\n",
        "    except:\n",
        "      continue\n",
        "    \n",
        "    if image_prediction_.shape[0] == 0:\n",
        "      continue\n",
        "    \n",
        "    # get various classes detected in image\n",
        "    image_classes = get_unique(image_prediction_[:, -1])\n",
        "\n",
        "    for c in image_classes:\n",
        "      # nms\n",
        "      # get detections with 1 particular class\n",
        "      class_mask = image_prediction_ * (image_prediction_[:, -1] == c).float().unsqueeze(1)\n",
        "      class_mask_index = torch.nonzero(class_mask[:, -2]).squeeze()\n",
        "      image_prediction_class = image_prediction_[class_mask_index].view(-1, 7)\n",
        "\n",
        "      # sort detection\n",
        "      # confidence at top\n",
        "      confidence_sorted_index = torch.sort(image_prediction_class[:, 4], descending=True)[1]\n",
        "      image_prediction_class = image_prediction_class[confidence_sorted_index]\n",
        "      index = image_prediction_class.size(0)\n",
        "\n",
        "      for idx in range(index):\n",
        "        # get ious of all boxes\n",
        "        try:\n",
        "          ious = get_bounding_boxes_iou(image_prediction_class[idx].unsqueeze(0), image_prediction_class[idx + 1:])\n",
        "        except ValueError:\n",
        "          break\n",
        "        except IndexError:\n",
        "          break\n",
        "        \n",
        "        # mark zero all detections iou > threshold\n",
        "        iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
        "        image_prediction_class[idx + 1:] *= iou_mask\n",
        "\n",
        "        # remove non-zero entries\n",
        "        non_zero_index = torch.nonzero(image_prediction_class[:, 4]).squeeze()\n",
        "        image_prediction_class = image_prediction_class[non_zero_index].view(-1, 7)\n",
        "      \n",
        "      batch_index = image_prediction_class.new(image_prediction_class.size(0), 1).fill_(i)\n",
        "      s = batch_index, image_prediction_class\n",
        "\n",
        "      if not check:\n",
        "        output = torch.cat(s, 1)\n",
        "        check = True\n",
        "      else:\n",
        "        output = torch.cat((output, torch.cat(s, 1)))\n",
        "      \n",
        "  try:\n",
        "    return output\n",
        "  except:\n",
        "    return 0"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvMj1fcDmys2"
      },
      "source": [
        "def get_unique(tensor):\n",
        "  np_tensor = tensor.cpu().numpy()\n",
        "  unique = np.unique(np_tensor)\n",
        "  unique_tensor = torch.from_numpy(unique)\n",
        "  result = tensor.new(unique_tensor.shape)\n",
        "  result.copy_(unique_tensor)\n",
        "\n",
        "  return result"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L1EL5yfqAQt"
      },
      "source": [
        "def get_bounding_boxes_iou(b1, b2):\n",
        "  \"\"\"\n",
        "  Returns iou of 2 bouding boxes\n",
        "  \"\"\"\n",
        "\n",
        "  # get coordinates of 2 bounding boxes\n",
        "  b1_x1, b1_y1, b1_x2, b1_y2 = b1[:, 0], b1[:, 1], b1[:, 2], b1[:, 3]\n",
        "  b2_x1, b2_y1, b2_x2, b2_y2 = b2[:, 0], b2[:, 1], b2[:, 2], b2[:, 3]\n",
        "\n",
        "  # get coordinates of overclap rectangle\n",
        "  x1 = torch.max(b1_x1, b2_x1)\n",
        "  y1 = torch.max(b1_y1, b2_y1)\n",
        "  x2 = torch.min(b1_x2, b2_x2)\n",
        "  y2 = torch.min(b1_y2, b2_y2)\n",
        "\n",
        "  # overclap area\n",
        "  area = torch.clamp(x2 - x1 + 1, min=0) * torch.clamp(y2 - y1 + 1, min=0)\n",
        "\n",
        "  # union area\n",
        "  b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
        "  b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
        "\n",
        "  return area / (b1_area + b2_area - area)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buiHVHnpUPZY"
      },
      "source": [
        "def resize_image(img, input_dim):\n",
        "    \"\"\"\n",
        "    resize image with unchanged aspect ratio using padding\n",
        "    \"\"\"\n",
        "    width, height = img.shape[1], img.shape[0]\n",
        "    w, h = input_dim\n",
        "    new_width = int(width * min(w / width, h / height))\n",
        "    new_height = int(height * min(w / width, h / height))\n",
        "    resized_image = cv2.resize(img, (new_width, new_height), interpolation = cv2.INTER_CUBIC)\n",
        "    \n",
        "    canvas = np.full((input_dim[1], input_dim[0], 3), 128)\n",
        "    canvas[(h - new_height) // 2: (h - new_height) // 2 + new_height,(w - new_width) // 2: (w - new_width) // 2 + new_width,  :] = resized_image\n",
        "    return canvas"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiJ5sHZgpfvt"
      },
      "source": [
        "def pre_image(img, input_dim):\n",
        "  \"\"\"\n",
        "  Prepare image as input for neural network\n",
        "  \"\"\"\n",
        "\n",
        "  img = resize_image(img, (input_dim, input_dim))\n",
        "  img = img[:, :, ::-1].transpose((2, 0, 1)).copy()\n",
        "  img = torch.from_numpy(img).float().div(255.0).unsqueeze(0)\n",
        "  return img"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTGiD8L61HxB"
      },
      "source": [
        "def draw_result(x, results, colors, classes):\n",
        "  t1 = tuple(x[1: 3].int())\n",
        "  t2 = tuple(x[3: 5].int())\n",
        "  img = results[int(x[0])]\n",
        "  text_font = cv2.FONT_HERSHEY_PLAIN\n",
        "  cls = int(x[-1])\n",
        "  color = random.choice(colors)\n",
        "  label = \"{}\".format(classes[cls])\n",
        "  cv2.rectangle(img, t1, t2, color, 1)\n",
        "  text_size = cv2.getTextSize(label, text_font, 1, 1)[0]\n",
        "  t2 = t1[0] + text_size[0] + 3, t1[1] + text_size[1] + 4\n",
        "  cv2.rectangle(img, t1, t2, color, -1)\n",
        "  text_pos = t1[0], t1[1] + text_size[1] + 4\n",
        "  cv2.putText(img, label, text_pos, text_font, 1, [255, 255, 255], 1)\n",
        "  return img"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8IjIIknF0i2"
      },
      "source": [
        "def load_dataset(file_path):\n",
        "  file = open(file_path, \"r\")\n",
        "  names = file.read().split(\"\\n\")[:-1]\n",
        "  return names"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsja-s4xSf7j",
        "outputId": "ded91246-2790-46f7-dc9c-2509e6cfe4eb"
      },
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import cv2 \n",
        "import argparse\n",
        "import os \n",
        "import os.path as osp\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def parse_arg():\n",
        "  \"\"\"\n",
        "  Parse arguments to detect module\n",
        "  \"\"\"\n",
        "\n",
        "  parser = argparse.ArgumentParser(description=\"reYOLO Detection Module\")\n",
        "  parser.add_argument(\"--images\", dest=\"images\", default=\"/content/dog-cycle-car.png\", type=str, help=\"Image path or directory containing images to perform detection\")\n",
        "  parser.add_argument(\"--det\", dest=\"det\", default=\"det\", type=str, help=\"Imgage path or directory to store detections\")\n",
        "  parser.add_argument(\"--bs\", dest=\"bs\", default=1, help=\"Batch size\")\n",
        "  parser.add_argument(\"--confidence\", dest=\"confidence\", default=0.5, help=\"Object confidence to filter predictions\")\n",
        "  parser.add_argument(\"--nms\", dest=\"nms\", default=0.4, help=\"NMS Threshold\")\n",
        "  parser.add_argument(\"--cfg\", dest=\"cfg_file\", default=\"/content/yolov3.cfg\", type=str, help=\"Config file path\")\n",
        "  parser.add_argument(\"--weights\", dest=\"weights_file\", default=\"/content/yolov3.weights\", type=str, help=\"Weights file path\")\n",
        "  parser.add_argument(\"--dataset\", dest=\"dataset\", default=\"/content/coco.names\", type=str, help=\"Dataset file path\")\n",
        "  parser.add_argument(\"--colors\", dest=\"colors_file\", default=\"/content/pallete\", type=str, help=\"Colors file path\")\n",
        "\n",
        "  args, unknown = parser.parse_known_args()\n",
        "  return args\n",
        "\n",
        "class ImageDetect():\n",
        "  def __init__(self):\n",
        "    args = parse_arg()\n",
        "    self.images = args.images\n",
        "    self.cfg_file = args.cfg_file\n",
        "    self.weights_file = args.weights_file\n",
        "    self.det = args.det\n",
        "    self.batch_size = int(args.bs)\n",
        "    self.confidence = float(args.confidence)\n",
        "    self.nms = float(args.nms)\n",
        "    self.CUDA = torch.cuda.is_available()\n",
        "    self.classes = load_dataset(args.dataset)\n",
        "    self.num_classes = len(self.classes)\n",
        "    self.colors_file = args.colors_file\n",
        "  \n",
        "  def load_network(self):\n",
        "    \"\"\"\n",
        "    Setup neural network\n",
        "    \"\"\"\n",
        "    self.model = Darknet(self.cfg_file)\n",
        "    self.model.load_weight(self.weights_file)\n",
        "    self.input_dim = int(self.model.net[\"height\"])\n",
        "    assert self.input_dim % 32 == 0\n",
        "    assert self.input_dim > 32\n",
        "  \n",
        "  def get_detections(self):\n",
        "    self.load_network()\n",
        "    if self.CUDA:         # if cuda available\n",
        "      self.model.cuda()\n",
        "    \n",
        "    self.model.eval()       # set model in evaluation mode\n",
        "    read_time = time.time()\n",
        "\n",
        "    try:\n",
        "      image_list = [osp.join(osp.realpath(\".\"), self.images, img) for img in os.listdir(self.images)]\n",
        "    except NotADirectoryError:\n",
        "      image_list = []\n",
        "      image_list.append(osp.join(osp.realpath(\".\"), self.images))\n",
        "    except FileNotFoundError:\n",
        "      print(\"No file or directory with name {}\".format(self.images))\n",
        "      exit()\n",
        "\n",
        "    if not os.path.exists(self.det):\n",
        "      os.makedirs(self.det)\n",
        "\n",
        "    load_batch_time = time.time()\n",
        "    loaded_img_list = [cv2.imread(x) for x in image_list]\n",
        "    # pytorch variables for images\n",
        "    img_batches = list(map(pre_image, loaded_img_list, [self.input_dim for i in range(len(image_list))]))\n",
        "    # dimensions of original images\n",
        "    img_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_img_list]\n",
        "    img_dim_list = torch.FloatTensor(img_dim_list).repeat(1, 2)\n",
        "\n",
        "    # create batches\n",
        "    left_over = 0\n",
        "    if len(img_dim_list) % self.batch_size:\n",
        "      left_over = 1\n",
        "    \n",
        "    if self.batch_size != 1:\n",
        "      num_batches = len(image_list) // self.batch_size + left_over\n",
        "      img_batches = [torch.car((img_batches[i * self.batch_size: min((i + 1) * self.batch_size, len(img_batches))])) for i in range(num_batches)]\n",
        "    \n",
        "    check = 0\n",
        "    if self.CUDA:\n",
        "      img_dim_list = img_dim_list.cuda()\n",
        "\n",
        "    start_detect_loop_time = time.time()\n",
        "\n",
        "    # detection loop\n",
        "    for i, batch in enumerate(img_batches):\n",
        "      start = time.time()\n",
        "      if self.CUDA:\n",
        "        batch = batch.cuda()\n",
        "      with torch.no_grad():\n",
        "        prediction = self.model(Variable(batch), self.CUDA)\n",
        "      \n",
        "      prediction = get_result(prediction, self.confidence, self.num_classes, nms_conf=self.nms)\n",
        "\n",
        "      end = time.time()\n",
        "      if type(prediction) == int:\n",
        "        for img_num, image in enumerate(image_list[i * self.batch_size: min((i + 1) * self.batch_size, len(image_list))]):\n",
        "          img_id = i * self.batch_size + img_num\n",
        "          print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start) / self.batch_size))\n",
        "          print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
        "          print(\"*********************************************\")\n",
        "        continue\n",
        "      \n",
        "      # transform attr from index in batch to index in image list\n",
        "      prediction[:, 0] += i * self.batch_size\n",
        "      if not check:           # initialize output\n",
        "        output = prediction\n",
        "        check = 1\n",
        "      else:\n",
        "        output = torch.cat((output, prediction))\n",
        "      \n",
        "      for img_num, image in enumerate(image_list[i * self.batch_size: min((i + 1) * self.batch_size, len(image_list))]):\n",
        "          img_id = i * self.batch_size + img_num\n",
        "          objects = [self.classes[int(x[-1])] for x in output if int(x[0]) == img_id]\n",
        "          print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start) / self.batch_size))\n",
        "          print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objects)))\n",
        "          print(\"*********************************************\")\n",
        "      \n",
        "      if self.CUDA:\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # draw bouding boxes on images\n",
        "    try:\n",
        "      output\n",
        "    except NameError:\n",
        "      print(\"No detection were made\")\n",
        "      exit()\n",
        "    \n",
        "    img_dim_list = torch.index_select(img_dim_list, 0, output[:, 0].long())\n",
        "    scale_factor = torch.min(self.input_dim / img_dim_list, 1)[0].view(-1, 1)\n",
        "    output[:, [1, 3]] -= (self.input_dim - scale_factor * img_dim_list[:, 0].view(-1, 1)) / 2\n",
        "    output[:, [2, 4]] -= (self.input_dim - scale_factor * img_dim_list[:, 1].view(-1, 1)) / 2\n",
        "    output[:, 1:5] /= scale_factor\n",
        "\n",
        "    for i in range(output.shape[0]):\n",
        "      output[i, [1, 3]] = torch.clamp(output[i, [1, 3]], 0.0, img_dim_list[i, 0])\n",
        "      output[i, [2, 4]] = torch.clamp(output[i, [2, 4]], 0.0, img_dim_list[i, 1])\n",
        "    \n",
        "    output_recast_time = time.time()\n",
        "    class_load_time = time.time()\n",
        "    colors = pkl.load(open(self.colors_file, \"rb\"))\n",
        "    draw_time = time.time()\n",
        "\n",
        "    list(map(lambda x: draw_result(x, loaded_img_list, colors, self.classes), output))\n",
        "    detect_names = pd.Series(image_list).apply(lambda x: \"{}/detect_{}\".format(self.det, x.split(\"/\")[-1]))\n",
        "    list(map(cv2.imwrite, detect_names, loaded_img_list))\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Results\")\n",
        "    print(\"*********************************************\")\n",
        "    print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Reading\", load_batch_time - read_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_detect_loop_time - load_batch_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(image_list)) +  \" images)\", output_recast_time - start_detect_loop_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Output processing\", class_load_time - output_recast_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Drawing boxes\", end - draw_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Average time per img\", (end - load_batch_time) / len(image_list)))\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "test = ImageDetect()\n",
        "test.get_detections()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog-cycle-car.png    predicted in  2.362 seconds\n",
            "Objects Detected:    bicycle truck dog\n",
            "*********************************************\n",
            "Results\n",
            "*********************************************\n",
            "Task                     : Time Taken (in seconds)\n",
            "Reading                  : 0.000\n",
            "Loading batch            : 0.022\n",
            "Detection (1 images)     : 2.364\n",
            "Output processing        : 0.000\n",
            "Drawing boxes            : 0.017\n",
            "Average time per img     : 2.403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "Jk2fLjeaD2_c",
        "outputId": "dba3d508-0bc1-46b7-851f-0a87c422d095"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from __future__ import division\n",
        "import time\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import cv2 \n",
        "import argparse\n",
        "import os \n",
        "import os.path as osp\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def parse_arg():\n",
        "  \"\"\"\n",
        "  Parse arguments to detect module\n",
        "  \"\"\"\n",
        "\n",
        "  parser = argparse.ArgumentParser(description=\"reYOLO Detection Module\")\n",
        "  parser.add_argument(\"--video\", dest=\"video_file\", default=\"/content/videoplayback.mp4\", type=str, help=\"Image path or directory containing images to perform detection\")\n",
        "  parser.add_argument(\"--bs\", dest=\"bs\", default=1, help=\"Batch size\")\n",
        "  parser.add_argument(\"--confidence\", dest=\"confidence\", default=0.5, help=\"Object confidence to filter predictions\")\n",
        "  parser.add_argument(\"--nms\", dest=\"nms\", default=0.4, help=\"NMS Threshold\")\n",
        "  parser.add_argument(\"--cfg\", dest=\"cfg_file\", default=\"/content/yolov3.cfg\", type=str, help=\"Config file path\")\n",
        "  parser.add_argument(\"--weights\", dest=\"weights_file\", default=\"/content/yolov3.weights\", type=str, help=\"Weights file path\")\n",
        "  parser.add_argument(\"--dataset\", dest=\"dataset\", default=\"/content/coco.names\", type=str, help=\"Dataset file path\")\n",
        "  parser.add_argument(\"--colors\", dest=\"colors_file\", default=\"/content/pallete\", type=str, help=\"Colors file path\")\n",
        "\n",
        "  args, unknown = parser.parse_known_args()\n",
        "  return args\n",
        "\n",
        "class VideoDetect():\n",
        "  def __init__(self):\n",
        "    args = parse_arg()\n",
        "    self.video_file = args.video_file\n",
        "    self.batch_size = args.bs\n",
        "    self.confidence = args.confidence\n",
        "    self.nms = args.nms\n",
        "    self.cfg_file = args.cfg_file\n",
        "    self.weights_file = args.weights_file\n",
        "    self.classes = load_dataset(args.dataset)\n",
        "    self.num_classes = len(self.classes)\n",
        "    self.colors_file = args.colors_file\n",
        "    self.CUDA = torch.cuda.is_available()\n",
        "  \n",
        "  def load_network(self):\n",
        "    \"\"\"\n",
        "    Setup neural network\n",
        "    \"\"\"\n",
        "    self.model = Darknet(self.cfg_file)\n",
        "    self.model.load_weight(self.weights_file)\n",
        "    self.input_dim = int(self.model.net[\"height\"])\n",
        "    assert self.input_dim % 32 == 0\n",
        "    assert self.input_dim > 32\n",
        "  \n",
        "  def get_detections(self):\n",
        "    self.load_network()\n",
        "    if self.CUDA:         # if cuda available\n",
        "      self.model.cuda()\n",
        "    \n",
        "    self.model.eval()     # set model in evaluation mode\n",
        "\n",
        "    # get video capture from source (file/webcam)\n",
        "    cap = cv2.VideoCapture(self.video_file)\n",
        "    # cap = cv2.VideoCapture(0)   # webcam\n",
        "    assert cap.isOpened(), 'Cannot captutre video source'\n",
        "    \n",
        "    frames = 0\n",
        "    start = time.time()\n",
        "    while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "\n",
        "      if ret:\n",
        "        image = pre_image(frame, self.input_dim)\n",
        "        img_dim = frame.shape[1], frame.shape[0]\n",
        "        img_dim = torch.FloatTensor(img_dim).repeat(1, 2)\n",
        "\n",
        "        if self.CUDA:\n",
        "          img_dim = img_dim.cuda()\n",
        "          image = image.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          prediction = self.model(Variable(image, volatile=True), self.CUDA)\n",
        "        prediction = get_result(prediction, self.confidence, self.num_classes, nms_conf=self.nms)\n",
        "        if type(prediction) == int:\n",
        "          frames += 1\n",
        "          print(\"FPS: {:5.4f}\".format(frames / (time.time() - start)))\n",
        "          # cv2.imshow(\"frame\", frame)\n",
        "          cv2_imshow(frame)\n",
        "          key = cv2.waitKey(1)\n",
        "          if key & 0xFF == ord('q'):    # exit if press q\n",
        "            break\n",
        "          continue\n",
        "        \n",
        "        img_dim = img_dim.repeat(prediction.size(0), 1)\n",
        "        scale_factor = torch.min(self.input_dim / img_dim, 1)[0].view(-1, 1)\n",
        "        prediction[:, [1, 3]] -= (self.input_dim - scale_factor * img_dim[:, 0].view(-1, 1)) / 2\n",
        "        prediction[:, [2, 4]] -= (self.input_dim - scale_factor * img_dim[:, 1].view(-1, 1)) / 2\n",
        "        prediction[:, 1: 5] /= scale_factor\n",
        "\n",
        "        for i in range(prediction.shape[0]):\n",
        "          prediction[i, [1, 3]] = torch.clamp(prediction[i, [1, 3]], 0.0, img_dim[i, 0])\n",
        "          prediction[i, [2, 4]] = torch.clamp(prediction[i, [2, 4]], 0.0, img_dim[i, 1])\n",
        "        \n",
        "        list(map(lambda x: draw_result(x, frame, self.colors, self.classes), prediction))\n",
        "        # cv2.imshow(\"frame\", frame)\n",
        "        cv2_imshow(frame)\n",
        "        key = cv2.waitKey(1)\n",
        "        if key & 0xFF == ord('q'):\n",
        "          break\n",
        "        frames += 1\n",
        "        t = time.time() - start\n",
        "        print(\"Predicted in {1:6.3f} seconds\".format(t))\n",
        "        print(\"FPS: {:5.2f}\".format(frames / (time.time() - start)))\n",
        "      else:\n",
        "        break\n",
        "\n",
        "test = VideoDetect()\n",
        "test.get_detections()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "curl: (51) SSL: no alternative certificate subject name matches target host name 'www.sample-videos.com'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-8b45d84a39b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoDetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-8b45d84a39b6>\u001b[0m in \u001b[0;36mget_detections\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# cap = cv2.VideoCapture(0)   # webcam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cannot captutre video source'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Cannot captutre video source"
          ]
        }
      ]
    }
  ]
}