{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reyolo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6940b8f2969647c78aa030aa6202d53a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0471f986d78a4c2fa4c18d3e71ac393b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5e6dac75524e406da66f096ce033383c",
              "IPY_MODEL_62701191bc01484b8a1cb5f78b159d7d",
              "IPY_MODEL_1fae7a19aa274bd98dd708818ca5ce02"
            ]
          }
        },
        "0471f986d78a4c2fa4c18d3e71ac393b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "5e6dac75524e406da66f096ce033383c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ae4238878cd48608dcefd0b616a2388",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 0:   7%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fad3747418404e5c9def55adfb820389"
          }
        },
        "62701191bc01484b8a1cb5f78b159d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_773139c79bb741ae83214e704f51c5fe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 3822,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 260,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b940b089b54d412b8a94978c8b6894bc"
          }
        },
        "1fae7a19aa274bd98dd708818ca5ce02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0585552258ae4dfda3f59894d50bf574",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 260/3822 [1:02:35&lt;14:17:36, 14.45s/it, train_loss_step=23.80]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_feadefbc35214dd6b274adc0b6035841"
          }
        },
        "3ae4238878cd48608dcefd0b616a2388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fad3747418404e5c9def55adfb820389": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "773139c79bb741ae83214e704f51c5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b940b089b54d412b8a94978c8b6894bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0585552258ae4dfda3f59894d50bf574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "feadefbc35214dd6b274adc0b6035841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUql6KyLZROt"
      },
      "source": [
        "# Detection Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GD9M5U4ZMDn",
        "outputId": "5e83e0db-6c02-4877-de6d-78093d07a066"
      },
      "source": [
        "pip install pytorch-lightning"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.7/dist-packages (1.4.0rc0)\n",
            "Requirement already satisfied: torchmetrics>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.6.0)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2021.11.0)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.41.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (5.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEHTy5XagBNO"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DetectionLayer(pl.LightningModule):\n",
        "  \"\"\"\n",
        "  Use for yolo module\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, anchors, num_classes, apply_focal_loss, image_dim):\n",
        "    super(DetectionLayer, self).__init__()\n",
        "    self.anchors = anchors\n",
        "    self.apply_focal_loss = apply_focal_loss\n",
        "    self.num_anchors = len(anchors)\n",
        "    self.num_classes = num_classes\n",
        "    self.ignore_thres = 0.5\n",
        "    self.mse_loss = nn.MSELoss()\n",
        "    self.bce_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
        "    self.obj_scale = 1\n",
        "    self.no_obj_scale = 100\n",
        "    self.metrics = {}\n",
        "    self.image_dim = image_dim\n",
        "    self.grid_size = 0\n",
        "    self.focal_loss = FocalLoss(self.bce_loss,gamma=1.5, alpha=0.25)\n",
        "  \n",
        "  def compute_bce_loss(self, inputs, targets, apply_focal_loss):\n",
        "    if apply_focal_loss:\n",
        "      self.no_obj_scale = 1\n",
        "      return self.focal_loss(inputs, targets)\n",
        "    else:\n",
        "      self.bce_loss = nn.BCELoss()\n",
        "      return self.bce_loss(inputs, targets)\n",
        "  \n",
        "  def compute_grid_offsets(self, grid_size, CUDA=True):\n",
        "    self.grid_size = grid_size\n",
        "    g = self.grid_size\n",
        "    FloatTensor = torch.cuda.FloatTensor if CUDA else torch.FloatTensor\n",
        "    self.stride = self.image_dim / self.grid_size\n",
        "\n",
        "    self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
        "    self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
        "\n",
        "    self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n",
        "    self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
        "    self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
        "  \n",
        "  def forward(self, x, targets=None, image_dim=None):\n",
        "    FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
        "    LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
        "\n",
        "    self.image_dim = image_dim\n",
        "    num_samples = x.size(0)\n",
        "    grid_size = x.size(2)\n",
        "\n",
        "    prediction = (x.view(num_samples, self.num_anchors, self.num_classes + 5, grid_size, grid_size).permute(0, 1, 3, 4, 2).contiguous())\n",
        "\n",
        "    # center x, y\n",
        "    x = torch.sigmoid(prediction[..., 0])\n",
        "    y = torch.sigmoid(prediction[..., 1])\n",
        "    # width, height\n",
        "    w = prediction[..., 2]\n",
        "    h = prediction[..., 3]\n",
        "\n",
        "    pred_conf = torch.sigmoid(prediction[..., 4])\n",
        "    pred_cls = torch.sigmoid(prediction[..., 5:])\n",
        "\n",
        "    if grid_size != self.grid_size:\n",
        "      self.compute_grid_offsets(grid_size, CUDA=x.is_cuda)\n",
        "    \n",
        "    # Add offset and scale with anchors\n",
        "    pred_boxes = FloatTensor(prediction[..., :4].shape)\n",
        "    pred_boxes[..., 0] = x.data + self.grid_x\n",
        "    pred_boxes[..., 1] = y.data + self.grid_y\n",
        "    pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w\n",
        "    pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h\n",
        "\n",
        "    res = (pred_boxes.view(num_samples, -1, 4) * self.stride, pred_conf.view(num_samples, -1, 1), pred_cls.view(num_samples, -1, self.num_classes),)\n",
        "    output = torch.cat(res, -1)\n",
        "\n",
        "    if targets is None:\n",
        "      return output, 0\n",
        "    else:\n",
        "      iou_scores, class_mask, obj_mask, no_obj_mask, tx, ty, tw, th, tcls, tconf = build_targets(\n",
        "        pred_boxes=pred_boxes,\n",
        "        pred_cls=pred_cls,\n",
        "        target=targets,\n",
        "        anchors=self.scaled_anchors,\n",
        "        ignore_thres=self.ignore_thres,\n",
        "      )\n",
        "\n",
        "      # loss\n",
        "      loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n",
        "      loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n",
        "      loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n",
        "      loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n",
        "      loss_conf_obj = self.compute_bce_loss(pred_conf[obj_mask], tconf[obj_mask],self.apply_focal_loss)\n",
        "      loss_conf_no_obj = self.compute_bce_loss(pred_conf[no_obj_mask], tconf[no_obj_mask],self.apply_focal_loss)\n",
        "      loss_conf = self.obj_scale * loss_conf_obj + self.no_obj_scale * loss_conf_no_obj\n",
        "      loss_cls = self.compute_bce_loss(pred_cls[obj_mask], tcls[obj_mask], self.apply_focal_loss)\n",
        "      total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n",
        "\n",
        "      # metrics\n",
        "      cls_acc = 100 * class_mask[obj_mask].mean()\n",
        "      conf_obj = pred_conf[obj_mask].mean()\n",
        "      conf_no_obj = pred_conf[no_obj_mask].mean()\n",
        "      conf50 = (pred_conf > 0.5).float()\n",
        "      iou50 = (iou_scores > 0.5).float()\n",
        "      iou75 = (iou_scores > 0.75).float()\n",
        "      detected_mask = conf50 * class_mask\n",
        "      precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)\n",
        "      recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)\n",
        "      recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)\n",
        "\n",
        "      self.metrics = {\n",
        "        \"loss\": to_cpu(total_loss).item(),\n",
        "        \"x\": to_cpu(loss_x).item(),\n",
        "        \"y\": to_cpu(loss_y).item(),\n",
        "        \"w\": to_cpu(loss_w).item(),\n",
        "        \"h\": to_cpu(loss_h).item(),\n",
        "        \"conf\": to_cpu(loss_conf).item(),\n",
        "        \"cls\": to_cpu(loss_cls).item(),\n",
        "        \"cls_acc\": to_cpu(cls_acc).item(),\n",
        "        \"recall50\": to_cpu(recall50).item(),\n",
        "        \"recall75\": to_cpu(recall75).item(),\n",
        "        \"precision\": to_cpu(precision).item(),\n",
        "        \"conf_obj\": to_cpu(conf_obj).item(),\n",
        "        \"conf_no_obj\": to_cpu(conf_no_obj).item(),\n",
        "        \"grid_size\": grid_size,\n",
        "      }\n",
        "\n",
        "      return output, total_loss"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch47yydy50eI"
      },
      "source": [
        "class Upsample(pl.LightningModule):\n",
        "  \"\"\"\n",
        "  nn.Upsample is deprecated\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, scale_factor, mode=\"nearest\"):\n",
        "    super(Upsample, self).__init__()\n",
        "    self.scale_factor = scale_factor\n",
        "    self.mode = mode\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
        "    return x"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPvDoC-E_a42"
      },
      "source": [
        "class FocalLoss(pl.LightningModule):\n",
        "  def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):\n",
        "    super(FocalLoss, self).__init__()\n",
        "    self.loss_fcn = loss_fcn\n",
        "    self.gamma = gamma\n",
        "    self.alpha = alpha\n",
        "    self.reduction = loss_fcn.reduction\n",
        "    self.loss_fcn.reduction = \"none\"\n",
        "  \n",
        "  def forward(self, pred, t):\n",
        "    loss = self.loss_fcn(pred, t)\n",
        "    prediction = torch.sigmoid(pred)\n",
        "    pt = t * prediction * (1 - t) * (1 - prediction)\n",
        "    alpha_factor = t * self.alpha + (1 - t) * (1 - self.alpha)\n",
        "    m_factor = (1 - pt) ** self.gamma\n",
        "    loss *= alpha_factor * m_factor\n",
        "\n",
        "    if self.reduction == \"mean\":\n",
        "      return loss.mean()\n",
        "    elif self.redduction == \"sum\":\n",
        "      return loss.sum()\n",
        "    else:\n",
        "      return loss"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyz8LNnPK17e"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class Darknet(pl.LightningModule):\n",
        "  def __init__(self, cfg_file, apply_focal_loss=False):\n",
        "    super(Darknet, self).__init__()\n",
        "    self.apply_focal_loss = apply_focal_loss\n",
        "    self.blocks = parse_cfg(cfg_file)\n",
        "    self.net, self.module_list = create_modules(self.blocks, self.apply_focal_loss)\n",
        "    self.detection_layers = [layer[0] for layer in self.module_list if isinstance(layer[0], DetectionLayer)]\n",
        "    self.image_size = int(self.net[\"height\"])\n",
        "    self.seen = 0\n",
        "    self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    \"\"\"\n",
        "    Calculate the output\n",
        "    Transform the output detection feature maps in a vay can be processed easier\n",
        "    \"\"\"\n",
        "\n",
        "    image_dim = x.shape[2]\n",
        "    loss = 0\n",
        "    layer_outputs, detection_outputs = [], []\n",
        "\n",
        "    for i, (module_def, module) in enumerate(zip(self.blocks, self.module_list)):     \n",
        "      module_type = module_def[\"type\"]\n",
        "      \n",
        "      if module_type in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
        "        x = module(x)\n",
        "      elif module_type == \"route\":\n",
        "        x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n",
        "      elif  module_type == \"shortcut\":\n",
        "        layer_i = int(module_def[\"from\"])\n",
        "        x = layer_outputs[-1] + layer_outputs[layer_i]\n",
        "      elif module_type == \"yolo\":\n",
        "        x, layer_loss = module[0](x, targets, image_dim)\n",
        "        loss += layer_loss\n",
        "        detection_outputs.append(x)\n",
        "      \n",
        "      layer_outputs.append(x)\n",
        "    detection_outputs = to_cpu(torch.cat(detection_outputs, 1))\n",
        "    return detection_outputs if targets is None else (loss, detection_outputs)\n",
        "\n",
        "  def load_weight(self, file_path):\n",
        "    # first 5 items in weight file are header information\n",
        "    # major ver, minor ver, subversion, images seen by the network\n",
        "    with open(file_path, \"rb\") as file:\n",
        "      header = np.fromfile(file, dtype=np.int32, count=5)\n",
        "      self.header_info = header\n",
        "      self.seen = self.header_info[3]\n",
        "      weights = np.fromfile(file, dtype=np.float32)\n",
        "\n",
        "    cutoff = None\n",
        "    if \"darknet53.conv.74\" in file_path:\n",
        "      cutoff = 75\n",
        "    \n",
        "    n = 0\n",
        "    for i, (module_def, module) in enumerate(zip(self.blocks, self.module_list)):\n",
        "      module_type = module_def[\"type\"]\n",
        "      if i == cutoff:\n",
        "        break\n",
        "\n",
        "      # if not convolutional, ignore\n",
        "      if module_type == \"convolutional\":\n",
        "        convol_layer = module[0]\n",
        "        try:\n",
        "          batch_normalize = int(module_def[\"batch_normalize\"])\n",
        "        except:\n",
        "          batch_normalize = 0\n",
        "        # batch normalize layer\n",
        "        if batch_normalize:\n",
        "          batch_norm_layer = module[1]\n",
        "          num_biases = batch_norm_layer.bias.numel()\n",
        "          \n",
        "          # load weights\n",
        "          bnl_biases = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          bnl_weights = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          bnl_running_mean = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          bnl_running_var = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          # cast weights into dimensions of model weights\n",
        "          bnl_biases = bnl_biases.view_as(batch_norm_layer.bias.data)\n",
        "          bnl_weights = bnl_weights.view_as(batch_norm_layer.weight.data)\n",
        "          bnl_running_mean = bnl_running_mean.view_as(batch_norm_layer.running_mean)\n",
        "          bnl_running_var = bnl_running_var.view_as(batch_norm_layer.running_var)\n",
        "\n",
        "          # copy data to model\n",
        "          batch_norm_layer.bias.data.copy_(bnl_biases)\n",
        "          batch_norm_layer.weight.data.copy_(bnl_weights)\n",
        "          batch_norm_layer.running_mean.copy_(bnl_running_mean)\n",
        "          batch_norm_layer.running_var.copy_(bnl_running_var)\n",
        "        else:     # convolutional layer\n",
        "          num_biases = convol_layer.bias.numel()\n",
        "\n",
        "          # load weights\n",
        "          convol_biases = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          # cast weights into dimensions of model weights\n",
        "          convol_biases = convol_biases.view_as(convol_layer.bias.data)\n",
        "\n",
        "          # copy data to model\n",
        "          convol_layer.bias.data.copy_(convol_biases)\n",
        "        \n",
        "        # weights of convolutional layerss\n",
        "        num_weights = convol_layer.weight.numel()\n",
        "        convol_weights = torch.from_numpy(weights[n: n + num_weights])\n",
        "        n += num_weights\n",
        "        convol_weights = convol_weights.view_as(convol_layer.weight.data)\n",
        "        convol_layer.weight.data.copy_(convol_weights)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCFJA6nVoHN5"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "import fnmatch\n",
        "\n",
        "def predict_transform(predict, input_dim, anchors, num_classes, CUDA=False):\n",
        "  \"\"\"\n",
        "  Transfer input (which is output of forward()) into 2d tensor.\n",
        "  Each row of the tensor corresponds to attributes of a bounding box.\n",
        "  \"\"\"\n",
        "\n",
        "  batch_size = predict.size(0)\n",
        "  stride = input_dim // predict.size(2)\n",
        "  grid_size = input_dim // stride\n",
        "  bounding_box_attrs = num_classes + 5\n",
        "\n",
        "  predict = predict.view(batch_size, bounding_box_attrs * len(anchors), grid_size ** 2)\n",
        "  predict = predict.transpose(1,2).contiguous()\n",
        "  predict = predict.view(batch_size, grid_size ** 2 * len(anchors), bounding_box_attrs)\n",
        "\n",
        "  # dimensions of anchors are in accordance to height and width attr of net block\n",
        "  anchors = [(a[0] / stride, a[1] / stride) for a in anchors]\n",
        "\n",
        "  # sigmoid x, y coordinates and objectness score\n",
        "  # center_x, center_y, object_confidence\n",
        "  predict[:, :, 0] = torch.sigmoid(predict[:, :, 0])\n",
        "  predict[:, :, 1] = torch.sigmoid(predict[:, :, 1])\n",
        "  predict[:, :, 4] = torch.sigmoid(predict[:, :, 4])\n",
        "\n",
        "  # add center offsets\n",
        "  grid = np.arange(grid_size)\n",
        "  x, y = np.meshgrid(grid, grid)\n",
        "  x_offset = torch.FloatTensor(x).view(-1, 1)\n",
        "  y_offset = torch.FloatTensor(y).view(-1, 1)\n",
        "\n",
        "  if CUDA:\n",
        "    x_offset = x_offset.cuda()\n",
        "    y_offset = y_offset.cuda()\n",
        "  \n",
        "  xy_offset = torch.cat((x_offset, y_offset), 1).repeat(1, len(anchors)).view(-1, 2).unsqueeze(0)\n",
        "  predict[:, :, :2] += xy_offset\n",
        "\n",
        "  # apply anchors to dimensions of bounding box\n",
        "  anchors = torch.FloatTensor(anchors)\n",
        "  if CUDA:\n",
        "    anchors = anchors.cuda()\n",
        "\n",
        "  anchors = anchors.repeat(grid_size ** 2, 1).unsqueeze(0)\n",
        "\n",
        "  predict[:, :, 2: 4] = torch.exp(predict[:, :, 2: 4]) * anchors\n",
        "  # apply sigmoid to class scores\n",
        "  predict[:, :, 5: num_classes + 5] = torch.sigmoid(predict[:, :, 5: num_classes + 5])\n",
        "  # resize detections map to size of input image\n",
        "  predict[:, :, :4] *= stride\n",
        "\n",
        "  return predict"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OQtVW-_RjtJ"
      },
      "source": [
        "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n",
        "    BoolTensor = torch.cuda.BoolTensor if pred_boxes.is_cuda else torch.BoolTensor\n",
        "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
        "\n",
        "    nB = pred_boxes.size(0)\n",
        "    nA = pred_boxes.size(1)\n",
        "    nC = pred_cls.size(-1)\n",
        "    nG = pred_boxes.size(2)\n",
        "\n",
        "    # output tensors\n",
        "    obj_mask = BoolTensor(nB, nA, nG, nG).fill_(0)\n",
        "    no_obj_mask = BoolTensor(nB, nA, nG, nG).fill_(1)\n",
        "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
        "\n",
        "    # convert to position relative to box\n",
        "    target_boxes = target[:, 2:6] * nG\n",
        "    gxy = target_boxes[:, :2]\n",
        "    gwh = target_boxes[:, 2:]\n",
        "    \n",
        "    # get anchors with best iou\n",
        "    ious = torch.stack([bounding_box_wh_iou(anchor, gwh) for anchor in anchors])\n",
        "    _, best_n = ious.max(0)\n",
        "    \n",
        "    # separate target values\n",
        "    b, target_labels = target[:, :2].long().t()\n",
        "    gx, gy = gxy.t()\n",
        "    gw, gh = gwh.t()\n",
        "    gi, gj = gxy.long().t()\n",
        "    \n",
        "    # masks\n",
        "    obj_mask[b, best_n, gj, gi] = 1\n",
        "    no_obj_mask[b, best_n, gj, gi] = 0\n",
        "\n",
        "    # set no obj mask to zero where iou exceeds ignore threshold\n",
        "    for i, anchor_ious in enumerate(ious.t()):\n",
        "        no_obj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
        "\n",
        "    # coordinates\n",
        "    tx[b, best_n, gj, gi] = gx - gx.floor()\n",
        "    ty[b, best_n, gj, gi] = gy - gy.floor()\n",
        "    \n",
        "    # width and height\n",
        "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
        "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n",
        "    \n",
        "    # one-hot encoding of label\n",
        "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
        "    \n",
        "    # compute label correctness and iou at best anchor\n",
        "    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
        "    iou_scores[b, best_n, gj, gi] = get_bounding_boxes_iou(pred_boxes[b, best_n, gj, gi], target_boxes)\n",
        "\n",
        "    tconf = obj_mask.float()\n",
        "    return iou_scores, class_mask, obj_mask, no_obj_mask, tx, ty, tw, th, tcls, tconf"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFZUogB-VYHi"
      },
      "source": [
        "def bounding_box_wh_iou(wh1, wh2):\n",
        "    wh2 = wh2.t()\n",
        "    w1, h1 = wh1[0], wh1[1]\n",
        "    w2, h2 = wh2[0], wh2[1]\n",
        "\n",
        "    area_1 = torch.min(w1, w2) * torch.min(h1, h2)\n",
        "    area_2 = (w1 * h1 + 1e-16) + w2 * h2 - area_1\n",
        "    return area_1 / area_2"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLuHRI-uWlx9"
      },
      "source": [
        "def parse_cfg(file):\n",
        "  \"\"\"\n",
        "  Parse config from file. Returns a list of blocks.\n",
        "  Each blocks describes a block in neural network to be built.\n",
        "  \"\"\"\n",
        "\n",
        "  file = open(file, 'r')\n",
        "  lines = file.read().split('\\n')\n",
        "  lines = [x for x in lines if x and not x.startswith('#')]\n",
        "  lines = [x.rstrip().lstrip() for x in lines]\n",
        "  module_defs = []\n",
        "\n",
        "  for line in lines:\n",
        "    if line.startswith(\"[\"):                 # Check for new block\n",
        "      module_defs.append({})                 # Check if block not empty\n",
        "      module_defs[-1][\"type\"] = line[1:-1].rstrip()\n",
        "      if module_defs[-1][\"type\"] == \"convolutional\":\n",
        "        module_defs[-1][\"batch_normalize\"] = 0\n",
        "    else:\n",
        "      key, value = line.split(\"=\")           # get key-value from line\n",
        "      value = value.strip()\n",
        "      module_defs[-1][key.rstrip()] = value.strip()\n",
        "\n",
        "  return module_defs"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3FOSgxGWwJb"
      },
      "source": [
        "def create_modules(module_defs, focal_loss):\n",
        "  hyperparams = module_defs.pop(0)                # net info about the input and pre-processing\n",
        "  momentum = float(hyperparams[\"momentum\"])\n",
        "  module_list = nn.ModuleList()\n",
        "  in_channels = 3\n",
        "  output_filters = [int(hyperparams[\"channels\"])]\n",
        "\n",
        "  for module_i, module_def in enumerate(module_defs):\n",
        "    modules = nn.Sequential()\n",
        "    module_type = module_def[\"type\"]\n",
        "\n",
        "    # check type of block\n",
        "    # create new module for block\n",
        "    # append to module list (modules )\n",
        "    if module_type == \"convolutional\":\n",
        "      batch_normalize = int(module_def[\"batch_normalize\"])\n",
        "      filters = int(module_def[\"filters\"])\n",
        "      kernel_size = int(module_def[\"size\"])\n",
        "      pad = (kernel_size - 1) // 2\n",
        "\n",
        "      # convolutional layer\n",
        "      convol_layer = nn.Conv2d(in_channels=output_filters[-1], out_channels=filters, kernel_size=kernel_size, stride=int(module_def[\"stride\"]), padding=pad, bias=not batch_normalize)\n",
        "      modules.add_module(\"conv_{}\".format(module_i), convol_layer)\n",
        "\n",
        "      # batch norm layer\n",
        "      if batch_normalize:\n",
        "        modules.add_module(\"batch_norm_{}\".format(module_i), nn.BatchNorm2d(filters, momentum=momentum, eps=1e-5))\n",
        "      # linear or leaky relu for yolo\n",
        "      if module_def[\"activation\"] == \"leaky\":\n",
        "        modules.add_module(\"leaky_{}\".format(module_i), nn.LeakyReLU(0.1))\n",
        "    # maxpool layers\n",
        "    elif module_type == \"maxpool\":\n",
        "      kernel_size = int(module_def[\"size\"])\n",
        "      stride = int(module_def[\"stride\"])\n",
        "\n",
        "      if kernel_size == 2 and stride == 1:\n",
        "        modules.add_module('ZeroPad2d', nn.ZeroPad2d((0, 1, 0, 1)))\n",
        "      \n",
        "      maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))\n",
        "      modules.add_module(\"maxpool_{}\".format(module_i), maxpool)\n",
        "    # unsample layers\n",
        "    elif module_type == \"upsample\":\n",
        "      upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n",
        "      modules.add_module(\"upsample_{}\".format(module_i), upsample)\n",
        "    # route layer\n",
        "    elif module_type == \"route\":\n",
        "      layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
        "      filters = sum([output_filters[1:][i] for i in layers])\n",
        "      modules.add_module(\"route_{}\".format(module_i), nn.Sequential())\n",
        "    # shortcut\n",
        "    elif module_type == \"shortcut\":\n",
        "      filters = output_filters[1:][int(module_def[\"from\"])]\n",
        "      modules.add_module(\"shortcut_{}\".format(module_i), nn.Sequential())\n",
        "    # yolo: detection layer\n",
        "    elif module_type == \"yolo\":\n",
        "      anchor_indexs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n",
        "\n",
        "      anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")]\n",
        "      anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n",
        "      anchors = [anchors[i] for i in anchor_indexs]\n",
        "      num_classes = int(module_def[\"classes\"])\n",
        "      image_size = int(hyperparams[\"height\"])\n",
        "\n",
        "      detection = DetectionLayer(anchors, num_classes, focal_loss, image_size)\n",
        "      modules.add_module(\"Detection_{}\".format(module_i), detection)\n",
        "\n",
        "    module_list.append(modules)\n",
        "    output_filters.append(filters)\n",
        "\n",
        "  return hyperparams, module_list"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F7cDfhlY7mw"
      },
      "source": [
        "def test_input(file_path, img_size):\n",
        "    img = cv2.imread(file_path)\n",
        "    img = cv2.resize(img, img_size)\n",
        "    img_result = img[:, :, ::-1].transpose((2, 0, 1))     # BGR -> RGB\n",
        "    img_result = img_result[np.newaxis, :, :, :]/255.0    # Add a channel at 0\n",
        "    img_result = torch.from_numpy(img_result).float()     # Convert to float\n",
        "    img_result = Variable(img_result)                     # Convert to Variable\n",
        "    return img_result"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3IXXibifjgQ"
      },
      "source": [
        "def get_result(prediction, confidence, num_classes, nms_conf=0.4):\n",
        "  # object confidence thresholding\n",
        "  # each bounding box having objectness score below a threshold\n",
        "  # set the value of entrie row representing the bounding box to zero\n",
        "  conf_mask = (prediction[:, :, 4] > confidence).float().unsqueeze(2)\n",
        "  prediction *= conf_mask\n",
        "\n",
        "  # transform center_x, center_y, height, width of box\n",
        "  # to top_left_corner_x, top_right_corner_y, right_bottom_corner_x, right_bottom_corner_y \n",
        "  box = prediction.new(prediction.shape)\n",
        "  box[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
        "  box[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
        "  box[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
        "  box[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
        "  prediction[:, :, :4] = box[:, :, :4]\n",
        "\n",
        "  batch_size = prediction.size(0)\n",
        "  check = False\n",
        "\n",
        "  # the number of true detections in every image may be different\n",
        "  # confidence thresholding and nms has to be done for one image at conce\n",
        "  # must loop over the 1st dimension of prediction\n",
        "  for i in range(batch_size):\n",
        "    image_prediction = prediction[i]      # image tensor\n",
        "\n",
        "    # each bounding box has 85 attri\n",
        "    # 80 attri are class scores\n",
        "    max_confidence, max_confidence_score = torch.max(image_prediction[:, 5: num_classes + 5], 1)\n",
        "    max_confidence = max_confidence.float().unsqueeze(1)\n",
        "    max_confidence_score = max_confidence_score.float().unsqueeze(1)\n",
        "    image_prediction = torch.cat((image_prediction[:, :5], max_confidence, max_confidence_score), 1)\n",
        "\n",
        "    non_zero = torch.nonzero(image_prediction[:, 4])\n",
        "    try:\n",
        "      image_prediction_ = image_prediction[non_zero.squeeze(), :].view(-1, 7)\n",
        "    except:\n",
        "      continue\n",
        "    \n",
        "    if image_prediction_.shape[0] == 0:\n",
        "      continue\n",
        "    \n",
        "    # get various classes detected in image\n",
        "    image_classes = get_unique(image_prediction_[:, -1])\n",
        "\n",
        "    for c in image_classes:\n",
        "      # nms\n",
        "      # get detections with 1 particular class\n",
        "      class_mask = image_prediction_ * (image_prediction_[:, -1] == c).float().unsqueeze(1)\n",
        "      class_mask_index = torch.nonzero(class_mask[:, -2]).squeeze()\n",
        "      image_prediction_class = image_prediction_[class_mask_index].view(-1, 7)\n",
        "\n",
        "      # sort detection\n",
        "      # confidence at top\n",
        "      confidence_sorted_index = torch.sort(image_prediction_class[:, 4], descending=True)[1]\n",
        "      image_prediction_class = image_prediction_class[confidence_sorted_index]\n",
        "      index = image_prediction_class.size(0)\n",
        "\n",
        "      for idx in range(index):\n",
        "        # get ious of all boxes\n",
        "        try:\n",
        "          ious = get_bounding_boxes_iou(image_prediction_class[idx].unsqueeze(0), image_prediction_class[idx + 1:])\n",
        "        except ValueError:\n",
        "          break\n",
        "        except IndexError:\n",
        "          break\n",
        "        \n",
        "        # mark zero all detections iou > threshold\n",
        "        iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
        "        image_prediction_class[idx + 1:] *= iou_mask\n",
        "\n",
        "        # remove non-zero entries\n",
        "        non_zero_index = torch.nonzero(image_prediction_class[:, 4]).squeeze()\n",
        "        image_prediction_class = image_prediction_class[non_zero_index].view(-1, 7)\n",
        "      \n",
        "      batch_index = image_prediction_class.new(image_prediction_class.size(0), 1).fill_(i)\n",
        "      s = batch_index, image_prediction_class\n",
        "\n",
        "      if not check:\n",
        "        output = torch.cat(s, 1)\n",
        "        check = True\n",
        "      else:\n",
        "        output = torch.cat((output, torch.cat(s, 1)))\n",
        "      \n",
        "  try:\n",
        "    return output\n",
        "  except:\n",
        "    return 0"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvMj1fcDmys2"
      },
      "source": [
        "def get_unique(tensor):\n",
        "  np_tensor = tensor.cpu().numpy()\n",
        "  unique = np.unique(np_tensor)\n",
        "  unique_tensor = torch.from_numpy(unique)\n",
        "  result = tensor.new(unique_tensor.shape)\n",
        "  result.copy_(unique_tensor)\n",
        "\n",
        "  return result"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L1EL5yfqAQt"
      },
      "source": [
        "def get_bounding_boxes_iou(b1, b2):\n",
        "  \"\"\"\n",
        "  Returns iou of 2 bouding boxes\n",
        "  \"\"\"\n",
        "\n",
        "  # get coordinates of 2 bounding boxes\n",
        "  b1_x1, b1_y1, b1_x2, b1_y2 = b1[:, 0], b1[:, 1], b1[:, 2], b1[:, 3]\n",
        "  b2_x1, b2_y1, b2_x2, b2_y2 = b2[:, 0], b2[:, 1], b2[:, 2], b2[:, 3]\n",
        "\n",
        "  # get coordinates of overclap rectangle\n",
        "  x1 = torch.max(b1_x1, b2_x1)\n",
        "  y1 = torch.max(b1_y1, b2_y1)\n",
        "  x2 = torch.min(b1_x2, b2_x2)\n",
        "  y2 = torch.min(b1_y2, b2_y2)\n",
        "\n",
        "  # overclap area\n",
        "  area = torch.clamp(x2 - x1 + 1, min=0) * torch.clamp(y2 - y1 + 1, min=0)\n",
        "\n",
        "  # union area\n",
        "  b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
        "  b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
        "\n",
        "  return area / (b1_area + b2_area - area)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buiHVHnpUPZY"
      },
      "source": [
        "def resize_image(img, input_dim):\n",
        "    \"\"\"\n",
        "    resize image with unchanged aspect ratio using padding\n",
        "    \"\"\"\n",
        "    width, height = img.shape[1], img.shape[0]\n",
        "    w, h = input_dim\n",
        "    new_width = int(width * min(w / width, h / height))\n",
        "    new_height = int(height * min(w / width, h / height))\n",
        "    resized_image = cv2.resize(img, (new_width, new_height), interpolation = cv2.INTER_CUBIC)\n",
        "    \n",
        "    canvas = np.full((input_dim[1], input_dim[0], 3), 128)\n",
        "    canvas[(h - new_height) // 2: (h - new_height) // 2 + new_height,(w - new_width) // 2: (w - new_width) // 2 + new_width,  :] = resized_image\n",
        "    return canvas"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiJ5sHZgpfvt"
      },
      "source": [
        "def pre_image(img, input_dim):\n",
        "  \"\"\"\n",
        "  Prepare image as input for neural network\n",
        "  \"\"\"\n",
        "\n",
        "  img = resize_image(img, (input_dim, input_dim))\n",
        "  img = img[:, :, ::-1].transpose((2, 0, 1)).copy()\n",
        "  img = torch.from_numpy(img).float().div(255.0).unsqueeze(0)\n",
        "  return img"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTGiD8L61HxB"
      },
      "source": [
        "def draw_result(x, results, colors, classes):\n",
        "  t1 = tuple(x[1: 3].int())\n",
        "  t2 = tuple(x[3: 5].int())\n",
        "  img = results[int(x[0])]\n",
        "  text_font = cv2.FONT_HERSHEY_PLAIN\n",
        "  cls = int(x[-1])\n",
        "  color = random.choice(colors)\n",
        "  label = \"{}\".format(classes[cls])\n",
        "  cv2.rectangle(img, t1, t2, color, 1)\n",
        "  text_size = cv2.getTextSize(label, text_font, 1, 1)[0]\n",
        "  t2 = t1[0] + text_size[0] + 3, t1[1] + text_size[1] + 4\n",
        "  cv2.rectangle(img, t1, t2, color, -1)\n",
        "  text_pos = t1[0], t1[1] + text_size[1] + 4\n",
        "  cv2.putText(img, label, text_pos, text_font, 1, [255, 255, 255], 1)\n",
        "  return img"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8IjIIknF0i2"
      },
      "source": [
        "def load_dataset(file_path):\n",
        "  file = open(file_path, \"r\")\n",
        "  names = file.read().split(\"\\n\")[:-1]\n",
        "  return names"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ8TVAJPTAwd"
      },
      "source": [
        "def to_cpu(tensor):\n",
        "    return tensor.detach().cpu()"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18-ivSdT-9o3"
      },
      "source": [
        "def save_code_files(output_path, root_path):\n",
        "  def match_patterns(include, exclude):\n",
        "    def _ignore_patterns(path, names):\n",
        "      # If current path in exclude list, ignore everything\n",
        "      if path in set(name for pattern in exclude for name in fnmatch.filter([path], pattern)):\n",
        "        return names\n",
        "      # Get initial keep list from include patterns\n",
        "      keep = set(name for pattern in include for name in fnmatch.filter(names, pattern))\n",
        "      # Add subdirectories to keep list\n",
        "      keep = set(list(keep) + [name for name in names if os.path.isdir(os.path.join(path, name))])\n",
        "      # Remove exclude patterns from keep list\n",
        "      keep_ex = set(name for pattern in exclude for name in fnmatch.filter(keep, pattern))\n",
        "      keep = [name for name in keep if name not in keep_ex]\n",
        "      # Ignore files not in keep list\n",
        "      return set(name for name in names if name not in keep)\n",
        "\n",
        "    return _ignore_patterns\n",
        "\n",
        "\n",
        "  dst_dir = os.path.join(output_path, \"code\")\n",
        "  if os.path.exists(dst_dir):\n",
        "    shutil.rmtree(dst_dir)\n",
        "  shutil.copytree(root_path, dst_dir, ignore=match_patterns(include=['*.py', '*.data', '*.cfg'],\n",
        "                                                            exclude=['experiment*',\n",
        "                                                                      '*.idea',\n",
        "                                                                      '*__pycache__',\n",
        "                                                                      'weights',\n",
        "                                                                      'wandb',\n",
        "                                                                      'asets'\n",
        "                                                                      ]))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XNRDOVshKjT"
      },
      "source": [
        "## Image detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsja-s4xSf7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d46d992-6d3e-4659-d99b-08a17e1fb8a7"
      },
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import cv2\n",
        "import argparse\n",
        "import os\n",
        "import os.path as osp\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "\n",
        "def parse_arg():\n",
        "  \"\"\"\n",
        "  Parse arguments to detect module\n",
        "  \"\"\"\n",
        "\n",
        "  parser = argparse.ArgumentParser(description=\"reYOLO Detection Module\")\n",
        "  parser.add_argument(\"--images\", default=\"/content/eagle.jpg\", type=str, help=\"Image path or directory containing images to perform detection\")\n",
        "  parser.add_argument(\"--det\", default=\"det\", type=str, help=\"Imgage path or directory to store detections\")\n",
        "  parser.add_argument(\"--bs\", default=1, help=\"Batch size\")\n",
        "  parser.add_argument(\"--confidence\", default=0.5, help=\"Object confidence to filter predictions\")\n",
        "  parser.add_argument(\"--nms\", default=0.4, help=\"NMS Threshold\")\n",
        "  parser.add_argument(\"--cfg\", dest=\"cfg_file\", default=\"/content/yolov3-tiny.cfg\", type=str, help=\"Config file path\")\n",
        "  parser.add_argument(\"--weights\", dest=\"weights_file\", default=\"/content/yolov3-tiny.weights\", type=str, help=\"Weights file path\")\n",
        "  parser.add_argument(\"--dataset\", default=\"/content/coco.names\", type=str, help=\"Dataset file path\")\n",
        "  parser.add_argument(\"--colors\", dest=\"colors_file\", default=\"/content/pallete\", type=str, help=\"Colors file path\")\n",
        "\n",
        "  args, _ = parser.parse_known_args()\n",
        "  return args\n",
        "\n",
        "class ImageDetect():\n",
        "  def __init__(self):\n",
        "    args = parse_arg()\n",
        "    self.images = args.images\n",
        "    self.cfg_file = args.cfg_file\n",
        "    self.weights_file = args.weights_file\n",
        "    self.det = args.det\n",
        "    self.batch_size = int(args.bs)\n",
        "    self.confidence = float(args.confidence)\n",
        "    self.nms = float(args.nms)\n",
        "    self.CUDA = torch.cuda.is_available()\n",
        "    self.classes = load_dataset(args.dataset)\n",
        "    self.num_classes = len(self.classes)\n",
        "    self.colors_file = args.colors_file\n",
        "  \n",
        "  def load_network(self):\n",
        "    \"\"\"\n",
        "    Setup neural network\n",
        "    \"\"\"\n",
        "    self.model = Darknet(self.cfg_file)\n",
        "    self.model.load_weight(self.weights_file)\n",
        "    self.input_dim = int(self.model.net[\"height\"])\n",
        "    assert self.input_dim % 32 == 0\n",
        "    assert self.input_dim > 32\n",
        "  \n",
        "  def get_detections(self):\n",
        "    self.load_network()\n",
        "    if self.CUDA:         # if cuda available\n",
        "      self.model.cuda()\n",
        "    \n",
        "    self.model.eval()       # set model in evaluation mode\n",
        "    read_time = time.time()\n",
        "\n",
        "    try:\n",
        "      image_list = [osp.join(osp.realpath(\".\"), self.images, img) for img in os.listdir(self.images)]\n",
        "    except NotADirectoryError:\n",
        "      image_list = []\n",
        "      image_list.append(osp.join(osp.realpath(\".\"), self.images))\n",
        "    except FileNotFoundError:\n",
        "      print(\"No file or directory with name {}\".format(self.images))\n",
        "      exit()\n",
        "\n",
        "    if not os.path.exists(self.det):\n",
        "      os.makedirs(self.det)\n",
        "\n",
        "    load_batch_time = time.time()\n",
        "    loaded_img_list = [cv2.imread(x) for x in image_list]\n",
        "    # pytorch variables for images\n",
        "    img_batches = list(map(pre_image, loaded_img_list, [self.input_dim for i in range(len(image_list))]))\n",
        "    # dimensions of original images\n",
        "    img_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_img_list]\n",
        "    img_dim_list = torch.FloatTensor(img_dim_list).repeat(1, 2)\n",
        "\n",
        "    # create batches\n",
        "    left_over = 0\n",
        "    if len(img_dim_list) % self.batch_size:\n",
        "      left_over = 1\n",
        "    \n",
        "    if self.batch_size != 1:\n",
        "      num_batches = len(image_list) // self.batch_size + left_over\n",
        "      img_batches = [torch.car((img_batches[i * self.batch_size: min((i + 1) * self.batch_size, len(img_batches))])) for i in range(num_batches)]\n",
        "    \n",
        "    check = 0\n",
        "    if self.CUDA:\n",
        "      img_dim_list = img_dim_list.cuda()\n",
        "\n",
        "    start_detect_loop_time = time.time()\n",
        "\n",
        "    # detection loop\n",
        "    for i, batch in enumerate(img_batches):\n",
        "      start = time.time()\n",
        "      if self.CUDA:\n",
        "        batch = batch.cuda()\n",
        "      with torch.no_grad():\n",
        "        prediction = self.model(Variable(batch))\n",
        "      \n",
        "      prediction = get_result(prediction, self.confidence, self.num_classes, nms_conf=self.nms)\n",
        "\n",
        "      end = time.time()\n",
        "      if type(prediction) == int:\n",
        "        for img_num, image in enumerate(image_list[i * self.batch_size: min((i + 1) * self.batch_size, len(image_list))]):\n",
        "          img_id = i * self.batch_size + img_num\n",
        "          print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start) / self.batch_size))\n",
        "          print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
        "          print(\"*********************************************\")\n",
        "        continue\n",
        "      \n",
        "      # transform attr from index in batch to index in image list\n",
        "      prediction[:, 0] += i * self.batch_size\n",
        "      if not check:           # initialize output\n",
        "        output = prediction\n",
        "        check = 1\n",
        "      else:\n",
        "        output = torch.cat((output, prediction))\n",
        "      \n",
        "      for img_num, image in enumerate(image_list[i * self.batch_size: min((i + 1) * self.batch_size, len(image_list))]):\n",
        "          img_id = i * self.batch_size + img_num\n",
        "          objects = [self.classes[int(x[-1])] for x in output if int(x[0]) == img_id]\n",
        "          print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start) / self.batch_size))\n",
        "          print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objects)))\n",
        "          print(\"*********************************************\")\n",
        "      \n",
        "      if self.CUDA:\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # draw bouding boxes on images\n",
        "    try:\n",
        "      output\n",
        "    except NameError:\n",
        "      print(\"No detection were made\")\n",
        "      exit()\n",
        "    \n",
        "    img_dim_list = torch.index_select(img_dim_list, 0, output[:, 0].long())\n",
        "    scale_factor = torch.min(self.input_dim / img_dim_list, 1)[0].view(-1, 1)\n",
        "    output[:, [1, 3]] -= (self.input_dim - scale_factor * img_dim_list[:, 0].view(-1, 1)) / 2\n",
        "    output[:, [2, 4]] -= (self.input_dim - scale_factor * img_dim_list[:, 1].view(-1, 1)) / 2\n",
        "    output[:, 1:5] /= scale_factor\n",
        "\n",
        "    for i in range(output.shape[0]):\n",
        "      output[i, [1, 3]] = torch.clamp(output[i, [1, 3]], 0.0, img_dim_list[i, 0])\n",
        "      output[i, [2, 4]] = torch.clamp(output[i, [2, 4]], 0.0, img_dim_list[i, 1])\n",
        "    \n",
        "    output_recast_time = time.time()\n",
        "    class_load_time = time.time()\n",
        "    colors = pkl.load(open(self.colors_file, \"rb\"))\n",
        "    draw_time = time.time()\n",
        "\n",
        "    list(map(lambda x: draw_result(x, loaded_img_list, colors, self.classes), output))\n",
        "    detect_names = pd.Series(image_list).apply(lambda x: \"{}/detect_{}\".format(self.det, x.split(\"/\")[-1]))\n",
        "    list(map(cv2.imwrite, detect_names, loaded_img_list))\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Results\")\n",
        "    print(\"*********************************************\")\n",
        "    print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Reading\", load_batch_time - read_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_detect_loop_time - load_batch_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(image_list)) +  \" images)\", output_recast_time - start_detect_loop_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Output processing\", class_load_time - output_recast_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Drawing boxes\", end - draw_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Average time per img\", (end - load_batch_time) / len(image_list)))\n",
        "    print(\"Result Folder: {}\".format(self.det))\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "test = ImageDetect()\n",
        "test.get_detections()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eagle.jpg            predicted in  0.187 seconds\n",
            "Objects Detected:    bird\n",
            "*********************************************\n",
            "Results\n",
            "*********************************************\n",
            "Task                     : Time Taken (in seconds)\n",
            "Reading                  : 0.000\n",
            "Loading batch            : 0.014\n",
            "Detection (1 images)     : 0.195\n",
            "Output processing        : 0.000\n",
            "Drawing boxes            : 0.030\n",
            "Average time per img     : 0.240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDUBbazZhSol"
      },
      "source": [
        "## Video detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk2fLjeaD2_c"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from __future__ import division\n",
        "import time\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import cv2\n",
        "import argparse\n",
        "\n",
        "def parse_arg():\n",
        "  \"\"\"\n",
        "  Parse arguments to detect module\n",
        "  \"\"\"\n",
        "\n",
        "  parser = argparse.ArgumentParser(description=\"reYOLO Detection Module\")\n",
        "  parser.add_argument(\"--video\", dest=\"video_file\", default=\"/content/videoplayback.mp4\", type=str, help=\"Image path or directory containing images to perform detection\")\n",
        "  parser.add_argument(\"--bs\", default=1, help=\"Batch size\")\n",
        "  parser.add_argument(\"--confidence\", default=0.5, help=\"Object confidence to filter predictions\")\n",
        "  parser.add_argument(\"--nms\", default=0.4, help=\"NMS Threshold\")\n",
        "  parser.add_argument(\"--cfg\", dest=\"cfg_file\", default=\"/content/yolov3.cfg\", type=str, help=\"Config file path\")\n",
        "  parser.add_argument(\"--weights\", dest=\"weights_file\", default=\"/content/yolov3.weights\", type=str, help=\"Weights file path\")\n",
        "  parser.add_argument(\"--dataset\", default=\"/content/coco.names\", type=str, help=\"Dataset file path\")\n",
        "  parser.add_argument(\"--colors\", dest=\"colors_file\", default=\"/content/pallete\", type=str, help=\"Colors file path\")\n",
        "  parser.add_argument(\"--source\", default=\"file\", type=str, help=\"Video source\")\n",
        "  \n",
        "  args, _ = parser.parse_known_args()\n",
        "  return args\n",
        "\n",
        "class VideoDetect():\n",
        "  def __init__(self):\n",
        "    args = parse_arg()\n",
        "    self.video_file = args.video_file\n",
        "    self.batch_size = args.bs\n",
        "    self.confidence = args.confidence\n",
        "    self.nms = args.nms\n",
        "    self.cfg_file = args.cfg_file\n",
        "    self.weights_file = args.weights_file\n",
        "    self.classes = load_dataset(args.dataset)\n",
        "    self.num_classes = len(self.classes)\n",
        "    self.colors_file = args.colors_file\n",
        "    self.CUDA = torch.cuda.is_available()\n",
        "    self.source = args.source\n",
        "  \n",
        "  def load_network(self):\n",
        "    \"\"\"\n",
        "    Setup neural network\n",
        "    \"\"\"\n",
        "    self.model = Darknet(self.cfg_file)\n",
        "    self.model.load_weight(self.weights_file)\n",
        "    self.input_dim = int(self.model.net[\"height\"])\n",
        "    assert self.input_dim % 32 == 0\n",
        "    assert self.input_dim > 32\n",
        "  \n",
        "  def get_detections(self):\n",
        "    self.load_network()\n",
        "    if self.CUDA:         # if cuda available\n",
        "      self.model.cuda()\n",
        "    \n",
        "    self.model.eval()     # set model in evaluation mode\n",
        "\n",
        "    # get video capture from source (file/webcam)\n",
        "    if self.source == \"video\":\n",
        "      cap = cv2.VideoCapture(self.video_file)\n",
        "    else:\n",
        "      cap = cv2.VideoCapture(0)   # webcam\n",
        "    assert cap.isOpened(), 'Cannot captutre video source'\n",
        "    \n",
        "    frames = 0\n",
        "    start = time.time()\n",
        "    while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "\n",
        "      if ret:\n",
        "        image = pre_image(frame, self.input_dim)\n",
        "        img_dim = frame.shape[1], frame.shape[0]\n",
        "        img_dim = torch.FloatTensor(img_dim).repeat(1, 2)\n",
        "\n",
        "        if self.CUDA:\n",
        "          img_dim = img_dim.cuda()\n",
        "          image = image.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          prediction = self.model(Variable(image))\n",
        "\n",
        "        prediction = get_result(prediction, self.confidence, self.num_classes, nms_conf=self.nms)\n",
        "        if type(prediction) == int:\n",
        "          frames += 1\n",
        "          print(\"FPS: {:5.4f}\".format(frames / (time.time() - start)))\n",
        "          # cv2.imshow(\"frame\", frame)\n",
        "          cv2_imshow(frame)\n",
        "          key = cv2.waitKey(1)\n",
        "          if key & 0xFF == ord('q'):    # exit if press q\n",
        "            break\n",
        "          continue\n",
        "        \n",
        "        img_dim = img_dim.repeat(prediction.size(0), 1)\n",
        "        scale_factor = torch.min(self.input_dim / img_dim, 1)[0].view(-1, 1)\n",
        "        prediction[:, [1, 3]] -= (self.input_dim - scale_factor * img_dim[:, 0].view(-1, 1)) / 2\n",
        "        prediction[:, [2, 4]] -= (self.input_dim - scale_factor * img_dim[:, 1].view(-1, 1)) / 2\n",
        "        prediction[:, 1: 5] /= scale_factor\n",
        "\n",
        "        for i in range(prediction.shape[0]):\n",
        "          prediction[i, [1, 3]] = torch.clamp(prediction[i, [1, 3]], 0.0, img_dim[i, 0])\n",
        "          prediction[i, [2, 4]] = torch.clamp(prediction[i, [2, 4]], 0.0, img_dim[i, 1])\n",
        "        \n",
        "        list(map(lambda x: draw_result(x, frame, self.colors, self.classes), prediction))\n",
        "        # cv2.imshow(\"frame\", frame)\n",
        "        cv2_imshow(frame)\n",
        "        key = cv2.waitKey(1)\n",
        "        if key & 0xFF == ord('q'):\n",
        "          break\n",
        "        frames += 1\n",
        "        t = time.time() - start\n",
        "        print(\"Predicted in {1:6.3f} seconds\".format(t))\n",
        "        print(\"FPS: {:5.2f}\".format(frames / (time.time() - start)))\n",
        "      else:\n",
        "        break\n",
        "\n",
        "test = VideoDetect()\n",
        "test.get_detections()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfuYTDiZZHRV"
      },
      "source": [
        "# Training Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpkPM8Vco_N2"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, images_path, image_size, max_objects=100, multiscale=True, transform=None, quick=False):\n",
        "    with open(images_path, \"r\") as file:\n",
        "      self.image_files = [name.rstrip() for name in file.readlines()]\n",
        "\n",
        "    self.label_files = [\n",
        "      path.replace(\"images\", \"labels\").replace(\".png\", \".txt\").replace(\".jpg\", \".txt\")\n",
        "      for path in self.image_files\n",
        "    ]\n",
        "\n",
        "    if quick:\n",
        "      self.image_files = self.image_files[:1000]\n",
        "\n",
        "    self.image_size = image_size\n",
        "    self.max_objects = max_objects\n",
        "    self.multiscale = multiscale\n",
        "    self.min_size = self.image_size - 3 * 32\n",
        "    self.max_size = self.image_size + 3 * 32\n",
        "    self.batch_count = 0\n",
        "    self.transform = transform\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    try:\n",
        "      image_path = self.image_files[index % len(self.image_files)].rstrip()\n",
        "      image = np.array(Image.open(image_path).convert('RGB'), dtype=np.uint8)\n",
        "    except Exception:\n",
        "      print(f\"Cannot read image '{image_path}'.\")\n",
        "\n",
        "    try:\n",
        "      label_path = self.label_files[index % len(self.image_files)].rstrip()\n",
        "      with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        boxes = np.loadtxt(label_path).reshape(-1, 5)\n",
        "    except Exception:\n",
        "      print(f\"Cannot read label '{label_path}'.\")\n",
        "      return\n",
        "  \n",
        "    if self.transform:\n",
        "      try:\n",
        "        image, targets = self.transform((image, boxes))\n",
        "      except Exception:\n",
        "        print(\"Cannot apply transform.\")\n",
        "        return\n",
        "    \n",
        "    return image_path, image, targets\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    self.batch_count += 1\n",
        "\n",
        "    # Drop invalid images\n",
        "    batch = [data for data in batch if data is not None]\n",
        "\n",
        "    paths, imgs, targets = list(zip(*batch))\n",
        "\n",
        "    # Selects new image size every tenth batch\n",
        "    if self.multiscale and self.batch_count % 10 == 0:\n",
        "      self.image_size = random.choice(\n",
        "          range(self.min_size, self.max_size + 1, 32))\n",
        "\n",
        "    # Resize images to input shape\n",
        "    imgs = torch.stack([resize(img, self.image_size) for img in imgs])\n",
        "\n",
        "    # Add sample index to targets\n",
        "    for i, boxes in enumerate(targets):\n",
        "      boxes[:, 0] = i\n",
        "    targets = torch.cat(targets, 0)\n",
        "\n",
        "    return paths, imgs, targets\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_files)\n",
        "\n",
        "def resize(image, size):\n",
        "  image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n",
        "  return image"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiUd6sr-N8tX"
      },
      "source": [
        "import imgaug.augmenters as iaa\n",
        "import torch\n",
        "import numpy as np\n",
        "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
        "import torchvision.transforms as transforms\n",
        "from dataclasses import dataclass\n",
        "\n",
        "def xywh2xyxy_np(x):\n",
        "  y = np.zeros_like(x)\n",
        "  y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
        "  y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
        "  y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
        "  y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
        "  return y\n",
        "\n",
        "class ImageAugmenter(object):\n",
        "  def __init__(self, augmentations=[]):\n",
        "    self.augmentations = augmentations\n",
        "\n",
        "  def __call__(self, data):\n",
        "    image, boxes = data\n",
        "    # Convert xywh to xyxy\n",
        "    boxes = np.array(boxes)\n",
        "    boxes[:, 1:] = xywh2xyxy_np(boxes[:, 1:])\n",
        "\n",
        "    bounding_boxes = BoundingBoxesOnImage([BoundingBox(*box[1:], label=box[0]) for box in boxes], shape=image.shape)\n",
        "    image, bounding_boxes = self.augmentations(image=image, bounding_boxes=bounding_boxes)\n",
        "    bounding_boxes = bounding_boxes.clip_out_of_image()\n",
        "    boxes = np.zeros((len(bounding_boxes), 5))\n",
        "    for i, box in enumerate(bounding_boxes):\n",
        "      x1 = box.x1\n",
        "      y1 = box.y1\n",
        "      x2 = box.x2\n",
        "      y2 = box.y2\n",
        "\n",
        "      # (x, y, w, h)\n",
        "      boxes[i, 0] = box.label\n",
        "      boxes[i, 1] = (x1 + x2) / 2\n",
        "      boxes[i, 2] = (y1 + y2) / 2\n",
        "      boxes[i, 3] = x2 - x1\n",
        "      boxes[i, 4] = y2 - y1\n",
        "    \n",
        "    return image, boxes\n",
        "\n",
        "class RelativeLabels(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, data):\n",
        "    image, boxes = data\n",
        "    h, w, _ = image.shape\n",
        "    boxes[:, [1, 3]] /= w\n",
        "    boxes[:, [2, 4]] /= h\n",
        "    return image, boxes\n",
        "\n",
        "class AbsoluteLabels(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, data):\n",
        "    image, boxes = data\n",
        "    h, w, _ = image.shape\n",
        "    boxes[:, [1, 3]] *= w\n",
        "    boxes[:, [2, 4]] *= h\n",
        "    return image, boxes\n",
        "\n",
        "class PadSquare(ImageAugmenter):\n",
        "  def __init__(self):\n",
        "    self.augmentations = iaa.Sequential([\n",
        "      iaa.PadToAspectRatio(\n",
        "        1.0,\n",
        "        position=\"center-center\").to_deterministic()\n",
        "    ])\n",
        "\n",
        "class ToTensor(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, data):\n",
        "    image, boxes = data\n",
        "    # Extract image as PyTorch tensor\n",
        "    image = transforms.ToTensor()(image)\n",
        "\n",
        "    targets = torch.zeros((len(boxes), 6))\n",
        "    targets[:, 1:] = transforms.ToTensor()(boxes)\n",
        "\n",
        "    return image, targets\n",
        "\n",
        "class DefaultAugmenter(ImageAugmenter):\n",
        "  def __init__(self):\n",
        "    self.augmentations = iaa.Sequential([\n",
        "      iaa.Sharpen((0.0, 0.1)),\n",
        "      iaa.Affine(rotate=(-0, 0), translate_percent=(-0.1, 0.1), scale=(0.8, 1.5)),\n",
        "      iaa.AddToBrightness((-60, 40)),\n",
        "      iaa.AddToHue((-10, 10)),\n",
        "      iaa.Fliplr(0.5),\n",
        "    ])\n",
        "@dataclass\n",
        "class Transform:\n",
        "  train =  transforms.Compose([\n",
        "    AbsoluteLabels(),\n",
        "    PadSquare(),\n",
        "    RelativeLabels(),\n",
        "    ToTensor(),\n",
        "  ])\n",
        "\n",
        "  val = transforms.Compose([\n",
        "    AbsoluteLabels(),\n",
        "    DefaultAugmenter(),\n",
        "    PadSquare(),\n",
        "    RelativeLabels(),\n",
        "    ToTensor(),\n",
        "  ])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwYKZScuV0T2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358,
          "referenced_widgets": [
            "6940b8f2969647c78aa030aa6202d53a",
            "0471f986d78a4c2fa4c18d3e71ac393b",
            "5e6dac75524e406da66f096ce033383c",
            "62701191bc01484b8a1cb5f78b159d7d",
            "1fae7a19aa274bd98dd708818ca5ce02",
            "3ae4238878cd48608dcefd0b616a2388",
            "fad3747418404e5c9def55adfb820389",
            "773139c79bb741ae83214e704f51c5fe",
            "b940b089b54d412b8a94978c8b6894bc",
            "0585552258ae4dfda3f59894d50bf574",
            "feadefbc35214dd6b274adc0b6035841"
          ]
        },
        "outputId": "e141b5dc-dfe8-4061-c859-d12c978d048d"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from wcmatch.pathlib import Path\n",
        "import pytorch_lightning as pl\n",
        "from datetime import datetime\n",
        "from loguru import logger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "from darknet import Darknet\n",
        "from helpers import load_dataset, save_code_files\n",
        "from train.dataset import ImageDataset, Transform\n",
        "\n",
        "def parse_arg():\n",
        "  parser = argparse.ArgumentParser(description=\"reYOLO Training Module\")\n",
        "  parser.add_argument(\"--cfg\", dest=\"cfg_file\", type=str, default=\"/content/yolov3-tiny.cfg\", help=\"Config file path\")\n",
        "  parser.add_argument(\"--dataset\", type=str, default=\"/content/coco.names\", help=\"Dataset file path\")\n",
        "  parser.add_argument(\"--train_path\", type=str, default=\"/content/data/trainvalno5k.txt\")\n",
        "  parser.add_argument(\"--valid_path\", type=str, default=\"/content/data/5k.txt\")\n",
        "  parser.add_argument(\"--nms\", default=0.4, help=\"NMS Threshold\")\n",
        "  parser.add_argument(\"--iou\", default=0.5, help=\"NMS Threshold\")\n",
        "  parser.add_argument(\"--confidence\", default=0.5, help=\"Object confidence to filter predictions\")\n",
        "  parser.add_argument(\"--epochs\", type=int, default=300, help=\"Number of epochs\")\n",
        "  parser.add_argument(\"--cpus\", type=int, default=0, help=\"Number of cpu threads during batch generation\")\n",
        "  parser.add_argument(\"--pretrained_weights\", default=\"/content/yolov3-tiny.weights\", type=str, help=\"Checkpoint file path (.weights or .pth)\")\n",
        "  parser.add_argument(\"--multiscale_train\", action=\"store_true\", help=\"Allow multi-scale training\")\n",
        "  parser.add_argument(\"--seed\", type=int, default=-1)\n",
        "  args, _ = parser.parse_known_args()\n",
        "  return args\n",
        "\n",
        "def load_model(path, device, weights=None):\n",
        "  model = Darknet(path).to(device)\n",
        "\n",
        "  if weights:\n",
        "    if weights.endswith(\".pth\"):\n",
        "      model.load_state_dict(torch.load(weights, map_location=device))\n",
        "    else:\n",
        "      model.load_weight(weights)\n",
        "  \n",
        "  return model\n",
        "\n",
        "class TrainingModule():\n",
        "  def __init__(self):\n",
        "    args = parse_arg()\n",
        "    self.seed = args.seed\n",
        "    self.classes = load_dataset(args.dataset)\n",
        "    self.train_path = args.train_path\n",
        "    self.valid_path = args.valid_path\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model = load_model(args.cfg_file, self.device, args.pretrained_weights)\n",
        "    self.multiscale_train = args.multiscale_train\n",
        "    self.cpus = args.cpus\n",
        "    self.epochs = args.epochs\n",
        "    self.mini_batch_size = int(self.model.net[\"batch\"]) // int(self.model.net[\"subdivisions\"])\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    dataset = ImageDataset(self.train_path, image_size=int(self.model.net[\"height\"]), multiscale=self.multiscale_train, transform=Transform.train, quick=True)\n",
        "    return DataLoader(dataset, batch_size=self.mini_batch_size, shuffle=True, num_workers=self.cpus, pin_memory=True, collate_fn=dataset.collate_fn)\n",
        "\n",
        "  def valid_dataloader(self):\n",
        "    dataset = ImageDataset(self.valid_path, image_size=int(self.model.net[\"height\"]), multiscale=False, transform=Transform.val, quick=True)\n",
        "    return DataLoader(dataset, batch_size=self.mini_batch_size, shuffle=False, num_workers=self.cpus, pin_memory=True, collate_fn=dataset.collate_fn)\n",
        "\n",
        "  def train(self):\n",
        "    pl.seed_everything(self.seed, workers=True)\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "      monitor=\"val_loss\",\n",
        "      dirpath=\"lightning_logs/ckpt1\",\n",
        "      filename=\"yolo-{epoch:02d}-{val_loss:.2f}\",\n",
        "      save_top_k=3,\n",
        "      mode=\"min\",\n",
        "    )\n",
        "  \n",
        "    trainer = pl.Trainer(accelerator=\"cpu\", devices=self.cpus, callbacks=[checkpoint_callback], weights_save_path=\"weights\", weights_summary=\"full\", enable_model_summary=True)\n",
        "    trainer.fit(self.model, train_dataloaders=self.train_dataloader(), val_dataloaders=self.valid_dataloader())\n",
        "    print(\"Best checkpoint: \", checkpoint_callback.best_model_path)\n",
        "    return self.model\n",
        "\n",
        "class DataModule(pl.LightningDataModule):\n",
        "  def __init__(self, train_ds, val_ds, batch_size, cpus):\n",
        "    super().__init__()\n",
        "    self.train_ds = train_ds\n",
        "    self.val_ds = val_ds\n",
        "    self.batch_size = batch_size\n",
        "    self.cpus = cpus\n",
        "  \n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True, num_workers=self.cpus, pin_memory=True, collate_fn=self.train_ds.collate_fn)\n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False, num_workers=self.cpus, pin_memory=True, collate_fn=self.val_ds.collate_fn)\n",
        "\n",
        "\n",
        "class Net(pl.LightningModule):\n",
        "  def __init__(self, model, img_size, batch_size, args):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.valid_path = args.valid_path\n",
        "    self.img_size = img_size\n",
        "    self.batch_size = batch_size\n",
        "    self.nms = args.nms\n",
        "    self.conf = args.confidence\n",
        "    self.iou = args.iou\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "  \n",
        "  def training_step(self, batch, batch_index):\n",
        "    images, targets = batch[1:]\n",
        "    cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "    loss, outputs = self.model(Variable(images.to(device)), targets)\n",
        "    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def get_progress_bar_dict(self):\n",
        "    items = super().get_progress_bar_dict()\n",
        "    items.pop(\"v_num\", None)\n",
        "    items.pop(\"loss\", None)\n",
        "    return items\n",
        "  \n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    imgs, targets = batch[1:]\n",
        "    loss = self.model(imgs, targets)\n",
        "    self.log('val_loss',loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "    if \"optimizer\" not in self.model.net or self.model.net[\"optimizer\"] == \"adam\":\n",
        "      optimizer = torch.optim.Adam(self.model.parameters(), lr=float(self.model.net[\"learning_rate\"]), weight_decay=float(self.model.net[\"decay\"]))\n",
        "    elif self.net[\"optimizer\"] == \"sgd\":\n",
        "      optimizer = torch.optim.SGD(self.model.parameters(), lr=float(self.model.net[\"learning_rate\"]), weight_decay=float(self.model.net[\"decay\"]), momentum=self.model.net[\"momentum\"])\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "  def validation_epoch_end(self, loss):\n",
        "    if self.detect:\n",
        "      metrics_output = evaluate(\n",
        "        self.model,\n",
        "        path=self.valid_path,\n",
        "        iou_thres=self.iou,\n",
        "        conf_thres=self.conf,\n",
        "        nms_thres=self.nms,\n",
        "        img_size=self.img_size,\n",
        "        batch_size=self.batch_size,\n",
        "      )\n",
        "\n",
        "      if metrics_output is not None:\n",
        "        precision, recall, AP, f1, ap_class = metrics_output\n",
        "        self.log('precision', precision.mean(), on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('recal', recall.mean(), on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('f1', f1.mean(), on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('mAP', AP.mean(), on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
        "        for i, c in enumerate(ap_class):\n",
        "            ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
        "        print(AsciiTable(ap_table).table)\n",
        "        print(f\"---- mAP {AP.mean()}\")\n",
        "      else:\n",
        "        print(\"---- mAP not measured (no detections found by model)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  level = \"DEBUG\"\n",
        "  experiment_root = 'experiments'\n",
        "  exp_root_path = Path(experiment_root)\n",
        "  wandb.login()\n",
        "  experiment_dir = exp_root_path / \"train\" / f\"exp_{datetime.now()}\"\n",
        "  log_file = experiment_dir / f\"log_{datetime.now()}.log\"\n",
        "  logger.opt(record=True).add(log_file, format=\" {time:YYYY-MMM HH:mm:ss} {name}:{function}:{line} <lvl>{message}</>\", level=level, rotation=\"5 MB\")\n",
        "  experiment_dir.mkdir(exist_ok=True)\n",
        "\n",
        "  args = parse_arg()\n",
        "  logger.opt(colors=True).info(args)\n",
        "  save_code_files(experiment_dir, os.path.abspath(''))\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  darknet = Darknet(args.cfg_file)\n",
        "  img_size = int(darknet.net[\"height\"])\n",
        "  batch_size = int(darknet.net[\"batch\"]) // int(darknet.net[\"subdivisions\"])\n",
        "  classes = load_dataset(args.dataset)\n",
        "  \n",
        "  train_ds = ImageDataset(images_path=args.train_path, multiscale=args.multiscale_train, image_size=img_size, transform=Transform.train)\n",
        "  val_ds = ImageDataset(images_path=args.valid_path, multiscale=args.multiscale_train, image_size=img_size, transform=Transform.val)\n",
        "\n",
        "  model = Net(darknet, img_size, batch_size, args)\n",
        "  model = model.to(device)\n",
        "\n",
        "  data_module = DataModule(train_ds, val_ds, batch_size, args.cpus)\n",
        "\n",
        "  wandb_logger = WandbLogger(project=\"reYOLO\", save_dir=experiment_dir, offline=False, name=\"test\")\n",
        "  checkpoint_callback = ModelCheckpoint(dirpath=experiment_dir, mode=\"min\", monitor=\"val_loss\")\n",
        "\n",
        "  logger.opt(colors=True).info(\"Start training\")\n",
        "  trainer = pl.Trainer(logger=wandb_logger, auto_scale_batch_size='binsearch', num_sanity_val_steps=0, callbacks=[checkpoint_callback], weights_save_path=\"weights\")\n",
        "  trainer.fit(model, data_module)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "2021-11-09 11:04:41.208 | INFO     | __main__:<module>:182 - Namespace(cfg_file='/content/yolov3-tiny.cfg', confidence=0.5, cpus=0, dataset='/content/coco.names', epochs=300, iou=0.5, multiscale_train=False, nms=0.4, pretrained_weights='/content/yolov3-tiny.weights', seed=-1, train_path='/content/data/trainvalno5k.txt', valid_path='/content/data/5k.txt')\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:427: UserWarning: Checkpoint directory experiments/train/exp_2021-11-09 11:04:41.194012 exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "2021-11-09 11:04:44.139 | INFO     | __main__:<module>:202 - Start training\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "\n",
            "  | Name  | Type    | Params\n",
            "----------------------------------\n",
            "0 | model | Darknet | 8.9 M \n",
            "----------------------------------\n",
            "8.9 M     Trainable params\n",
            "0         Non-trainable params\n",
            "8.9 M     Total params\n",
            "35.409    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6940b8f2969647c78aa030aa6202d53a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lqzdO6DtHMa",
        "outputId": "b39b4f76-569c-46d5-9b66-43725d55895a"
      },
      "source": [
        "!mkdir images\n",
        "!cd images\n",
        "\n",
        "# Download Images\n",
        "!wget -c \"https://pjreddie.com/media/files/train2014.zip\" --header \"Referer: pjreddie.com\"\n",
        "!unzip -q train2014.zip && rm train2014.zip\n",
        "!wget -c \"https://pjreddie.com/media/files/val2014.zip\" --header \"Referer: pjreddie.com\"\n",
        "!unzip -q val2014.zip && rm val2014.zip\n",
        "\n",
        "\n",
        "# Download COCO Metadata\n",
        "!wget -c \"https://pjreddie.com/media/files/instances_train-val2014.zip\" --header \"Referer: pjreddie.com\"\n",
        "!wget -c \"https://pjreddie.com/media/files/coco/5k.part\" --header \"Referer: pjreddie.com\"\n",
        "!wget -c \"https://pjreddie.com/media/files/coco/trainvalno5k.part\" --header \"Referer: pjreddie.com\"\n",
        "!wget -c \"https://pjreddie.com/media/files/coco/labels.tgz\" --header \"Referer: pjreddie.com\"\n",
        "!tar xzf labels.tgz\n",
        "!unzip -q instances_train-val2014.zip\n",
        "\n",
        "# Set Up Image Lists\n",
        "!paste <(awk \"{print \\\"$PWD\\\"}\" <5k.part) 5k.part | tr -d '\\t' > 5k.txt\n",
        "!paste <(awk \"{print \\\"$PWD\\\"}\" <trainvalno5k.part) trainvalno5k.part | tr -d '\\t' > trainvalno5k.txt\n",
        "\n",
        "!rm instances_train-val2014.zip 5k.part trainvalno5k.part labels.tgz\n",
        "!rm -rf sample_data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘images’: File exists\n",
            "--2021-11-09 05:02:03--  https://pjreddie.com/media/files/train2014.zip\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 206 Partial Content\n",
            "Length: 13510435630 (13G), 12900000861 (12G) remaining [application/zip]\n",
            "Saving to: ‘train2014.zip’\n",
            "\n",
            "train2014.zip       100%[===================>]  12.58G  44.1MB/s    in 4m 48s  \n",
            "\n",
            "2021-11-09 05:06:51 (42.7 MB/s) - ‘train2014.zip’ saved [13510435630/13510435630]\n",
            "\n",
            "--2021-11-09 05:11:14--  https://pjreddie.com/media/files/val2014.zip\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645013297 (6.2G) [application/zip]\n",
            "Saving to: ‘val2014.zip’\n",
            "\n",
            "val2014.zip         100%[===================>]   6.19G  41.5MB/s    in 2m 46s  \n",
            "\n",
            "2021-11-09 05:14:00 (38.3 MB/s) - ‘val2014.zip’ saved [6645013297/6645013297]\n",
            "\n",
            "--2021-11-09 05:15:46--  https://pjreddie.com/media/files/instances_train-val2014.zip\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 165168220 (158M) [application/zip]\n",
            "Saving to: ‘instances_train-val2014.zip’\n",
            "\n",
            "instances_train-val 100%[===================>] 157.52M  38.2MB/s    in 4.4s    \n",
            "\n",
            "2021-11-09 05:15:51 (36.0 MB/s) - ‘instances_train-val2014.zip’ saved [165168220/165168220]\n",
            "\n",
            "--2021-11-09 05:15:51--  https://pjreddie.com/media/files/coco/5k.part\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 230000 (225K) [application/octet-stream]\n",
            "Saving to: ‘5k.part’\n",
            "\n",
            "5k.part             100%[===================>] 224.61K  1.17MB/s    in 0.2s    \n",
            "\n",
            "2021-11-09 05:15:51 (1.17 MB/s) - ‘5k.part’ saved [230000/230000]\n",
            "\n",
            "--2021-11-09 05:15:51--  https://pjreddie.com/media/files/coco/trainvalno5k.part\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5722468 (5.5M) [application/octet-stream]\n",
            "Saving to: ‘trainvalno5k.part’\n",
            "\n",
            "trainvalno5k.part   100%[===================>]   5.46M  10.1MB/s    in 0.5s    \n",
            "\n",
            "2021-11-09 05:15:52 (10.1 MB/s) - ‘trainvalno5k.part’ saved [5722468/5722468]\n",
            "\n",
            "--2021-11-09 05:15:52--  https://pjreddie.com/media/files/coco/labels.tgz\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17940023 (17M) [application/octet-stream]\n",
            "Saving to: ‘labels.tgz’\n",
            "\n",
            "labels.tgz          100%[===================>]  17.11M  22.0MB/s    in 0.8s    \n",
            "\n",
            "2021-11-09 05:15:53 (22.0 MB/s) - ‘labels.tgz’ saved [17940023/17940023]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}