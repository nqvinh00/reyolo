{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reyolo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2819cb10aaf84bc5b12f88f08cfabf85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fae895336b2344938d8ba4ccaca0e223",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d1bfeaedafe449379519191c60aa7b57",
              "IPY_MODEL_439172cb93b2418ab0464534c3447641",
              "IPY_MODEL_cbfba78e84044780843e46913d01459a"
            ]
          }
        },
        "fae895336b2344938d8ba4ccaca0e223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "d1bfeaedafe449379519191c60aa7b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a3d1c21650d84305ab2aaac68366700f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Validation sanity check:   0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db5a2f78200c4465a37101a25b9a8374"
          }
        },
        "439172cb93b2418ab0464534c3447641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_144fd1c6fafa43e9be88b711d12bdb89",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cababaf6e2b342bdbbe978302eee23bf"
          }
        },
        "cbfba78e84044780843e46913d01459a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_950b67892dab4996a363735479c3258b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/2 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28b35294ab27463fbe08ee113c1a5c1c"
          }
        },
        "a3d1c21650d84305ab2aaac68366700f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db5a2f78200c4465a37101a25b9a8374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "144fd1c6fafa43e9be88b711d12bdb89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cababaf6e2b342bdbbe978302eee23bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "950b67892dab4996a363735479c3258b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28b35294ab27463fbe08ee113c1a5c1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUql6KyLZROt"
      },
      "source": [
        "# Detection Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GD9M5U4ZMDn",
        "outputId": "2209e6a5-0b5c-4d19-9f23-ee13c725cf91"
      },
      "source": [
        "pip install pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.5.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 12.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.7.4.3)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[K     |████████████████████████████████| 329 kB 20.9 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 27.4 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 62.1 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.3)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.41.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 64.9 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 48.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 49.4 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.7)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.0-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.6.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=4e357f772c19accf4d2df1b58b179412263952d2b4492c6d5c7061ade6a25026\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.0 aiosignal-1.2.0 async-timeout-4.0.0 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.0 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.0 torchmetrics-0.6.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OydlIuq8WD4I"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import cv2 \n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJUwz7yebSo"
      },
      "source": [
        "class EmptyLayer(pl.LightningModule):\n",
        "  \"\"\"\n",
        "  Use for route module\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(EmptyLayer, self).__init__()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEHTy5XagBNO"
      },
      "source": [
        "class DetectionLayer(pl.LightningModule):\n",
        "  \"\"\"\n",
        "  Use for yolo module\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, anchors):\n",
        "    super(DetectionLayer, self).__init__()\n",
        "    self.anchors = anchors"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyz8LNnPK17e"
      },
      "source": [
        "class Darknet(pl.LightningModule):\n",
        "  def __init__(self, cfg_file):\n",
        "    super(Darknet, self).__init__()\n",
        "    self.blocks = parse_cfg(cfg_file)\n",
        "    self.net, self.module_list = create_modules(self.blocks)\n",
        "\n",
        "  def forward(self, x, CUDA=None):\n",
        "    \"\"\"\n",
        "    Calculate the output\n",
        "    Transform the output detection feature maps in a vay can be processed easier\n",
        "    \"\"\"\n",
        "    modules = self.blocks[1:] # skip first element of blocks, which is net info\n",
        "    outputs = {}\n",
        "    check = 0\n",
        "\n",
        "    for i, module in enumerate(modules):        \n",
        "      module_type = (module[\"type\"])\n",
        "      \n",
        "      if module_type == \"convolutional\" or module_type == \"upsample\" or module_type==\"maxpool\":\n",
        "        x = self.module_list[i](x)\n",
        "      elif module_type == \"route\":\n",
        "        layers = module[\"layers\"]\n",
        "        layers = [int(a) for a in layers]\n",
        "\n",
        "        if layers[0] > 0:\n",
        "            layers[0] -= i\n",
        "\n",
        "        if len(layers) == 1:\n",
        "            x = outputs[i + layers[0]]\n",
        "        else:\n",
        "            if layers[1] > 0:\n",
        "                layers[1] -= i\n",
        "\n",
        "            map1 = outputs[i + layers[0]]\n",
        "            map2 = outputs[i + layers[1]]\n",
        "            x = torch.cat((map1, map2), 1)\n",
        "      elif  module_type == \"shortcut\":\n",
        "          f = int(module[\"from\"])\n",
        "          x = outputs[i - 1] + outputs[i + f]\n",
        "      elif module_type == \"yolo\":        \n",
        "          anchors = self.module_list[i][0].anchors   # anchors\n",
        "          input_dim = int (self.net[\"height\"])       # input dimension\n",
        "          num_classes = int (module[\"classes\"])      # number of classes\n",
        "  \n",
        "          # transform \n",
        "          x = x.data\n",
        "          x = predict_transform(x, input_dim, anchors, num_classes, CUDA)\n",
        "          if not check:\n",
        "              detections = x\n",
        "              check = 1\n",
        "          else:       \n",
        "              detections = torch.cat((detections, x), 1)\n",
        "      outputs[i] = x\n",
        "    \n",
        "    return detections\n",
        "\n",
        "  def training_step(self, batch, batch_index):\n",
        "    x, y = batch\n",
        "    logits = self.forward(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    self.log(\"train_loss\", loss)\n",
        "    return loss\n",
        "  \n",
        "  def validation_step(self, batch, batch_index):\n",
        "    x, y = batch\n",
        "    logits = self.forward(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    _, prediction = torch.max(logits, dim=1)\n",
        "    self.log(\"val_loss\", loss)\n",
        "    self.log(\"val_acc\", (prediction == t).sum() / len(y))\n",
        "    return prediction\n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "    if \"optimizer\" not in self.net or self.net[\"optimizer\"] == \"adam\":\n",
        "      optimizer = torch.optim.Adam(self.parameters(), lr=float(self.net[\"learning_rate\"]), weight_decay=float(self.net[\"decay\"]))\n",
        "    elif self.net[\"optimizer\"] == \"sgd\":\n",
        "      optimizer = torch.optim.SGD(self.parameters(), lr=float(self.net[\"learning_rate\"]), weight_decay=float(self.net[\"decay\"]), momentum=self.net[\"momentum\"])\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "  def load_weight(self, file_path):\n",
        "    file = open(file_path, \"rb\")\n",
        "\n",
        "    # first 5 items in weight file are header information\n",
        "    # major ver, minor ver, subversion, images seen by the network\n",
        "    header = np.fromfile(file, dtype=np.int32, count=5)\n",
        "    self.header = torch.from_numpy(header)\n",
        "    self.network_seen = self.header[3]\n",
        "    weights = np.fromfile(file, dtype=np.float32)\n",
        "\n",
        "    n = 0\n",
        "    for i in range(len(self.module_list)):\n",
        "      module_type = self.blocks[i + 1][\"type\"]\n",
        "      # if not convolutional, ignore\n",
        "      if module_type == \"convolutional\":\n",
        "        module = self.module_list[i]\n",
        "        try:\n",
        "          batch_normalize = int(self.blocks[i + 1][\"batch_normalize\"])\n",
        "        except:\n",
        "          batch_normalize = 0\n",
        "        \n",
        "        convol_layer = module[0]\n",
        "\n",
        "        # batch normalize layer\n",
        "        if batch_normalize:\n",
        "          batch_norm_layer = module[1]\n",
        "          num_biases = batch_norm_layer.bias.numel()\n",
        "          \n",
        "          # load weights\n",
        "          bnl_biases = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          bnl_weights = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          bnl_running_mean = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          bnl_running_var = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          # cast weights into dimensions of model weights\n",
        "          bnl_biases = bnl_biases.view_as(batch_norm_layer.bias.data)\n",
        "          bnl_weights = bnl_weights.view_as(batch_norm_layer.weight.data)\n",
        "          bnl_running_mean = bnl_running_mean.view_as(batch_norm_layer.running_mean)\n",
        "          bnl_running_var = bnl_running_var.view_as(batch_norm_layer.running_var)\n",
        "\n",
        "          # copy data to model\n",
        "          batch_norm_layer.bias.data.copy_(bnl_biases)\n",
        "          batch_norm_layer.weight.data.copy_(bnl_weights)\n",
        "          batch_norm_layer.running_mean.copy_(bnl_running_mean)\n",
        "          batch_norm_layer.running_var.copy_(bnl_running_var)\n",
        "        else:     # convolutional layer\n",
        "          num_biases = convol_layer.bias.numel()\n",
        "\n",
        "          # load weights\n",
        "          convol_biases = torch.from_numpy(weights[n: n + num_biases])\n",
        "          n += num_biases\n",
        "\n",
        "          # cast weights into dimensions of model weights\n",
        "          convol_biases = convol_biases.view_as(convol_layer.bias.data)\n",
        "\n",
        "          # copy data to model\n",
        "          convol_layer.bias.data.copy_(convol_biases)\n",
        "        \n",
        "        # weights of convolutional layerss\n",
        "        num_weights = convol_layer.weight.numel()\n",
        "        convol_weights = torch.from_numpy(weights[n: n + num_weights])\n",
        "        n += num_weights\n",
        "        convol_weights = convol_weights.view_as(convol_layer.weight.data)\n",
        "        convol_layer.weight.data.copy_(convol_weights)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCFJA6nVoHN5"
      },
      "source": [
        "def predict_transform(predict, input_dim, anchors, num_classes, CUDA=False):\n",
        "  \"\"\"\n",
        "  Transfer input (which is output of forward()) into 2d tensor.\n",
        "  Each row of the tensor corresponds to attributes of a bounding box.\n",
        "  \"\"\"\n",
        "\n",
        "  batch_size = predict.size(0)\n",
        "  stride = input_dim // predict.size(2)\n",
        "  grid_size = input_dim // stride\n",
        "  bounding_box_attrs = num_classes + 5\n",
        "\n",
        "  predict = predict.view(batch_size, bounding_box_attrs * len(anchors), grid_size ** 2)\n",
        "  predict = predict.transpose(1,2).contiguous()\n",
        "  predict = predict.view(batch_size, grid_size ** 2 * len(anchors), bounding_box_attrs)\n",
        "\n",
        "  # dimensions of anchors are in accordance to height and width attr of net block\n",
        "  anchors = [(a[0] / stride, a[1] / stride) for a in anchors]\n",
        "\n",
        "  # sigmoid x, y coordinates and objectness score\n",
        "  # center_x, center_y, object_confidence\n",
        "  predict[:, :, 0] = torch.sigmoid(predict[:, :, 0])\n",
        "  predict[:, :, 1] = torch.sigmoid(predict[:, :, 1])\n",
        "  predict[:, :, 4] = torch.sigmoid(predict[:, :, 4])\n",
        "\n",
        "  # add center offsets\n",
        "  grid = np.arange(grid_size)\n",
        "  x, y = np.meshgrid(grid, grid)\n",
        "  x_offset = torch.FloatTensor(x).view(-1, 1)\n",
        "  y_offset = torch.FloatTensor(y).view(-1, 1)\n",
        "\n",
        "\n",
        "  if CUDA:\n",
        "    x_offset = x_offset.cuda()\n",
        "    y_offset = y_offset.cuda()\n",
        "    # anchors = anchors.cuda()\n",
        "  \n",
        "  xy_offset = torch.cat((x_offset, y_offset), 1).repeat(1, len(anchors)).view(-1, 2).unsqueeze(0)\n",
        "  predict[:, :, :2] += xy_offset\n",
        "\n",
        "  # apply anchors to dimensions of bounding box\n",
        "  anchors = torch.FloatTensor(anchors)\n",
        "  if CUDA:\n",
        "    anchors = anchors.cuda()\n",
        "\n",
        "  anchors = anchors.repeat(grid_size ** 2, 1).unsqueeze(0)\n",
        "\n",
        "  predict[:, :, 2: 4] = torch.exp(predict[:, :, 2: 4]) * anchors\n",
        "  # apply sigmoid to class scores\n",
        "  predict[:, :, 5: num_classes + 5] = torch.sigmoid(predict[:, :, 5: num_classes + 5])\n",
        "  # resize detections map to size of input image\n",
        "  predict[:, :, :4] *= stride\n",
        "\n",
        "  return predict"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLuHRI-uWlx9"
      },
      "source": [
        "def parse_cfg(file):\n",
        "  \"\"\"\n",
        "  Parse config from file. Returns a list of blocks.\n",
        "  Each blocks describes a block in neural network to be built.\n",
        "  \"\"\"\n",
        "\n",
        "  file = open(file, 'r')\n",
        "  lines = file.read().split('\\n')\n",
        "  lines = [l for l in lines if len(l) > 0]\n",
        "  lines = [l for l in lines if l[0] != '#']\n",
        "  lines = [l.rstrip().lstrip() for l in lines]\n",
        "\n",
        "  b = {}\n",
        "  blocks = []\n",
        "\n",
        "  for l in lines:\n",
        "    if l[0] == \"[\":                 # Check for new block\n",
        "      if len(b) != 0:               # Check if block not empty\n",
        "        blocks.append(b)\n",
        "        b = {}\n",
        "      b[\"type\"] = l[1:-1].rstrip()\n",
        "    else:\n",
        "      key, value = l.split(\"=\")     # get key-value from line\n",
        "      b[key.rstrip()] = value.lstrip()\n",
        "\n",
        "  blocks.append(b)\n",
        "  return blocks"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3FOSgxGWwJb"
      },
      "source": [
        "def create_modules(blocks):\n",
        "  net = blocks[0]                  # net info about the input and pre-processing\n",
        "  modules = nn.ModuleList()\n",
        "  in_channels = 3\n",
        "  output_filters = []\n",
        "\n",
        "  for i, x in enumerate(blocks[1:]):\n",
        "    module = nn.Sequential()\n",
        "    module_type = x[\"type\"]\n",
        "\n",
        "    # check type of block\n",
        "    # create new module for block\n",
        "    # append to module list (modules variable)\n",
        "    if module_type == \"convolutional\":\n",
        "      activation = x[\"activation\"]\n",
        "      try:\n",
        "          batch_normalize = int(x[\"batch_normalize\"])\n",
        "          bias = False\n",
        "      except:\n",
        "          batch_normalize = 0\n",
        "          bias = True\n",
        "      \n",
        "      filters = int(x[\"filters\"])\n",
        "      padding = int(x[\"pad\"])\n",
        "      kernel_size = int(x[\"size\"])\n",
        "      stride = int(x[\"stride\"])\n",
        "\n",
        "      if padding:\n",
        "        pad = (kernel_size - 1) // 2\n",
        "      else:\n",
        "        pad = 0\n",
        "      \n",
        "      # convolutional layer\n",
        "      convol_layer = nn.Conv2d(\n",
        "          in_channels=in_channels,\n",
        "          out_channels=filters,\n",
        "          kernel_size=kernel_size,\n",
        "          stride=stride,\n",
        "          padding=pad,\n",
        "          bias=bias\n",
        "      )\n",
        "      module.add_module(\"conv_{}\".format(i), convol_layer)\n",
        "\n",
        "      # batch norm layer\n",
        "      if batch_normalize:\n",
        "          batch_norm_layer = nn.BatchNorm2d(num_features=filters)\n",
        "          module.add_module(\"batch_norm_{}\".format(i), batch_norm_layer)\n",
        "      \n",
        "      if activation == \"leaky\":      # linear or leaky relu for yolo\n",
        "          leaky_layer = nn.LeakyReLU(0.1, inplace=True)\n",
        "          module.add_module(\"leaky_{}\".format(i), leaky_layer)\n",
        "    # maxpool layers\n",
        "    elif module_type == \"maxpool\":\n",
        "      kernel_size = int(x[\"size\"])\n",
        "      stride = int(x[\"stride\"])\n",
        "\n",
        "      maxpool = nn.MaxPool2d(\n",
        "          kernel_size=kernel_size,\n",
        "          stride=stride,\n",
        "          padding=int((kernel_size - 1) // 2)\n",
        "      )\n",
        "\n",
        "      if kernel_size == 2 and stride == 1:\n",
        "        module.add_module('ZeroPad2d',nn.ZeroPad2d((0, 1, 0, 1)))\n",
        "        module.add_module('MaxPool2d',maxpool)\n",
        "      else:\n",
        "        module = maxpool\n",
        "    # unsample layers\n",
        "    elif module_type == \"upsample\":\n",
        "      stride = int(x[\"stride\"])\n",
        "      upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
        "      module.add_module(\"upsample_{}\".format(i), upsample)\n",
        "    # route layer\n",
        "    elif module_type == \"route\":\n",
        "      x[\"layers\"] = x[\"layers\"].split(\",\")\n",
        "      start = int(x[\"layers\"][0])\n",
        "      try:\n",
        "        end = int(x[\"layers\"][1])\n",
        "      except:\n",
        "        end = 0\n",
        "      \n",
        "      if start > 0:\n",
        "        start -= i\n",
        "      if end > 0:\n",
        "        end -= i\n",
        "\n",
        "      route = EmptyLayer()\n",
        "      module.add_module(\"route_{}\".format(i), route)\n",
        "      if end < 0:\n",
        "        filters = output_filters[i + start] + output_filters[i + end]\n",
        "      else:\n",
        "        filters = output_filters[i + start]\n",
        "    # shortcut\n",
        "    elif module_type == \"shortcut\":\n",
        "      shortcut = EmptyLayer()\n",
        "      module.add_module(\"shortcut_{}\".format(i), shortcut)\n",
        "    # yolo: detection layer\n",
        "    elif module_type == \"yolo\":\n",
        "      mask = x[\"mask\"].split(\",\")\n",
        "      mask = [int(m) for m in mask]\n",
        "\n",
        "      anchors = x[\"anchors\"].split(\",\")\n",
        "      anchors = [int(a) for a in anchors]\n",
        "      anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
        "      anchors = [anchors[m] for m in mask]\n",
        "\n",
        "      detection = DetectionLayer(anchors)\n",
        "      module.add_module(\"Detection_{}\".format(i), detection)\n",
        "\n",
        "    modules.append(module)\n",
        "    in_channels = filters\n",
        "    output_filters.append(filters)\n",
        "  return (net, modules)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F7cDfhlY7mw"
      },
      "source": [
        "def test_input(file_path, img_size):\n",
        "    img = cv2.imread(file_path)\n",
        "    img = cv2.resize(img, img_size)\n",
        "    img_result = img[:, :, ::-1].transpose((2, 0, 1))     # BGR -> RGB\n",
        "    img_result = img_result[np.newaxis, :, :, :]/255.0    # Add a channel at 0\n",
        "    img_result = torch.from_numpy(img_result).float()     # Convert to float\n",
        "    img_result = Variable(img_result)                     # Convert to Variable\n",
        "    return img_result"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3IXXibifjgQ"
      },
      "source": [
        "def get_result(prediction, confidence, num_classes, nms_conf=0.4):\n",
        "  # object confidence thresholding\n",
        "  # each bounding box having objectness score below a threshold\n",
        "  # set the value of entrie row representing the bounding box to zero\n",
        "  conf_mask = (prediction[:, :, 4] > confidence).float().unsqueeze(2)\n",
        "  prediction *= conf_mask\n",
        "\n",
        "  # transform center_x, center_y, height, width of box\n",
        "  # to top_left_corner_x, top_right_corner_y, right_bottom_corner_x, right_bottom_corner_y \n",
        "  box = prediction.new(prediction.shape)\n",
        "  box[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
        "  box[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
        "  box[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
        "  box[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
        "  prediction[:, :, :4] = box[:, :, :4]\n",
        "\n",
        "  batch_size = prediction.size(0)\n",
        "  check = False\n",
        "\n",
        "  # the number of true detections in every image may be different\n",
        "  # confidence thresholding and nms has to be done for one image at conce\n",
        "  # must loop over the 1st dimension of prediction\n",
        "  for i in range(batch_size):\n",
        "    image_prediction = prediction[i]      # image tensor\n",
        "\n",
        "    # each bounding box has 85 attri\n",
        "    # 80 attri are class scores\n",
        "    max_confidence, max_confidence_score = torch.max(image_prediction[:, 5: num_classes + 5], 1)\n",
        "    max_confidence = max_confidence.float().unsqueeze(1)\n",
        "    max_confidence_score = max_confidence_score.float().unsqueeze(1)\n",
        "    image_prediction = torch.cat((image_prediction[:, :5], max_confidence, max_confidence_score), 1)\n",
        "\n",
        "    non_zero = torch.nonzero(image_prediction[:, 4])\n",
        "    try:\n",
        "      image_prediction_ = image_prediction[non_zero.squeeze(), :].view(-1, 7)\n",
        "    except:\n",
        "      continue\n",
        "    \n",
        "    if image_prediction_.shape[0] == 0:\n",
        "      continue\n",
        "    \n",
        "    # get various classes detected in image\n",
        "    image_classes = get_unique(image_prediction_[:, -1])\n",
        "\n",
        "    for c in image_classes:\n",
        "      # nms\n",
        "      # get detections with 1 particular class\n",
        "      class_mask = image_prediction_ * (image_prediction_[:, -1] == c).float().unsqueeze(1)\n",
        "      class_mask_index = torch.nonzero(class_mask[:, -2]).squeeze()\n",
        "      image_prediction_class = image_prediction_[class_mask_index].view(-1, 7)\n",
        "\n",
        "      # sort detection\n",
        "      # confidence at top\n",
        "      confidence_sorted_index = torch.sort(image_prediction_class[:, 4], descending=True)[1]\n",
        "      image_prediction_class = image_prediction_class[confidence_sorted_index]\n",
        "      index = image_prediction_class.size(0)\n",
        "\n",
        "      for idx in range(index):\n",
        "        # get ious of all boxes\n",
        "        try:\n",
        "          ious = get_bounding_boxes_iou(image_prediction_class[idx].unsqueeze(0), image_prediction_class[idx + 1:])\n",
        "        except ValueError:\n",
        "          break\n",
        "        except IndexError:\n",
        "          break\n",
        "        \n",
        "        # mark zero all detections iou > threshold\n",
        "        iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
        "        image_prediction_class[idx + 1:] *= iou_mask\n",
        "\n",
        "        # remove non-zero entries\n",
        "        non_zero_index = torch.nonzero(image_prediction_class[:, 4]).squeeze()\n",
        "        image_prediction_class = image_prediction_class[non_zero_index].view(-1, 7)\n",
        "      \n",
        "      batch_index = image_prediction_class.new(image_prediction_class.size(0), 1).fill_(i)\n",
        "      s = batch_index, image_prediction_class\n",
        "\n",
        "      if not check:\n",
        "        output = torch.cat(s, 1)\n",
        "        check = True\n",
        "      else:\n",
        "        output = torch.cat((output, torch.cat(s, 1)))\n",
        "      \n",
        "  try:\n",
        "    return output\n",
        "  except:\n",
        "    return 0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvMj1fcDmys2"
      },
      "source": [
        "def get_unique(tensor):\n",
        "  np_tensor = tensor.cpu().numpy()\n",
        "  unique = np.unique(np_tensor)\n",
        "  unique_tensor = torch.from_numpy(unique)\n",
        "  result = tensor.new(unique_tensor.shape)\n",
        "  result.copy_(unique_tensor)\n",
        "\n",
        "  return result"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L1EL5yfqAQt"
      },
      "source": [
        "def get_bounding_boxes_iou(b1, b2):\n",
        "  \"\"\"\n",
        "  Returns iou of 2 bouding boxes\n",
        "  \"\"\"\n",
        "\n",
        "  # get coordinates of 2 bounding boxes\n",
        "  b1_x1, b1_y1, b1_x2, b1_y2 = b1[:, 0], b1[:, 1], b1[:, 2], b1[:, 3]\n",
        "  b2_x1, b2_y1, b2_x2, b2_y2 = b2[:, 0], b2[:, 1], b2[:, 2], b2[:, 3]\n",
        "\n",
        "  # get coordinates of overclap rectangle\n",
        "  x1 = torch.max(b1_x1, b2_x1)\n",
        "  y1 = torch.max(b1_y1, b2_y1)\n",
        "  x2 = torch.min(b1_x2, b2_x2)\n",
        "  y2 = torch.min(b1_y2, b2_y2)\n",
        "\n",
        "  # overclap area\n",
        "  area = torch.clamp(x2 - x1 + 1, min=0) * torch.clamp(y2 - y1 + 1, min=0)\n",
        "\n",
        "  # union area\n",
        "  b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
        "  b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
        "\n",
        "  return area / (b1_area + b2_area - area)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buiHVHnpUPZY"
      },
      "source": [
        "def resize_image(img, input_dim):\n",
        "    \"\"\"\n",
        "    resize image with unchanged aspect ratio using padding\n",
        "    \"\"\"\n",
        "    width, height = img.shape[1], img.shape[0]\n",
        "    w, h = input_dim\n",
        "    new_width = int(width * min(w / width, h / height))\n",
        "    new_height = int(height * min(w / width, h / height))\n",
        "    resized_image = cv2.resize(img, (new_width, new_height), interpolation = cv2.INTER_CUBIC)\n",
        "    \n",
        "    canvas = np.full((input_dim[1], input_dim[0], 3), 128)\n",
        "    canvas[(h - new_height) // 2: (h - new_height) // 2 + new_height,(w - new_width) // 2: (w - new_width) // 2 + new_width,  :] = resized_image\n",
        "    return canvas"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiJ5sHZgpfvt"
      },
      "source": [
        "def pre_image(img, input_dim):\n",
        "  \"\"\"\n",
        "  Prepare image as input for neural network\n",
        "  \"\"\"\n",
        "\n",
        "  img = resize_image(img, (input_dim, input_dim))\n",
        "  img = img[:, :, ::-1].transpose((2, 0, 1)).copy()\n",
        "  img = torch.from_numpy(img).float().div(255.0).unsqueeze(0)\n",
        "  return img"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTGiD8L61HxB"
      },
      "source": [
        "def draw_result(x, results, colors, classes):\n",
        "  t1 = tuple(x[1: 3].int())\n",
        "  t2 = tuple(x[3: 5].int())\n",
        "  img = results[int(x[0])]\n",
        "  text_font = cv2.FONT_HERSHEY_PLAIN\n",
        "  cls = int(x[-1])\n",
        "  color = random.choice(colors)\n",
        "  label = \"{}\".format(classes[cls])\n",
        "  cv2.rectangle(img, t1, t2, color, 1)\n",
        "  text_size = cv2.getTextSize(label, text_font, 1, 1)[0]\n",
        "  t2 = t1[0] + text_size[0] + 3, t1[1] + text_size[1] + 4\n",
        "  cv2.rectangle(img, t1, t2, color, -1)\n",
        "  text_pos = t1[0], t1[1] + text_size[1] + 4\n",
        "  cv2.putText(img, label, text_pos, text_font, 1, [255, 255, 255], 1)\n",
        "  return img"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8IjIIknF0i2"
      },
      "source": [
        "def load_dataset(file_path):\n",
        "  file = open(file_path, \"r\")\n",
        "  names = file.read().split(\"\\n\")[:-1]\n",
        "  return names"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XNRDOVshKjT"
      },
      "source": [
        "## Image detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsja-s4xSf7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96deef57-24e1-429b-d992-a7429c44f665"
      },
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import cv2 \n",
        "import argparse\n",
        "import os \n",
        "import os.path as osp\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "\n",
        "def parse_arg():\n",
        "  \"\"\"\n",
        "  Parse arguments to detect module\n",
        "  \"\"\"\n",
        "\n",
        "  parser = argparse.ArgumentParser(description=\"reYOLO Detection Module\")\n",
        "  parser.add_argument(\"--images\", default=\"/content/eagle.jpg\", type=str, help=\"Image path or directory containing images to perform detection\")\n",
        "  parser.add_argument(\"--det\", default=\"det\", type=str, help=\"Imgage path or directory to store detections\")\n",
        "  parser.add_argument(\"--bs\", default=1, help=\"Batch size\")\n",
        "  parser.add_argument(\"--confidence\", default=0.5, help=\"Object confidence to filter predictions\")\n",
        "  parser.add_argument(\"--nms\", default=0.4, help=\"NMS Threshold\")\n",
        "  parser.add_argument(\"--cfg\", dest=\"cfg_file\", default=\"/content/yolov3-tiny.cfg\", type=str, help=\"Config file path\")\n",
        "  parser.add_argument(\"--weights\", dest=\"weights_file\", default=\"/content/yolov3-tiny.weights\", type=str, help=\"Weights file path\")\n",
        "  parser.add_argument(\"--dataset\", default=\"/content/coco.names\", type=str, help=\"Dataset file path\")\n",
        "  parser.add_argument(\"--colors\", dest=\"colors_file\", default=\"/content/pallete\", type=str, help=\"Colors file path\")\n",
        "\n",
        "  args, unknown = parser.parse_known_args()\n",
        "  return args\n",
        "\n",
        "class ImageDetect():\n",
        "  def __init__(self):\n",
        "    args = parse_arg()\n",
        "    self.images = args.images\n",
        "    self.cfg_file = args.cfg_file\n",
        "    self.weights_file = args.weights_file\n",
        "    self.det = args.det\n",
        "    self.batch_size = int(args.bs)\n",
        "    self.confidence = float(args.confidence)\n",
        "    self.nms = float(args.nms)\n",
        "    self.CUDA = torch.cuda.is_available()\n",
        "    self.classes = load_dataset(args.dataset)\n",
        "    self.num_classes = len(self.classes)\n",
        "    self.colors_file = args.colors_file\n",
        "  \n",
        "  def load_network(self):\n",
        "    \"\"\"\n",
        "    Setup neural network\n",
        "    \"\"\"\n",
        "    self.model = Darknet(self.cfg_file)\n",
        "    self.model.load_weight(self.weights_file)\n",
        "    self.input_dim = int(self.model.net[\"height\"])\n",
        "    assert self.input_dim % 32 == 0\n",
        "    assert self.input_dim > 32\n",
        "  \n",
        "  def get_detections(self):\n",
        "    self.load_network()\n",
        "    if self.CUDA:         # if cuda available\n",
        "      self.model.cuda()\n",
        "    \n",
        "    self.model.eval()       # set model in evaluation mode\n",
        "    read_time = time.time()\n",
        "\n",
        "    try:\n",
        "      image_list = [osp.join(osp.realpath(\".\"), self.images, img) for img in os.listdir(self.images)]\n",
        "    except NotADirectoryError:\n",
        "      image_list = []\n",
        "      image_list.append(osp.join(osp.realpath(\".\"), self.images))\n",
        "    except FileNotFoundError:\n",
        "      print(\"No file or directory with name {}\".format(self.images))\n",
        "      exit()\n",
        "\n",
        "    if not os.path.exists(self.det):\n",
        "      os.makedirs(self.det)\n",
        "\n",
        "    load_batch_time = time.time()\n",
        "    loaded_img_list = [cv2.imread(x) for x in image_list]\n",
        "    # pytorch variables for images\n",
        "    img_batches = list(map(pre_image, loaded_img_list, [self.input_dim for i in range(len(image_list))]))\n",
        "    # dimensions of original images\n",
        "    img_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_img_list]\n",
        "    img_dim_list = torch.FloatTensor(img_dim_list).repeat(1, 2)\n",
        "\n",
        "    # create batches\n",
        "    left_over = 0\n",
        "    if len(img_dim_list) % self.batch_size:\n",
        "      left_over = 1\n",
        "    \n",
        "    if self.batch_size != 1:\n",
        "      num_batches = len(image_list) // self.batch_size + left_over\n",
        "      img_batches = [torch.car((img_batches[i * self.batch_size: min((i + 1) * self.batch_size, len(img_batches))])) for i in range(num_batches)]\n",
        "    \n",
        "    check = 0\n",
        "    if self.CUDA:\n",
        "      img_dim_list = img_dim_list.cuda()\n",
        "\n",
        "    start_detect_loop_time = time.time()\n",
        "\n",
        "    # detection loop\n",
        "    for i, batch in enumerate(img_batches):\n",
        "      start = time.time()\n",
        "      if self.CUDA:\n",
        "        batch = batch.cuda()\n",
        "      with torch.no_grad():\n",
        "        prediction = self.model(Variable(batch), self.CUDA)\n",
        "      \n",
        "      prediction = get_result(prediction, self.confidence, self.num_classes, nms_conf=self.nms)\n",
        "\n",
        "      end = time.time()\n",
        "      if type(prediction) == int:\n",
        "        for img_num, image in enumerate(image_list[i * self.batch_size: min((i + 1) * self.batch_size, len(image_list))]):\n",
        "          img_id = i * self.batch_size + img_num\n",
        "          print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start) / self.batch_size))\n",
        "          print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
        "          print(\"*********************************************\")\n",
        "        continue\n",
        "      \n",
        "      # transform attr from index in batch to index in image list\n",
        "      prediction[:, 0] += i * self.batch_size\n",
        "      if not check:           # initialize output\n",
        "        output = prediction\n",
        "        check = 1\n",
        "      else:\n",
        "        output = torch.cat((output, prediction))\n",
        "      \n",
        "      for img_num, image in enumerate(image_list[i * self.batch_size: min((i + 1) * self.batch_size, len(image_list))]):\n",
        "          img_id = i * self.batch_size + img_num\n",
        "          objects = [self.classes[int(x[-1])] for x in output if int(x[0]) == img_id]\n",
        "          print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start) / self.batch_size))\n",
        "          print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objects)))\n",
        "          print(\"*********************************************\")\n",
        "      \n",
        "      if self.CUDA:\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # draw bouding boxes on images\n",
        "    try:\n",
        "      output\n",
        "    except NameError:\n",
        "      print(\"No detection were made\")\n",
        "      exit()\n",
        "    \n",
        "    img_dim_list = torch.index_select(img_dim_list, 0, output[:, 0].long())\n",
        "    scale_factor = torch.min(self.input_dim / img_dim_list, 1)[0].view(-1, 1)\n",
        "    output[:, [1, 3]] -= (self.input_dim - scale_factor * img_dim_list[:, 0].view(-1, 1)) / 2\n",
        "    output[:, [2, 4]] -= (self.input_dim - scale_factor * img_dim_list[:, 1].view(-1, 1)) / 2\n",
        "    output[:, 1:5] /= scale_factor\n",
        "\n",
        "    for i in range(output.shape[0]):\n",
        "      output[i, [1, 3]] = torch.clamp(output[i, [1, 3]], 0.0, img_dim_list[i, 0])\n",
        "      output[i, [2, 4]] = torch.clamp(output[i, [2, 4]], 0.0, img_dim_list[i, 1])\n",
        "    \n",
        "    output_recast_time = time.time()\n",
        "    class_load_time = time.time()\n",
        "    colors = pkl.load(open(self.colors_file, \"rb\"))\n",
        "    draw_time = time.time()\n",
        "\n",
        "    list(map(lambda x: draw_result(x, loaded_img_list, colors, self.classes), output))\n",
        "    detect_names = pd.Series(image_list).apply(lambda x: \"{}/detect_{}\".format(self.det, x.split(\"/\")[-1]))\n",
        "    list(map(cv2.imwrite, detect_names, loaded_img_list))\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Results\")\n",
        "    print(\"*********************************************\")\n",
        "    print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Reading\", load_batch_time - read_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_detect_loop_time - load_batch_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(image_list)) +  \" images)\", output_recast_time - start_detect_loop_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Output processing\", class_load_time - output_recast_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Drawing boxes\", end - draw_time))\n",
        "    print(\"{:25s}: {:2.3f}\".format(\"Average time per img\", (end - load_batch_time) / len(image_list)))\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "test = ImageDetect()\n",
        "test.get_detections()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eagle.jpg            predicted in  0.051 seconds\n",
            "Objects Detected:    bird\n",
            "*********************************************\n",
            "Results\n",
            "*********************************************\n",
            "Task                     : Time Taken (in seconds)\n",
            "Reading                  : 0.000\n",
            "Loading batch            : 0.012\n",
            "Detection (1 images)     : 0.062\n",
            "Output processing        : 0.000\n",
            "Drawing boxes            : 0.070\n",
            "Average time per img     : 0.144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDUBbazZhSol"
      },
      "source": [
        "## Video detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk2fLjeaD2_c"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from __future__ import division\n",
        "import time\n",
        "import torch \n",
        "from torch.autograd import Variable\n",
        "import cv2 \n",
        "import argparse\n",
        "\n",
        "def parse_arg():\n",
        "  \"\"\"\n",
        "  Parse arguments to detect module\n",
        "  \"\"\"\n",
        "\n",
        "  parser = argparse.ArgumentParser(description=\"reYOLO Detection Module\")\n",
        "  parser.add_argument(\"--video\", dest=\"video_file\", default=\"/content/videoplayback.mp4\", type=str, help=\"Image path or directory containing images to perform detection\")\n",
        "  parser.add_argument(\"--bs\", default=1, help=\"Batch size\")\n",
        "  parser.add_argument(\"--confidence\", default=0.5, help=\"Object confidence to filter predictions\")\n",
        "  parser.add_argument(\"--nms\", default=0.4, help=\"NMS Threshold\")\n",
        "  parser.add_argument(\"--cfg\", dest=\"cfg_file\", default=\"/content/yolov3.cfg\", type=str, help=\"Config file path\")\n",
        "  parser.add_argument(\"--weights\", dest=\"weights_file\", default=\"/content/yolov3.weights\", type=str, help=\"Weights file path\")\n",
        "  parser.add_argument(\"--dataset\", default=\"/content/coco.names\", type=str, help=\"Dataset file path\")\n",
        "  parser.add_argument(\"--colors\", dest=\"colors_file\", default=\"/content/pallete\", type=str, help=\"Colors file path\")\n",
        "  parser.add_argument(\"--source\", default=\"file\", type=str, help=\"Video source\")\n",
        "  \n",
        "  args, unknown = parser.parse_known_args()\n",
        "  return args\n",
        "\n",
        "class VideoDetect():\n",
        "  def __init__(self):\n",
        "    args = parse_arg()\n",
        "    self.video_file = args.video_file\n",
        "    self.batch_size = args.bs\n",
        "    self.confidence = args.confidence\n",
        "    self.nms = args.nms\n",
        "    self.cfg_file = args.cfg_file\n",
        "    self.weights_file = args.weights_file\n",
        "    self.classes = load_dataset(args.dataset)\n",
        "    self.num_classes = len(self.classes)\n",
        "    self.colors_file = args.colors_file\n",
        "    self.CUDA = torch.cuda.is_available()\n",
        "    self.source = args.source\n",
        "  \n",
        "  def load_network(self):\n",
        "    \"\"\"\n",
        "    Setup neural network\n",
        "    \"\"\"\n",
        "    self.model = Darknet(self.cfg_file)\n",
        "    self.model.load_weight(self.weights_file)\n",
        "    self.input_dim = int(self.model.net[\"height\"])\n",
        "    assert self.input_dim % 32 == 0\n",
        "    assert self.input_dim > 32\n",
        "  \n",
        "  def get_detections(self):\n",
        "    self.load_network()\n",
        "    if self.CUDA:         # if cuda available\n",
        "      self.model.cuda()\n",
        "    \n",
        "    self.model.eval()     # set model in evaluation mode\n",
        "\n",
        "    # get video capture from source (file/webcam)\n",
        "    if self.source == \"video\":\n",
        "      cap = cv2.VideoCapture(self.video_file)\n",
        "    else:\n",
        "      cap = cv2.VideoCapture(0)   # webcam\n",
        "    assert cap.isOpened(), 'Cannot captutre video source'\n",
        "    \n",
        "    frames = 0\n",
        "    start = time.time()\n",
        "    while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "\n",
        "      if ret:\n",
        "        image = pre_image(frame, self.input_dim)\n",
        "        img_dim = frame.shape[1], frame.shape[0]\n",
        "        img_dim = torch.FloatTensor(img_dim).repeat(1, 2)\n",
        "\n",
        "        if self.CUDA:\n",
        "          img_dim = img_dim.cuda()\n",
        "          image = image.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          prediction = self.model(Variable(image, volatile=True), self.CUDA)\n",
        "        prediction = get_result(prediction, self.confidence, self.num_classes, nms_conf=self.nms)\n",
        "        if type(prediction) == int:\n",
        "          frames += 1\n",
        "          print(\"FPS: {:5.4f}\".format(frames / (time.time() - start)))\n",
        "          # cv2.imshow(\"frame\", frame)\n",
        "          cv2_imshow(frame)\n",
        "          key = cv2.waitKey(1)\n",
        "          if key & 0xFF == ord('q'):    # exit if press q\n",
        "            break\n",
        "          continue\n",
        "        \n",
        "        img_dim = img_dim.repeat(prediction.size(0), 1)\n",
        "        scale_factor = torch.min(self.input_dim / img_dim, 1)[0].view(-1, 1)\n",
        "        prediction[:, [1, 3]] -= (self.input_dim - scale_factor * img_dim[:, 0].view(-1, 1)) / 2\n",
        "        prediction[:, [2, 4]] -= (self.input_dim - scale_factor * img_dim[:, 1].view(-1, 1)) / 2\n",
        "        prediction[:, 1: 5] /= scale_factor\n",
        "\n",
        "        for i in range(prediction.shape[0]):\n",
        "          prediction[i, [1, 3]] = torch.clamp(prediction[i, [1, 3]], 0.0, img_dim[i, 0])\n",
        "          prediction[i, [2, 4]] = torch.clamp(prediction[i, [2, 4]], 0.0, img_dim[i, 1])\n",
        "        \n",
        "        list(map(lambda x: draw_result(x, frame, self.colors, self.classes), prediction))\n",
        "        # cv2.imshow(\"frame\", frame)\n",
        "        cv2_imshow(frame)\n",
        "        key = cv2.waitKey(1)\n",
        "        if key & 0xFF == ord('q'):\n",
        "          break\n",
        "        frames += 1\n",
        "        t = time.time() - start\n",
        "        print(\"Predicted in {1:6.3f} seconds\".format(t))\n",
        "        print(\"FPS: {:5.2f}\".format(frames / (time.time() - start)))\n",
        "      else:\n",
        "        break\n",
        "\n",
        "test = VideoDetect()\n",
        "test.get_detections()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfuYTDiZZHRV"
      },
      "source": [
        "# Training Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpkPM8Vco_N2"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, images_path, image_size, max_objects=100, multiscale=True, transform=None, quick=False):\n",
        "    super().__init__()\n",
        "    with open(images_path, \"r\") as file:\n",
        "      self.image_files = [name.rstrip() for name in file.readlines()]\n",
        "    self.label_files = []\n",
        "\n",
        "    for path in self.image_files:\n",
        "      image_dir = os.path.dirname(path)\n",
        "      label_dir = \"labels\".join(image_dir.rsplit(\"images\", 1))\n",
        "      label_file = os.path.join(label_dir, os.path.basename(path))\n",
        "      label_file = os.path.splitext(label_file)[0] + \".txt\"\n",
        "      self.label_files.append(label_file)\n",
        "\n",
        "    if quick:\n",
        "      self.image_files = self.image_files[:1000]\n",
        "    \n",
        "    self.images = []\n",
        "    for name in self.image_files:\n",
        "      f = open(name, \"rb\")\n",
        "      self.images.append(Image.open(f))\n",
        "\n",
        "    [image.load() for image in self.images]    \n",
        "    self.image_size = image_size\n",
        "    self.max_objects = max_objects\n",
        "    self.multiscale = multiscale\n",
        "    self.min_size = self.image_size - 3 * 32\n",
        "    self.max_size = self.image_size + 3 * 32\n",
        "    self.batch_count = 0\n",
        "    self.transform = transform\n",
        "    \n",
        "  def __getitem__(self, index):    \n",
        "    try:\n",
        "      label_path = self.label_files[index % len(self.image_files)].rstrip()\n",
        "      with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        boxes = np.loadtxt(label_path).reshape(-1, 5)\n",
        "    except Exception:\n",
        "      print(f\"Cannot read label '{label_path}'.\")\n",
        "      return\n",
        "  \n",
        "    if self.transform:\n",
        "      try:\n",
        "        image = self.images[index % len(self.image_files)]\n",
        "        image, targets = self.transform((np.array(image), boxes))\n",
        "      except Exception:\n",
        "        print(\"Cannot apply transform.\")\n",
        "        return\n",
        "    \n",
        "    return image, targets\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    self.batch_count += 1\n",
        "\n",
        "    # Drop invalid images\n",
        "    batch = [data for data in batch if data is not None]\n",
        "\n",
        "    imgs, targets = list(zip(*batch))\n",
        "\n",
        "    # Selects new image size every tenth batch\n",
        "    if self.multiscale and self.batch_count % 10 == 0:\n",
        "      self.image_size = random.choice(\n",
        "          range(self.min_size, self.max_size + 1, 32))\n",
        "\n",
        "    # Resize images to input shape\n",
        "    imgs = torch.stack([resize(img, self.image_size) for img in imgs])\n",
        "\n",
        "    # Add sample index to targets\n",
        "    for i, boxes in enumerate(targets):\n",
        "      boxes[:, 0] = i\n",
        "    targets = torch.cat(targets, 0)\n",
        "\n",
        "    return imgs, targets\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label_files)\n",
        "\n",
        "def resize(image, size):\n",
        "  image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n",
        "  return image"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiUd6sr-N8tX"
      },
      "source": [
        "import imgaug.augmenters as iaa\n",
        "import torch\n",
        "import numpy as np\n",
        "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
        "import torchvision.transforms as transforms\n",
        "from dataclasses import dataclass\n",
        "\n",
        "def xywh2xyxy_np(x):\n",
        "  y = np.zeros_like(x)\n",
        "  y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
        "  y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
        "  y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
        "  y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
        "  return y\n",
        "\n",
        "class ImageAugmenter(object):\n",
        "  def __init__(self, augmentations=[]):\n",
        "    self.augmentations = augmentations\n",
        "\n",
        "  def __call__(self, data):\n",
        "    image, boxes = data\n",
        "    # Convert xywh to xyxy\n",
        "    boxes = np.array(boxes)\n",
        "    boxes[:, 1:] = xywh2xyxy_np(boxes[:, 1:])\n",
        "\n",
        "    bounding_boxes = BoundingBoxesOnImage([BoundingBox(*box[1:], label=box[0]) for box in boxes], shape=image.shape)\n",
        "    image, bounding_boxes = self.augmentations(image=image, bounding_boxes=bounding_boxes)\n",
        "    bounding_boxes = bounding_boxes.clip_out_of_image()\n",
        "    boxes = np.zeros((len(bounding_boxes), 5))\n",
        "    for i, box in enumerate(bounding_boxes):\n",
        "      x1 = box.x1\n",
        "      y1 = box.y1\n",
        "      x2 = box.x2\n",
        "      y2 = box.y2\n",
        "\n",
        "      # (x, y, w, h)\n",
        "      boxes[i, 0] = box.label\n",
        "      boxes[i, 1] = (x1 + x2) / 2\n",
        "      boxes[i, 2] = (y1 + y2) / 2\n",
        "      boxes[i, 3] = x2 - x1\n",
        "      boxes[i, 4] = y2 - y1\n",
        "    \n",
        "    return image, boxes\n",
        "\n",
        "class RelativeLabels(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, data):\n",
        "    image, boxes = data\n",
        "    h, w, _ = image.shape\n",
        "    boxes[:, [1, 3]] /= w\n",
        "    boxes[:, [2, 4]] /= h\n",
        "    return image, boxes\n",
        "\n",
        "class AbsoluteLabels(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, data):\n",
        "    image, boxes = data\n",
        "    h, w, _ = image.shape\n",
        "    boxes[:, [1, 3]] *= w\n",
        "    boxes[:, [2, 4]] *= h\n",
        "    return image, boxes\n",
        "\n",
        "class PadSquare(ImageAugmenter):\n",
        "  def __init__(self):\n",
        "    self.augmentations = iaa.Sequential([\n",
        "      iaa.PadToAspectRatio(\n",
        "        1.0,\n",
        "        position=\"center-center\").to_deterministic()\n",
        "    ])\n",
        "\n",
        "class ToTensor(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, data):\n",
        "    image, boxes = data\n",
        "    # Extract image as PyTorch tensor\n",
        "    image = transforms.ToTensor()(image)\n",
        "\n",
        "    targets = torch.zeros((len(boxes), 6))\n",
        "    targets[:, 1:] = transforms.ToTensor()(boxes)\n",
        "\n",
        "    return image, targets\n",
        "\n",
        "class DefaultAugmenter(ImageAugmenter):\n",
        "  def __init__(self):\n",
        "    self.augmentations = iaa.Sequential([\n",
        "      iaa.Sharpen((0.0, 0.1)),\n",
        "      iaa.Affine(rotate=(-0, 0), translate_percent=(-0.1, 0.1), scale=(0.8, 1.5)),\n",
        "      iaa.AddToBrightness((-60, 40)),\n",
        "      iaa.AddToHue((-10, 10)),\n",
        "      iaa.Fliplr(0.5),\n",
        "    ])\n",
        "@dataclass\n",
        "class Transform:\n",
        "  train =  transforms.Compose([\n",
        "    AbsoluteLabels(),\n",
        "    PadSquare(),\n",
        "    RelativeLabels(),\n",
        "    ToTensor(),\n",
        "  ])\n",
        "\n",
        "  val = transforms.Compose([\n",
        "    AbsoluteLabels(),\n",
        "    DefaultAugmenter(),\n",
        "    PadSquare(),\n",
        "    RelativeLabels(),\n",
        "    ToTensor(),\n",
        "  ])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwYKZScuV0T2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2819cb10aaf84bc5b12f88f08cfabf85",
            "fae895336b2344938d8ba4ccaca0e223",
            "d1bfeaedafe449379519191c60aa7b57",
            "439172cb93b2418ab0464534c3447641",
            "cbfba78e84044780843e46913d01459a",
            "a3d1c21650d84305ab2aaac68366700f",
            "db5a2f78200c4465a37101a25b9a8374",
            "144fd1c6fafa43e9be88b711d12bdb89",
            "cababaf6e2b342bdbbe978302eee23bf",
            "950b67892dab4996a363735479c3258b",
            "28b35294ab27463fbe08ee113c1a5c1c"
          ]
        },
        "outputId": "702b43da-9aef-4a2f-dba7-56b43b348d38"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "def parse_arg():\n",
        "  parser = argparse.ArgumentParser(description=\"reYOLO Training Module\")\n",
        "  parser.add_argument(\"--cfg\", dest=\"cfg_file\", type=str, default=\"/content/yolov3-tiny.cfg\", help=\"Config file path\")\n",
        "  parser.add_argument(\"--dataset\", type=str, default=\"/content/coco.names\", help=\"Dataset file path\")\n",
        "  parser.add_argument(\"--train_path\", type=str, default=\"/content/data/trainvalno5k.txt\")\n",
        "  parser.add_argument(\"--valid_path\", type=str, default=\"/content/data/5k.txt\")\n",
        "  parser.add_argument(\"--epochs\", type=int, default=300, help=\"Number of epochs\")\n",
        "  parser.add_argument(\"--cpus\", type=int, default=0, help=\"Number of cpu threads during batch generation\")\n",
        "  parser.add_argument(\"--pretrained_weights\", default=\"/content/yolov3-tiny.weights\", type=str, help=\"Checkpoint file path (.weights or .pth)\")\n",
        "  parser.add_argument(\"--multiscale_train\", action=\"store_true\", help=\"Allow multi-scale training\")\n",
        "  parser.add_argument(\"--seed\", type=int, default=-1)\n",
        "  args, unknown = parser.parse_known_args()\n",
        "  return args\n",
        "\n",
        "def load_model(path, device, weights=None):\n",
        "  model = Darknet(path).to(device)\n",
        "\n",
        "  if weights:\n",
        "    if weights.endswith(\".pth\"):\n",
        "      model.load_state_dict(torch.load(weights, map_location=device))\n",
        "    else:\n",
        "      model.load_weight(weights)\n",
        "  \n",
        "  return model\n",
        "\n",
        "class TrainingModule():\n",
        "  def __init__(self):\n",
        "    args = parse_arg()\n",
        "    self.seed = args.seed\n",
        "    self.classes = load_dataset(args.dataset)\n",
        "    self.train_path = args.train_path\n",
        "    self.valid_path = args.valid_path\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model = load_model(args.cfg_file, self.device, args.pretrained_weights)\n",
        "    self.multiscale_train = args.multiscale_train\n",
        "    self.cpus = args.cpus\n",
        "    self.epochs = args.epochs\n",
        "    self.mini_batch_size = int(self.model.net[\"batch\"])\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    dataset = ImageDataset(self.train_path, image_size=int(self.model.net[\"height\"]), multiscale=self.multiscale_train, transform=Transform.train, quick=True)\n",
        "    return DataLoader(dataset, batch_size=self.mini_batch_size, shuffle=True, num_workers=self.cpus, pin_memory=True, collate_fn=dataset.collate_fn)\n",
        "\n",
        "  def valid_dataloader(self):\n",
        "    dataset = ImageDataset(self.valid_path, image_size=int(self.model.net[\"height\"]), multiscale=False, transform=Transform.val, quick=True)\n",
        "    return DataLoader(dataset, batch_size=self.mini_batch_size, shuffle=False, num_workers=self.cpus, pin_memory=True, collate_fn=dataset.collate_fn)\n",
        "\n",
        "  def train(self):\n",
        "    pl.seed_everything(self.seed, workers=True)\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "      monitor=\"val_loss\",\n",
        "      dirpath=\"lightning_logs/ckpt1\",\n",
        "      filename=\"yolo-{epoch:02d}-{val_loss:.2f}\",\n",
        "      save_top_k=3,\n",
        "      mode=\"min\",\n",
        "    )\n",
        "  \n",
        "    trainer = pl.Trainer(accelerator=\"cpu\", devices=self.cpus, callbacks=[checkpoint_callback], weights_save_path=\"weights\", weights_summary=\"full\", enable_model_summary=True)\n",
        "    trainer.fit(self.model, train_dataloaders=self.train_dataloader(), val_dataloaders=self.valid_dataloader())\n",
        "    print(\"Best checkpoint: \", checkpoint_callback.best_model_path)\n",
        "    return self.model\n",
        "\n",
        "test = TrainingModule()\n",
        "test.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/seed.py:64: UserWarning: -1 is not in bounds, numpy accepts from 0 to 4294967295\n",
            "  rank_zero_warn(f\"{seed} is not in bounds, numpy accepts from {min_seed_value} to {max_seed_value}\")\n",
            "Global seed set to 1298179137\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:188: LightningDeprecationWarning: Setting `Trainer(weights_summary=full)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.model_summary.ModelSummary` with `max_depth` directly to the Trainer's `callbacks` argument instead.\n",
            "  f\"Setting `Trainer(weights_summary={weights_summary})` is deprecated in v1.5 and will be removed\"\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "\n",
            "   | Name                         | Type           | Params\n",
            "-----------------------------------------------------------------\n",
            "0  | module_list                  | ModuleList     | 8.9 M \n",
            "1  | module_list.0                | Sequential     | 464   \n",
            "2  | module_list.0.conv_0         | Conv2d         | 432   \n",
            "3  | module_list.0.batch_norm_0   | BatchNorm2d    | 32    \n",
            "4  | module_list.0.leaky_0        | LeakyReLU      | 0     \n",
            "5  | module_list.1                | MaxPool2d      | 0     \n",
            "6  | module_list.2                | Sequential     | 4.7 K \n",
            "7  | module_list.2.conv_2         | Conv2d         | 4.6 K \n",
            "8  | module_list.2.batch_norm_2   | BatchNorm2d    | 64    \n",
            "9  | module_list.2.leaky_2        | LeakyReLU      | 0     \n",
            "10 | module_list.3                | MaxPool2d      | 0     \n",
            "11 | module_list.4                | Sequential     | 18.6 K\n",
            "12 | module_list.4.conv_4         | Conv2d         | 18.4 K\n",
            "13 | module_list.4.batch_norm_4   | BatchNorm2d    | 128   \n",
            "14 | module_list.4.leaky_4        | LeakyReLU      | 0     \n",
            "15 | module_list.5                | MaxPool2d      | 0     \n",
            "16 | module_list.6                | Sequential     | 74.0 K\n",
            "17 | module_list.6.conv_6         | Conv2d         | 73.7 K\n",
            "18 | module_list.6.batch_norm_6   | BatchNorm2d    | 256   \n",
            "19 | module_list.6.leaky_6        | LeakyReLU      | 0     \n",
            "20 | module_list.7                | MaxPool2d      | 0     \n",
            "21 | module_list.8                | Sequential     | 295 K \n",
            "22 | module_list.8.conv_8         | Conv2d         | 294 K \n",
            "23 | module_list.8.batch_norm_8   | BatchNorm2d    | 512   \n",
            "24 | module_list.8.leaky_8        | LeakyReLU      | 0     \n",
            "25 | module_list.9                | MaxPool2d      | 0     \n",
            "26 | module_list.10               | Sequential     | 1.2 M \n",
            "27 | module_list.10.conv_10       | Conv2d         | 1.2 M \n",
            "28 | module_list.10.batch_norm_10 | BatchNorm2d    | 1.0 K \n",
            "29 | module_list.10.leaky_10      | LeakyReLU      | 0     \n",
            "30 | module_list.11               | Sequential     | 0     \n",
            "31 | module_list.11.ZeroPad2d     | ZeroPad2d      | 0     \n",
            "32 | module_list.11.MaxPool2d     | MaxPool2d      | 0     \n",
            "33 | module_list.12               | Sequential     | 4.7 M \n",
            "34 | module_list.12.conv_12       | Conv2d         | 4.7 M \n",
            "35 | module_list.12.batch_norm_12 | BatchNorm2d    | 2.0 K \n",
            "36 | module_list.12.leaky_12      | LeakyReLU      | 0     \n",
            "37 | module_list.13               | Sequential     | 262 K \n",
            "38 | module_list.13.conv_13       | Conv2d         | 262 K \n",
            "39 | module_list.13.batch_norm_13 | BatchNorm2d    | 512   \n",
            "40 | module_list.13.leaky_13      | LeakyReLU      | 0     \n",
            "41 | module_list.14               | Sequential     | 1.2 M \n",
            "42 | module_list.14.conv_14       | Conv2d         | 1.2 M \n",
            "43 | module_list.14.batch_norm_14 | BatchNorm2d    | 1.0 K \n",
            "44 | module_list.14.leaky_14      | LeakyReLU      | 0     \n",
            "45 | module_list.15               | Sequential     | 130 K \n",
            "46 | module_list.15.conv_15       | Conv2d         | 130 K \n",
            "47 | module_list.16               | Sequential     | 0     \n",
            "48 | module_list.16.Detection_16  | DetectionLayer | 0     \n",
            "49 | module_list.17               | Sequential     | 0     \n",
            "50 | module_list.17.route_17      | EmptyLayer     | 0     \n",
            "51 | module_list.18               | Sequential     | 33.0 K\n",
            "52 | module_list.18.conv_18       | Conv2d         | 32.8 K\n",
            "53 | module_list.18.batch_norm_18 | BatchNorm2d    | 256   \n",
            "54 | module_list.18.leaky_18      | LeakyReLU      | 0     \n",
            "55 | module_list.19               | Sequential     | 0     \n",
            "56 | module_list.19.upsample_19   | Upsample       | 0     \n",
            "57 | module_list.20               | Sequential     | 0     \n",
            "58 | module_list.20.route_20      | EmptyLayer     | 0     \n",
            "59 | module_list.21               | Sequential     | 885 K \n",
            "60 | module_list.21.conv_21       | Conv2d         | 884 K \n",
            "61 | module_list.21.batch_norm_21 | BatchNorm2d    | 512   \n",
            "62 | module_list.21.leaky_21      | LeakyReLU      | 0     \n",
            "63 | module_list.22               | Sequential     | 65.5 K\n",
            "64 | module_list.22.conv_22       | Conv2d         | 65.5 K\n",
            "65 | module_list.23               | Sequential     | 0     \n",
            "66 | module_list.23.Detection_23  | DetectionLayer | 0     \n",
            "-----------------------------------------------------------------\n",
            "8.9 M     Trainable params\n",
            "0         Non-trainable params\n",
            "8.9 M     Total params\n",
            "35.409    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2819cb10aaf84bc5b12f88f08cfabf85",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Validation sanity check: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0000e+00, 3.9000e+01, 6.1536e-01, 4.0488e-01, 1.4860e-02, 4.9786e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 5.8568e-01, 4.0997e-01, 1.2729e-02, 3.8780e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 5.7360e-01, 4.0698e-01, 1.3063e-02, 5.1028e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 6.0223e-01, 4.0918e-01, 1.3360e-02, 4.4914e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 7.0633e-01, 5.8774e-01, 3.3945e-02, 5.8124e-02],\n",
            "        [0.0000e+00, 7.2000e+01, 7.6953e-01, 4.8406e-01, 1.7921e-01, 2.5577e-01],\n",
            "        [0.0000e+00, 3.9000e+01, 7.5286e-01, 5.9683e-01, 1.8380e-02, 6.7723e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 6.3522e-01, 3.5462e-01, 1.7418e-02, 3.1684e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 5.8757e-01, 3.0487e-01, 1.9437e-02, 2.8089e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 6.4426e-01, 2.9952e-01, 1.8121e-02, 2.9183e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 6.6462e-01, 2.9796e-01, 1.9734e-02, 3.1239e-02],\n",
            "        [0.0000e+00, 5.6000e+01, 3.2051e-01, 8.3616e-01, 2.8045e-01, 7.7676e-02],\n",
            "        [0.0000e+00, 4.0000e+01, 1.8276e-01, 4.1734e-01, 2.3772e-02, 5.2362e-02],\n",
            "        [0.0000e+00, 4.0000e+01, 1.6179e-01, 4.2651e-01, 2.9831e-02, 6.7704e-02],\n",
            "        [0.0000e+00, 4.0000e+01, 9.9750e-02, 4.3390e-01, 2.6514e-02, 6.1886e-02],\n",
            "        [0.0000e+00, 4.0000e+01, 1.5296e-02, 4.3745e-01, 3.0592e-02, 6.9724e-02],\n",
            "        [0.0000e+00, 4.0000e+01, 5.1400e-02, 4.3805e-01, 2.8608e-02, 7.3374e-02],\n",
            "        [0.0000e+00, 4.0000e+01, 7.4236e-02, 4.3682e-01, 2.1919e-02, 6.8427e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 1.8206e-01, 4.9177e-01, 4.9620e-02, 3.8539e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 1.1939e-01, 4.9607e-01, 4.3042e-02, 3.6168e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 2.3180e-01, 4.8524e-01, 3.7317e-02, 2.9887e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 2.6849e-01, 4.7766e-01, 3.7038e-02, 2.8738e-02],\n",
            "        [0.0000e+00, 4.5000e+01, 8.7564e-01, 4.5937e-01, 1.0404e-01, 1.9510e-02],\n",
            "        [0.0000e+00, 6.8000e+01, 6.3483e-01, 4.8877e-01, 9.8721e-02, 5.6253e-02],\n",
            "        [0.0000e+00, 6.9000e+01, 4.3780e-01, 5.5891e-01, 8.6751e-02, 6.3405e-02],\n",
            "        [0.0000e+00, 4.0000e+01, 3.6780e-02, 4.3726e-01, 2.0086e-02, 7.3522e-02],\n",
            "        [0.0000e+00, 4.0000e+01, 1.2112e-01, 4.2866e-01, 2.9386e-02, 7.0261e-02],\n",
            "        [0.0000e+00, 4.0000e+01, 1.4255e-01, 4.2945e-01, 1.9919e-02, 6.5536e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 2.0120e-01, 4.8808e-01, 4.0244e-02, 3.9355e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 8.5008e-01, 3.0763e-01, 2.8535e-02, 3.9615e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 8.8657e-01, 3.1103e-01, 2.8145e-02, 2.5403e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 8.2916e-01, 3.1858e-01, 1.1599e-02, 1.5193e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 2.1661e-01, 4.3321e-01, 3.2888e-02, 3.4927e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 1.4766e-01, 4.9026e-01, 2.8387e-02, 3.7446e-02],\n",
            "        [0.0000e+00, 4.5000e+01, 8.5010e-01, 4.3555e-01, 5.4067e-02, 8.6714e-03],\n",
            "        [0.0000e+00, 4.5000e+01, 8.7427e-01, 4.4933e-01, 1.0541e-01, 7.8007e-03],\n",
            "        [0.0000e+00, 4.5000e+01, 8.7740e-01, 4.4322e-01, 9.6405e-02, 9.7094e-03],\n",
            "        [0.0000e+00, 4.5000e+01, 8.7622e-01, 4.3891e-01, 1.0306e-01, 1.1247e-02],\n",
            "        [0.0000e+00, 4.5000e+01, 5.7035e-01, 3.6519e-01, 2.4754e-02, 1.6768e-02]])\n",
            "tensor([[ 0.0000,  0.0000,  0.4414,  0.6121,  0.1724,  0.2883],\n",
            "        [ 0.0000,  0.0000,  0.3613,  0.5844,  0.1027,  0.3250],\n",
            "        [ 0.0000,  0.0000,  0.9604,  0.6298,  0.0479,  0.2693],\n",
            "        [ 0.0000,  0.0000,  0.5613,  0.5254,  0.1459,  0.3088],\n",
            "        [ 0.0000, 34.0000,  0.9447,  0.6965,  0.0472,  0.1218]])\n",
            "tensor([[ 0.0000, 39.0000,  0.6727,  0.3317,  0.1750,  0.6620],\n",
            "        [ 0.0000, 56.0000,  0.8009,  0.3048,  0.0670,  0.1834],\n",
            "        [ 0.0000, 56.0000,  0.3722,  0.3595,  0.4131,  0.6137],\n",
            "        [ 0.0000, 60.0000,  0.5000,  0.6790,  0.6687,  0.6420],\n",
            "        [ 0.0000, 40.0000,  0.4508,  0.4695,  0.1874,  0.4549]])\n",
            "tensor([[0.0000e+00, 5.6000e+01, 9.7013e-01, 1.8289e-01, 5.9741e-02, 1.1579e-01],\n",
            "        [0.0000e+00, 5.3000e+01, 5.0000e-01, 5.0000e-01, 1.0000e+00, 7.5000e-01],\n",
            "        [0.0000e+00, 4.1000e+01, 9.7246e-01, 8.0390e-01, 5.5080e-02, 1.4219e-01],\n",
            "        [0.0000e+00, 6.0000e+01, 5.0000e-01, 8.4200e-01, 1.0000e+00, 6.6005e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 1.8884e-01, 3.0430e-01, 3.7768e-01, 3.5859e-01]])\n",
            "tensor([[0.0000e+00, 2.9000e+01, 7.5642e-01, 4.2743e-01, 4.0588e-02, 1.3637e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 6.4253e-01, 5.2301e-01, 3.1481e-01, 3.9893e-01]])\n",
            "tensor([[ 0.0000, 58.0000,  0.1664,  0.3951,  0.1424,  0.3034],\n",
            "        [ 0.0000, 15.0000,  0.6483,  0.5178,  0.5275,  0.4677],\n",
            "        [ 0.0000, 57.0000,  0.5170,  0.4951,  0.7923,  0.5126],\n",
            "        [ 0.0000, 65.0000,  0.2373,  0.5460,  0.2292,  0.1123],\n",
            "        [ 0.0000, 65.0000,  0.2885,  0.4890,  0.2532,  0.0774]])\n",
            "tensor([[0.0000e+00, 1.5000e+01, 7.5385e-01, 4.7819e-01, 5.7210e-02, 4.3699e-02],\n",
            "        [0.0000e+00, 5.7000e+01, 6.9088e-01, 5.7538e-01, 3.2952e-01, 2.4001e-01],\n",
            "        [0.0000e+00, 5.8000e+01, 6.2963e-01, 4.4859e-01, 5.1957e-02, 2.6129e-02],\n",
            "        [0.0000e+00, 6.5000e+01, 4.4184e-01, 6.9733e-01, 3.3481e-02, 3.5819e-02],\n",
            "        [0.0000e+00, 4.3000e+01, 5.5795e-01, 6.5831e-01, 6.7805e-02, 2.6971e-02],\n",
            "        [0.0000e+00, 5.6000e+01, 8.8205e-01, 5.2418e-01, 4.5157e-02, 1.3315e-01],\n",
            "        [0.0000e+00, 5.8000e+01, 5.2640e-01, 4.8740e-01, 2.2911e-02, 3.4951e-02],\n",
            "        [0.0000e+00, 5.8000e+01, 8.1318e-01, 4.2294e-01, 2.7499e-02, 3.1734e-02],\n",
            "        [0.0000e+00, 7.5000e+01, 6.8115e-01, 3.4450e-01, 1.1035e-02, 2.5941e-02],\n",
            "        [0.0000e+00, 7.5000e+01, 5.8655e-01, 4.1345e-01, 1.1047e-02, 3.1508e-02],\n",
            "        [0.0000e+00, 5.8000e+01, 8.1591e-01, 4.7447e-01, 6.1395e-02, 3.2790e-02],\n",
            "        [0.0000e+00, 5.8000e+01, 8.8021e-01, 4.2714e-01, 2.9698e-02, 3.7327e-02],\n",
            "        [0.0000e+00, 7.5000e+01, 2.3714e-01, 3.8004e-01, 3.3544e-02, 4.3058e-02],\n",
            "        [0.0000e+00, 7.5000e+01, 6.9847e-01, 3.7833e-01, 2.1202e-02, 2.7524e-02],\n",
            "        [0.0000e+00, 7.3000e+01, 4.8799e-01, 7.0553e-01, 4.8639e-02, 2.6581e-02],\n",
            "        [0.0000e+00, 5.7000e+01, 3.5537e-01, 6.0782e-01, 3.7983e-01, 3.0387e-01]])\n",
            "tensor([[ 0.0000, 20.0000,  0.5124,  0.4833,  0.7660,  0.6354],\n",
            "        [ 0.0000, 20.0000,  0.6499,  0.4445,  0.6479,  0.4688],\n",
            "        [ 0.0000, 20.0000,  0.1960,  0.4780,  0.2968,  0.6248]])\n",
            "tensor([[0.0000e+00, 2.9000e+01, 1.7003e-01, 3.7050e-01, 1.1503e-01, 4.6508e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.4135e-02, 4.8605e-01, 1.4827e-01, 4.9087e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.6558e-01, 5.8428e-01, 4.8672e-01, 5.1545e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 2.9168e-01, 4.1076e-01, 8.9423e-02, 2.4795e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.6849e-01, 3.5608e-01, 3.4036e-02, 1.0097e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 9.2148e-01, 3.5546e-01, 3.6123e-02, 9.8565e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 9.6903e-01, 3.5821e-01, 6.1936e-02, 1.7567e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.9697e-01, 3.8090e-01, 2.3148e-02, 7.3065e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.0619e-01, 3.5099e-01, 8.0200e-02, 1.6209e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.9904e-01, 3.4612e-01, 5.9245e-02, 1.3495e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.5090e-01, 3.6031e-01, 2.9358e-02, 9.0771e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.9521e-01, 3.5307e-01, 3.6414e-02, 1.3554e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.3460e-01, 3.6129e-01, 2.4285e-02, 9.6002e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.1798e-01, 3.5800e-01, 8.3596e-01, 1.6912e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.1798e-01, 3.5800e-01, 8.3596e-01, 1.6912e-01]])\n",
            "tensor([[0.0000e+00, 3.2000e+01, 8.0545e-01, 3.0424e-01, 3.9610e-02, 3.3785e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.4693e-01, 5.7074e-01, 5.9670e-01, 8.5853e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.5784e-01, 5.9705e-01, 5.4303e-01, 8.0590e-01],\n",
            "        [0.0000e+00, 3.5000e+01, 7.6384e-01, 3.0390e-01, 1.1742e-01, 9.3567e-02]])\n",
            "tensor([[ 0.0000, 23.0000,  0.5335,  0.5438,  0.2071,  0.6245]])\n",
            "tensor([[0.0000e+00, 1.8000e+01, 3.5245e-01, 6.7224e-01, 2.5259e-01, 3.2115e-01],\n",
            "        [0.0000e+00, 1.8000e+01, 6.9598e-01, 6.3357e-01, 2.0833e-01, 3.9848e-01],\n",
            "        [0.0000e+00, 1.8000e+01, 5.1967e-01, 7.1089e-01, 2.8416e-01, 2.4384e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.9674e-01, 3.7384e-01, 1.2995e-01, 4.1643e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 9.5637e-01, 3.8841e-01, 8.7260e-02, 4.4557e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.3640e-01, 3.5944e-01, 9.8978e-02, 3.8764e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 9.0529e-01, 2.3247e-01, 8.1786e-02, 1.3368e-01],\n",
            "        [0.0000e+00, 2.5000e+01, 9.5819e-01, 2.0734e-01, 8.3623e-02, 8.3430e-02],\n",
            "        [0.0000e+00, 7.0000e+00, 5.7748e-01, 2.7727e-01, 6.1906e-01, 2.2328e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 9.9213e-01, 4.0106e-01, 1.5736e-02, 4.6433e-01],\n",
            "        [0.0000e+00, 2.6000e+01, 8.0659e-01, 3.7743e-01, 8.8659e-02, 9.0038e-02],\n",
            "        [0.0000e+00, 2.6000e+01, 9.5870e-01, 3.8094e-01, 7.1391e-02, 6.6240e-02]])\n",
            "tensor([[0.0000e+00, 6.0000e+01, 7.3265e-01, 7.7958e-01, 1.9720e-01, 4.4085e-01],\n",
            "        [0.0000e+00, 4.2000e+01, 6.0346e-01, 5.4130e-01, 2.7540e-01, 2.2163e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.6768e-01, 5.4223e-01, 5.9787e-01, 9.1555e-01],\n",
            "        [0.0000e+00, 5.5000e+01, 8.2624e-01, 5.7425e-01, 1.0020e-02, 3.2988e-02],\n",
            "        [0.0000e+00, 5.5000e+01, 4.1027e-01, 8.5671e-01, 3.6643e-01, 2.8659e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.2383e-01, 3.8132e-01, 2.1484e-01, 4.0729e-01],\n",
            "        [0.0000e+00, 1.3000e+01, 7.2956e-01, 9.6105e-01, 2.0338e-01, 7.7892e-02],\n",
            "        [0.0000e+00, 4.4000e+01, 5.9781e-01, 5.4072e-01, 2.6432e-01, 2.1640e-01],\n",
            "        [0.0000e+00, 5.5000e+01, 7.8104e-01, 5.7630e-01, 5.2639e-02, 2.9465e-02]])\n",
            "tensor([[ 0.0000,  0.0000,  0.5928,  0.4253,  0.2933,  0.5715],\n",
            "        [ 0.0000, 31.0000,  0.5977,  0.6893,  0.1741,  0.0536],\n",
            "        [ 0.0000,  0.0000,  0.5953,  0.5526,  0.0310,  0.0699]])\n",
            "tensor([[ 0.0000, 20.0000,  0.3904,  0.4915,  0.4617,  0.3594],\n",
            "        [ 0.0000, 20.0000,  0.2245,  0.4317,  0.4490,  0.5260],\n",
            "        [ 0.0000, 20.0000,  0.9292,  0.4464,  0.1416,  0.5552],\n",
            "        [ 0.0000, 20.0000,  0.8263,  0.4152,  0.2179,  0.4930]])\n",
            "tensor([[0.0000e+00, 1.7000e+01, 5.5895e-01, 4.9849e-01, 1.0501e-01, 6.9204e-02],\n",
            "        [0.0000e+00, 1.7000e+01, 4.4216e-01, 4.9235e-01, 9.0700e-02, 7.7485e-02],\n",
            "        [0.0000e+00, 1.7000e+01, 6.4346e-01, 4.9921e-01, 2.8543e-02, 6.8421e-02],\n",
            "        [0.0000e+00, 1.7000e+01, 5.0137e-01, 4.9796e-01, 2.4236e-02, 6.4662e-02],\n",
            "        [0.0000e+00, 1.9000e+01, 5.5096e-01, 4.9695e-01, 1.3899e-01, 7.0594e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.5142e-01, 4.3086e-01, 4.1875e-02, 4.4087e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 1.3664e-01, 5.2425e-01, 1.3841e-02, 1.7521e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.6224e-02, 5.2966e-01, 1.4075e-02, 2.1104e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 1.7030e-02, 5.3559e-01, 1.0513e-02, 1.9440e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.2595e-01, 5.7526e-01, 2.6527e-02, 4.6789e-02],\n",
            "        [0.0000e+00, 7.0000e+00, 1.5392e-01, 6.4532e-01, 3.0783e-01, 1.2670e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.4255e-02, 5.5097e-01, 2.0869e-02, 2.3904e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.7347e-01, 5.0809e-01, 1.9772e-02, 1.7384e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.4410e-01, 5.0516e-01, 2.2670e-02, 2.2258e-02],\n",
            "        [0.0000e+00, 1.7000e+01, 6.0413e-01, 4.9570e-01, 7.1416e-02, 7.5840e-02],\n",
            "        [0.0000e+00, 1.9000e+01, 6.3468e-01, 4.9727e-01, 4.5242e-02, 7.2101e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.0000e-01, 5.6155e-01, 1.0000e+00, 2.1143e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.0000e-01, 5.6155e-01, 1.0000e+00, 2.1143e-01]])\n",
            "tensor([[ 0.0000,  0.0000,  0.5628,  0.4350,  0.2184,  0.2653],\n",
            "        [ 0.0000, 37.0000,  0.6170,  0.5713,  0.1925,  0.1010]])\n",
            "tensor([[0.0000, 9.0000, 0.8774, 0.3989, 0.1445, 0.2140],\n",
            "        [0.0000, 9.0000, 0.8017, 0.4512, 0.1342, 0.3229],\n",
            "        [0.0000, 9.0000, 0.9716, 0.4027, 0.0568, 0.1878]])\n",
            "tensor([[ 0.0000, 22.0000,  0.8823,  0.4852,  0.1838,  0.2606],\n",
            "        [ 0.0000, 22.0000,  0.2124,  0.5144,  0.1030,  0.3418],\n",
            "        [ 0.0000, 22.0000,  0.4588,  0.4268,  0.3189,  0.2868],\n",
            "        [ 0.0000, 22.0000,  0.7522,  0.4180,  0.1533,  0.2442],\n",
            "        [ 0.0000, 22.0000,  0.6729,  0.4958,  0.1233,  0.4415],\n",
            "        [ 0.0000, 22.0000,  0.0311,  0.3492,  0.0622,  0.0973],\n",
            "        [ 0.0000, 22.0000,  0.0894,  0.4854,  0.1788,  0.4561],\n",
            "        [ 0.0000, 22.0000,  0.5181,  0.5511,  0.2199,  0.3964],\n",
            "        [ 0.0000, 22.0000,  0.9007,  0.4496,  0.1986,  0.2664]])\n",
            "tensor([[0.0000e+00, 8.0000e+00, 2.1645e-01, 3.8407e-01, 3.5621e-01, 2.3989e-01],\n",
            "        [0.0000e+00, 8.0000e+00, 2.8107e-02, 1.9251e-01, 5.6213e-02, 3.2659e-02],\n",
            "        [0.0000e+00, 8.0000e+00, 2.0813e-01, 2.0243e-01, 7.4812e-02, 2.6162e-02],\n",
            "        [0.0000e+00, 8.0000e+00, 3.0842e-01, 1.9911e-01, 9.8013e-02, 3.8311e-02],\n",
            "        [0.0000e+00, 8.0000e+00, 7.6538e-01, 2.1565e-01, 5.7772e-02, 2.6365e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.7080e-01, 1.9899e-01, 1.8479e-02, 2.3217e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.1656e-01, 2.0609e-01, 2.0950e-02, 3.8769e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.8768e-01, 1.8477e-01, 8.2753e-03, 2.0493e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 1.5706e-01, 1.7539e-01, 9.1547e-03, 1.4654e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.7325e-01, 1.8626e-01, 1.1270e-02, 2.0729e-02],\n",
            "        [0.0000e+00, 8.0000e+00, 1.0964e-01, 1.8700e-01, 6.1055e-02, 2.9241e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.8099e-01, 1.9005e-01, 6.5154e-03, 1.0610e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.8063e-01, 1.9594e-01, 1.1592e-02, 2.5400e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.3186e-02, 1.8544e-01, 2.0239e-02, 2.4672e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.9771e-01, 1.8642e-01, 9.1038e-03, 2.2337e-02],\n",
            "        [0.0000e+00, 8.0000e+00, 3.7417e-01, 1.6181e-01, 4.8922e-02, 7.3622e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 9.1173e-01, 1.8867e-01, 7.0731e-03, 1.6448e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 6.0098e-01, 1.8404e-01, 8.7486e-03, 2.0087e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 6.4581e-02, 1.8454e-01, 9.1211e-03, 3.2423e-02],\n",
            "        [0.0000e+00, 8.0000e+00, 9.2756e-01, 2.1840e-01, 4.5910e-02, 3.7364e-02]])\n",
            "tensor([[0.0000, 6.0000, 0.4481, 0.5149, 0.1473, 0.2145],\n",
            "        [0.0000, 6.0000, 0.7466, 0.5000, 0.1724, 1.0000]])\n",
            "tensor([[0.0000, 6.0000, 0.3971, 0.4579, 0.4965, 0.5325],\n",
            "        [0.0000, 6.0000, 0.8131, 0.4915, 0.3738, 0.6428]])\n",
            "tensor([[0.0000, 4.0000, 0.6257, 0.5230, 0.3365, 0.1037],\n",
            "        [0.0000, 7.0000, 0.2646, 0.6023, 0.1321, 0.0781],\n",
            "        [0.0000, 2.0000, 0.2607, 0.6121, 0.1289, 0.0776]])\n",
            "tensor([[ 0.0000, 74.0000,  0.4903,  0.4944,  0.3685,  0.3162]])\n",
            "tensor([[0.0000e+00, 7.4000e+01, 2.7569e-01, 5.0889e-01, 3.6640e-02, 2.6763e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.8217e-01, 4.9721e-01, 9.6434e-01, 8.3943e-01],\n",
            "        [0.0000e+00, 3.6000e+01, 6.7232e-01, 7.3237e-01, 5.6056e-01, 3.8025e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.1187e-01, 9.1356e-01, 2.4347e-01, 1.7874e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.9942e-01, 2.4792e-01, 1.7388e-01, 1.2968e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.4815e-01, 8.8699e-01, 1.6140e-01, 7.1027e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.8533e-01, 7.5115e-01, 1.4231e-01, 1.3594e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.6367e-01, 7.3318e-01, 1.6367e-01, 1.7112e-01]])\n",
            "tensor([[0.0000e+00, 2.5000e+01, 5.0000e-01, 2.5244e-01, 7.5000e-01, 4.2037e-01],\n",
            "        [0.0000e+00, 5.6000e+01, 8.5033e-01, 8.8233e-01, 4.9333e-02, 2.3534e-01],\n",
            "        [0.0000e+00, 5.6000e+01, 8.5956e-01, 7.1788e-01, 3.0875e-02, 1.9185e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.0751e-01, 7.0549e-01, 2.0366e-01, 5.8315e-01]])\n",
            "tensor([[ 0.0000,  5.0000,  0.2649,  0.4142,  0.5299,  0.4972],\n",
            "        [ 0.0000,  0.0000,  0.3108,  0.4491,  0.0371,  0.0460],\n",
            "        [ 0.0000,  0.0000,  0.2641,  0.4352,  0.0703,  0.0727],\n",
            "        [ 0.0000,  7.0000,  0.6862,  0.4756,  0.0624,  0.0700],\n",
            "        [ 0.0000,  9.0000,  0.5440,  0.3003,  0.0173,  0.0441],\n",
            "        [ 0.0000,  9.0000,  0.7645,  0.3777,  0.0193,  0.0549],\n",
            "        [ 0.0000,  9.0000,  0.7685,  0.4527,  0.0190,  0.0163],\n",
            "        [ 0.0000, 10.0000,  0.7807,  0.5151,  0.0143,  0.0254],\n",
            "        [ 0.0000, 12.0000,  0.7639,  0.4996,  0.0152,  0.0269],\n",
            "        [ 0.0000, 10.0000,  0.9440,  0.5533,  0.0212,  0.0338]])\n",
            "tensor([[0.0000e+00, 1.7000e+01, 5.1195e-01, 4.0180e-01, 6.7322e-01, 5.8160e-01],\n",
            "        [0.0000e+00, 1.7000e+01, 2.6963e-01, 3.6425e-01, 5.1987e-02, 1.1167e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.0663e-01, 2.5847e-01, 1.1724e-01, 4.2204e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.9950e-02, 3.7766e-01, 8.7458e-02, 2.8040e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.5911e-02, 3.9251e-01, 3.1823e-02, 2.8528e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 2.7112e-01, 3.1173e-01, 5.5460e-02, 8.7077e-02]])\n",
            "tensor([[0.0000e+00, 6.2000e+01, 6.3897e-01, 4.3822e-01, 2.5460e-01, 2.1201e-01],\n",
            "        [0.0000e+00, 6.4000e+01, 5.8068e-01, 6.5675e-01, 5.6462e-02, 5.3938e-02],\n",
            "        [0.0000e+00, 6.6000e+01, 6.9020e-01, 6.1568e-01, 3.0261e-01, 1.0503e-01],\n",
            "        [0.0000e+00, 6.7000e+01, 5.3046e-01, 6.9830e-01, 3.1112e-02, 4.3844e-02],\n",
            "        [0.0000e+00, 6.3000e+01, 4.0132e-01, 5.4666e-01, 2.2067e-01, 2.5154e-01]])\n",
            "tensor([[0.0000, 8.0000, 0.5587, 0.5629, 0.4662, 0.1930],\n",
            "        [0.0000, 0.0000, 0.4235, 0.5188, 0.1023, 0.1220],\n",
            "        [0.0000, 0.0000, 0.6912, 0.5137, 0.1549, 0.1984],\n",
            "        [0.0000, 8.0000, 0.1803, 0.3689, 0.1202, 0.0565],\n",
            "        [0.0000, 8.0000, 0.2378, 0.3607, 0.2430, 0.0613],\n",
            "        [0.0000, 8.0000, 0.7828, 0.3778, 0.2445, 0.0729]])\n",
            "tensor([[0.0000e+00, 6.7000e+01, 3.9491e-01, 4.7582e-01, 1.2219e-01, 5.4706e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 6.3586e-01, 5.1058e-01, 5.2313e-01, 5.5390e-01],\n",
            "        [0.0000e+00, 2.0000e+00, 5.5337e-01, 2.6631e-01, 3.1923e-01, 7.3066e-02],\n",
            "        [0.0000e+00, 2.0000e+00, 1.8039e-01, 2.9107e-01, 2.6349e-01, 1.2257e-01]])\n",
            "tensor([[0.0000e+00, 4.6000e+01, 4.6245e-01, 4.9498e-01, 9.2490e-01, 5.8996e-01],\n",
            "        [0.0000e+00, 6.6000e+01, 4.8527e-03, 4.4789e-01, 9.7054e-03, 9.2063e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 9.7750e-01, 3.8176e-01, 4.4995e-02, 3.6352e-01]])\n",
            "tensor([[ 0.0000, 15.0000,  0.2951,  0.2257,  0.3402,  0.4514],\n",
            "        [ 0.0000,  2.0000,  0.5000,  0.5000,  0.7500,  1.0000]])\n",
            "tensor([[0.0000e+00, 6.0000e+01, 5.0000e-01, 5.0000e-01, 1.0000e+00, 7.5000e-01],\n",
            "        [0.0000e+00, 4.0000e+01, 5.7942e-01, 4.2043e-01, 2.8196e-01, 5.9085e-01],\n",
            "        [0.0000e+00, 4.0000e+01, 1.0444e-01, 3.9672e-01, 2.0887e-01, 5.4343e-01],\n",
            "        [0.0000e+00, 4.0000e+01, 9.7123e-01, 4.0795e-01, 5.7538e-02, 5.6589e-01],\n",
            "        [0.0000e+00, 4.0000e+01, 2.0138e-01, 2.9574e-01, 2.4344e-01, 3.4149e-01],\n",
            "        [0.0000e+00, 4.0000e+01, 9.3812e-01, 2.9802e-01, 1.2376e-01, 3.4605e-01],\n",
            "        [0.0000e+00, 4.0000e+01, 9.2433e-01, 1.8300e-01, 1.5134e-01, 1.1600e-01]])\n",
            "tensor([[0.0000e+00, 3.3000e+01, 2.0812e-01, 2.3946e-01, 1.0652e-01, 7.9624e-02],\n",
            "        [0.0000e+00, 8.0000e+00, 7.6021e-01, 6.2003e-01, 1.3342e-02, 3.5541e-03],\n",
            "        [0.0000e+00, 0.0000e+00, 7.4376e-01, 6.9310e-01, 3.1615e-02, 3.2518e-02],\n",
            "        [0.0000e+00, 3.7000e+01, 7.1903e-01, 7.0842e-01, 3.3007e-02, 9.6699e-03]])\n",
            "tensor([[ 0.0000, 75.0000,  0.3695,  0.7750,  0.1591,  0.4501]])\n",
            "tensor([[0.0000e+00, 5.6000e+01, 8.4977e-01, 6.2032e-01, 1.0045e-01, 7.5935e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.9756e-01, 5.0000e-01, 7.9512e-01, 1.0000e+00],\n",
            "        [0.0000e+00, 6.5000e+01, 3.9396e-01, 9.7039e-01, 1.3309e-01, 5.9229e-02],\n",
            "        [0.0000e+00, 6.5000e+01, 6.3524e-01, 9.3228e-01, 1.0942e-01, 1.1645e-01]])\n",
            "tensor([[ 0.0000, 14.0000,  0.5067,  0.5107,  0.9865,  0.4910]])\n",
            "tensor([[0.0000e+00, 7.2000e+01, 1.8633e-01, 5.2310e-01, 1.7400e-01, 3.7742e-01],\n",
            "        [0.0000e+00, 4.1000e+01, 5.3436e-03, 7.0718e-01, 1.0687e-02, 8.0496e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 3.2865e-01, 5.2649e-01, 2.5142e-02, 4.2280e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 3.0649e-01, 5.2492e-01, 2.5205e-02, 4.2364e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 3.6827e-01, 5.2487e-01, 2.0784e-02, 2.9898e-02],\n",
            "        [0.0000e+00, 6.9000e+01, 4.6891e-01, 6.8414e-01, 2.2655e-01, 2.4746e-01],\n",
            "        [0.0000e+00, 7.1000e+01, 7.5923e-01, 5.5293e-01, 1.8532e-01, 5.4223e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 9.6906e-01, 6.3055e-01, 2.8851e-02, 7.6201e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 3.5683e-01, 4.9015e-01, 2.9709e-02, 8.2654e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 3.3033e-01, 4.7909e-01, 3.0149e-02, 6.6730e-02],\n",
            "        [0.0000e+00, 4.4000e+01, 6.3956e-01, 5.4339e-01, 4.5569e-02, 1.7369e-02]])\n",
            "tensor([[ 0.0000, 16.0000,  0.5189,  0.5227,  0.7523,  0.3099],\n",
            "        [ 0.0000, 59.0000,  0.7666,  0.5117,  0.3726,  0.4265]])\n",
            "tensor([[ 0.0000,  0.0000,  0.4993,  0.4512,  0.4293,  0.4293],\n",
            "        [ 0.0000, 37.0000,  0.5203,  0.7066,  0.8168,  0.0530]])\n",
            "tensor([[ 0.0000, 22.0000,  0.8377,  0.5591,  0.3247,  0.3126],\n",
            "        [ 0.0000, 22.0000,  0.5401,  0.5551,  0.3348,  0.2590],\n",
            "        [ 0.0000, 22.0000,  0.2317,  0.5464,  0.3037,  0.2630],\n",
            "        [ 0.0000, 22.0000,  0.9148,  0.2406,  0.0718,  0.0396],\n",
            "        [ 0.0000, 22.0000,  0.9757,  0.2394,  0.0486,  0.0341],\n",
            "        [ 0.0000, 22.0000,  0.0513,  0.5439,  0.1025,  0.2358]])\n",
            "tensor([[0.0000e+00, 3.0000e+00, 4.7588e-01, 4.8032e-01, 9.5176e-01, 5.2939e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.8209e-01, 3.1392e-01, 2.3644e-01, 1.9659e-01],\n",
            "        [0.0000e+00, 3.9000e+01, 3.4747e-03, 2.2536e-01, 6.9495e-03, 1.9476e-02],\n",
            "        [0.0000e+00, 5.6000e+01, 8.6829e-01, 4.0187e-01, 1.2985e-01, 1.8692e-01],\n",
            "        [0.0000e+00, 4.1000e+01, 3.8364e-02, 2.2348e-01, 2.8920e-02, 1.5709e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 7.7080e-02, 2.2469e-01, 3.1276e-02, 1.8133e-02],\n",
            "        [0.0000e+00, 4.1000e+01, 1.0879e-02, 2.2543e-01, 2.1759e-02, 1.9616e-02]])\n",
            "tensor([[0.0000e+00, 3.9000e+01, 8.7013e-01, 5.6991e-01, 1.8513e-02, 6.2219e-02],\n",
            "        [0.0000e+00, 5.9000e+01, 4.6880e-01, 6.0704e-01, 6.8823e-01, 3.2772e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 3.0843e-01, 5.5221e-01, 2.6236e-01, 3.4801e-01]])\n",
            "tensor([[0.0000e+00, 6.7000e+01, 5.2929e-01, 6.7089e-01, 2.3992e-02, 3.8751e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.3935e-01, 7.2020e-01, 3.3953e-01, 3.8834e-01]])\n",
            "tensor([[ 0.0000, 32.0000,  0.6318,  0.0722,  0.1264,  0.1277],\n",
            "        [ 0.0000,  0.0000,  0.8653,  0.5989,  0.1413,  0.6750],\n",
            "        [ 0.0000,  0.0000,  0.7160,  0.5737,  0.2421,  0.5818],\n",
            "        [ 0.0000,  0.0000,  0.5504,  0.5513,  0.1508,  0.6206],\n",
            "        [ 0.0000,  0.0000,  0.3564,  0.6572,  0.5848,  0.6857]])\n",
            "tensor([[ 0.0000, 17.0000,  0.5889,  0.6932,  0.0882,  0.2179],\n",
            "        [ 0.0000, 19.0000,  0.2266,  0.2479,  0.0563,  0.1061],\n",
            "        [ 0.0000, 19.0000,  0.3812,  0.4056,  0.1314,  0.1962],\n",
            "        [ 0.0000, 19.0000,  0.7657,  0.3057,  0.1095,  0.1441],\n",
            "        [ 0.0000,  0.0000,  0.5822,  0.4783,  0.1444,  0.1773],\n",
            "        [ 0.0000,  0.0000,  0.5950,  0.5576,  0.1287,  0.1896],\n",
            "        [ 0.0000, 17.0000,  0.3822,  0.4069,  0.1296,  0.1986]])\n",
            "tensor([[0.0000, 2.0000, 0.3193, 0.2962, 0.6332, 0.3351],\n",
            "        [0.0000, 3.0000, 0.4701, 0.5790, 0.9402, 0.5680],\n",
            "        [0.0000, 0.0000, 0.4434, 0.5079, 0.4898, 0.7132],\n",
            "        [0.0000, 0.0000, 0.6911, 0.4506, 0.1694, 0.4965],\n",
            "        [0.0000, 2.0000, 0.8521, 0.3148, 0.1760, 0.1512]])\n",
            "tensor([[ 0.0000,  0.0000,  0.4622,  0.5483,  0.1291,  0.3636],\n",
            "        [ 0.0000, 30.0000,  0.4953,  0.7239,  0.2952,  0.0559]])\n",
            "tensor([[ 0.0000, 67.0000,  0.3301,  0.5749,  0.1308,  0.4388],\n",
            "        [ 0.0000,  0.0000,  0.5000,  0.4992,  1.0000,  0.6672]])\n",
            "tensor([[ 0.0000, 17.0000,  0.5763,  0.5670,  0.1204,  0.0927],\n",
            "        [ 0.0000, 17.0000,  0.3562,  0.5633,  0.0489,  0.0939],\n",
            "        [ 0.0000, 17.0000,  0.4986,  0.5762,  0.0607,  0.0648],\n",
            "        [ 0.0000, 17.0000,  0.4132,  0.5642,  0.0520,  0.0909],\n",
            "        [ 0.0000, 17.0000,  0.4552,  0.5623,  0.0434,  0.0905],\n",
            "        [ 0.0000, 17.0000,  0.5037,  0.5340,  0.0679,  0.0444]])\n",
            "tensor([[0.0000e+00, 5.8000e+01, 3.9407e-01, 2.5704e-01, 1.0136e-01, 1.4026e-01],\n",
            "        [0.0000e+00, 1.5000e+01, 5.7262e-01, 3.7649e-01, 1.9745e-01, 1.2859e-01],\n",
            "        [0.0000e+00, 5.6000e+01, 5.0403e-01, 7.1314e-01, 4.5898e-01, 5.0835e-01],\n",
            "        [0.0000e+00, 6.3000e+01, 3.4863e-01, 3.9072e-01, 3.1156e-01, 2.6757e-01],\n",
            "        [0.0000e+00, 6.6000e+01, 3.5900e-01, 4.5106e-01, 1.8321e-01, 5.9141e-02],\n",
            "        [0.0000e+00, 5.6000e+01, 2.5940e-01, 3.1708e-01, 1.9939e-01, 3.0290e-01]])\n",
            "tensor([[0.0000e+00, 2.5000e+01, 2.6205e-01, 3.3938e-01, 2.0301e-01, 9.7702e-02],\n",
            "        [0.0000e+00, 7.0000e+00, 4.8769e-01, 4.7537e-01, 5.3140e-01, 2.7018e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.5718e-01, 4.7975e-01, 6.4787e-02, 1.7259e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 9.1590e-01, 4.6195e-01, 1.0496e-01, 2.0063e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.2511e-01, 3.8485e-01, 3.0879e-02, 3.4198e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.8175e-01, 4.0289e-01, 4.8428e-02, 6.6281e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.1281e-01, 4.6259e-01, 5.9576e-02, 1.0261e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.5671e-01, 4.1080e-01, 3.8747e-02, 8.7890e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.3908e-01, 3.8700e-01, 2.7084e-02, 5.4485e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 6.8008e-01, 3.9628e-01, 3.1778e-02, 6.1361e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.4732e-01, 5.5352e-01, 1.7353e-01, 3.4435e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 3.0610e-01, 4.1243e-01, 2.7824e-02, 3.3788e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.1818e-01, 4.0660e-01, 3.7729e-02, 8.3843e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.0069e-01, 4.0985e-01, 3.7386e-02, 9.0310e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.7625e-01, 3.8165e-01, 1.5803e-02, 3.2479e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 9.2477e-01, 4.5195e-01, 1.5046e-01, 1.8514e-01]])\n",
            "tensor([[ 0.0000, 11.0000,  0.5122,  0.3729,  0.5901,  0.6401]])\n",
            "tensor([[0.0000, 6.0000, 0.5657, 0.5250, 0.3608, 0.2683]])\n",
            "tensor([[0.0000e+00, 7.1000e+01, 7.0252e-01, 5.7988e-01, 1.6342e-01, 6.0683e-02],\n",
            "        [0.0000e+00, 7.1000e+01, 9.7581e-01, 6.3905e-01, 4.8371e-02, 3.6308e-02]])\n",
            "tensor([[0.0000e+00, 3.0000e+00, 1.4188e-01, 1.6026e-01, 3.5058e-02, 4.0190e-02],\n",
            "        [0.0000e+00, 7.5000e+01, 8.6994e-01, 2.6543e-01, 2.6012e-01, 3.4649e-01]])\n",
            "tensor([[0.0000, 2.0000, 0.6255, 0.8190, 0.3623, 0.0245],\n",
            "        [0.0000, 2.0000, 0.5342, 0.8137, 0.1455, 0.0351],\n",
            "        [0.0000, 6.0000, 0.5644, 0.6633, 0.8711, 0.1177],\n",
            "        [0.0000, 2.0000, 0.8531, 0.8154, 0.1200, 0.0317],\n",
            "        [0.0000, 2.0000, 0.9521, 0.8104, 0.0957, 0.0416],\n",
            "        [0.0000, 7.0000, 0.9528, 0.8112, 0.0943, 0.0401]])\n",
            "tensor([[ 0.0000, 27.0000,  0.4454,  0.6778,  0.2972,  0.4445],\n",
            "        [ 0.0000,  0.0000,  0.4416,  0.5431,  0.8832,  0.6982]])\n",
            "tensor([[ 0.0000, 54.0000,  0.6531,  0.5000,  0.4438,  1.0000],\n",
            "        [ 0.0000, 41.0000,  0.3456,  0.5000,  0.4412,  1.0000]])\n",
            "tensor([[0.0000e+00, 6.7000e+01, 7.4479e-01, 4.5065e-01, 2.0619e-02, 2.0245e-02],\n",
            "        [0.0000e+00, 6.7000e+01, 4.2794e-01, 4.6417e-01, 2.9445e-02, 3.2871e-02],\n",
            "        [0.0000e+00, 6.7000e+01, 2.9810e-02, 4.8153e-01, 2.8765e-02, 7.7076e-03],\n",
            "        [0.0000e+00, 0.0000e+00, 3.3980e-01, 5.0000e-01, 1.9975e-01, 5.4062e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.7008e-01, 5.1468e-01, 2.5985e-01, 5.1127e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.0162e-01, 5.4193e-01, 2.0323e-01, 4.5677e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.3739e-03, 5.5123e-01, 1.2748e-02, 4.3817e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.7638e-01, 4.5482e-01, 1.6901e-01, 3.6024e-01],\n",
            "        [0.0000e+00, 2.4000e+01, 9.5624e-01, 5.4942e-01, 8.7524e-02, 2.2279e-01],\n",
            "        [0.0000e+00, 2.6000e+01, 5.6471e-01, 5.5538e-01, 1.4167e-01, 3.5851e-01]])\n",
            "tensor([[0.0000e+00, 1.7000e+01, 3.7578e-01, 6.3600e-01, 2.1988e-01, 2.9051e-01],\n",
            "        [0.0000e+00, 1.7000e+01, 1.6936e-01, 6.2709e-01, 1.5087e-01, 3.0833e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 3.6830e-01, 5.4487e-01, 2.5255e-01, 3.2719e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.9648e-01, 4.3258e-01, 1.2227e-01, 1.4491e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.1616e-02, 5.2806e-01, 3.4615e-02, 5.6584e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 3.9126e-02, 5.2752e-01, 1.4554e-02, 2.1399e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 3.9170e-03, 5.3359e-01, 7.8341e-03, 2.6571e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 6.6391e-01, 5.1939e-01, 1.1368e-02, 1.3237e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 1.1771e-01, 5.3608e-01, 7.3359e-03, 8.7721e-03]])\n",
            "tensor([[0.0000e+00, 5.9000e+01, 6.4729e-01, 6.1251e-01, 7.0541e-01, 4.4061e-01],\n",
            "        [0.0000e+00, 5.9000e+01, 5.8045e-01, 5.2054e-01, 6.9695e-01, 2.6501e-01],\n",
            "        [0.0000e+00, 7.4000e+01, 9.8617e-01, 5.2476e-01, 1.8635e-02, 2.7506e-02]])\n",
            "tensor([[ 0.0000, 53.0000,  0.5521,  0.5647,  0.7128,  0.4319]])\n",
            "tensor([[0.0000e+00, 6.7000e+01, 6.0114e-01, 6.2251e-01, 6.8148e-02, 9.4324e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.2891e-01, 4.9922e-01, 5.4218e-01, 6.6719e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 3.6822e-01, 5.2717e-01, 4.0799e-01, 6.1128e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 3.8659e-02, 4.9922e-01, 7.7319e-02, 6.6719e-01],\n",
            "        [0.0000e+00, 2.6000e+01, 4.4287e-01, 6.1153e-01, 2.8645e-01, 4.4257e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.5731e-01, 4.4367e-01, 7.7571e-02, 1.2128e-01],\n",
            "        [0.0000e+00, 2.6000e+01, 1.5431e-01, 7.2069e-01, 2.5201e-01, 2.2425e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.9436e-01, 4.3927e-01, 6.7412e-02, 1.0642e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 3.8859e-02, 5.2703e-01, 7.7718e-02, 3.9039e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.9240e-02, 5.1157e-01, 8.7306e-02, 1.6156e-01]])\n",
            "tensor([[0.0000e+00, 0.0000e+00, 3.9140e-01, 4.9185e-01, 4.5935e-01, 5.1972e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.6724e-01, 5.2707e-01, 3.3592e-02, 1.0740e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.3339e-01, 5.9551e-01, 2.3895e-02, 6.4176e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.9622e-02, 6.0609e-01, 3.9296e-02, 5.7227e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 9.0967e-01, 5.3109e-01, 2.1980e-02, 5.1168e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.8850e-01, 5.3892e-01, 3.3264e-02, 4.8022e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.2317e-01, 5.5569e-01, 3.4358e-02, 8.2860e-02],\n",
            "        [0.0000e+00, 3.3000e+01, 7.6405e-01, 2.8156e-01, 5.3887e-03, 6.5792e-03],\n",
            "        [0.0000e+00, 3.3000e+01, 8.4581e-01, 4.9761e-01, 4.8036e-02, 4.6750e-02],\n",
            "        [0.0000e+00, 3.3000e+01, 6.9781e-01, 3.0121e-01, 1.2214e-02, 1.2050e-02],\n",
            "        [0.0000e+00, 3.3000e+01, 7.6255e-01, 4.7615e-01, 2.9886e-02, 1.3568e-02],\n",
            "        [0.0000e+00, 3.3000e+01, 9.1815e-01, 4.1795e-01, 6.9075e-03, 5.3889e-03],\n",
            "        [0.0000e+00, 3.3000e+01, 7.9732e-01, 3.3690e-01, 1.3021e-02, 8.2886e-03],\n",
            "        [0.0000e+00, 3.3000e+01, 8.3814e-01, 3.4854e-01, 8.6994e-03, 5.6897e-03],\n",
            "        [0.0000e+00, 3.3000e+01, 4.7649e-01, 3.0909e-01, 4.3126e-02, 2.1994e-02],\n",
            "        [0.0000e+00, 3.3000e+01, 6.6777e-01, 3.3517e-01, 1.7398e-02, 9.6430e-03],\n",
            "        [0.0000e+00, 0.0000e+00, 6.5788e-01, 5.5498e-01, 2.8724e-02, 5.2016e-02],\n",
            "        [0.0000e+00, 3.3000e+01, 6.3726e-01, 4.7956e-01, 8.6303e-03, 9.1367e-03],\n",
            "        [0.0000e+00, 3.3000e+01, 7.1850e-01, 4.4463e-01, 7.4958e-03, 8.0837e-03],\n",
            "        [0.0000e+00, 3.3000e+01, 1.8750e-01, 3.1745e-01, 1.6372e-02, 7.4681e-03],\n",
            "        [0.0000e+00, 3.3000e+01, 8.5524e-01, 3.8900e-01, 3.6792e-03, 4.7459e-03],\n",
            "        [0.0000e+00, 0.0000e+00, 8.3713e-01, 5.4715e-01, 1.3514e-02, 3.7395e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.7976e-01, 5.1921e-01, 6.0921e-02, 8.8180e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.9962e-01, 5.0910e-01, 5.0840e-02, 1.2414e-01],\n",
            "        [0.0000e+00, 3.3000e+01, 4.1553e-01, 4.3446e-01, 7.3586e-01, 4.5410e-01]])\n",
            "tensor([[ 0.0000, 37.0000,  0.1387,  0.5796,  0.2675,  0.0796],\n",
            "        [ 0.0000, 37.0000,  0.7844,  0.5360,  0.3122,  0.0708],\n",
            "        [ 0.0000,  0.0000,  0.7365,  0.5575,  0.1052,  0.1987],\n",
            "        [ 0.0000,  0.0000,  0.1339,  0.5808,  0.0956,  0.2505]])\n",
            "tensor([[0.0000, 2.0000, 0.1251, 0.5751, 0.1522, 0.1166],\n",
            "        [0.0000, 9.0000, 0.8039, 0.3479, 0.3922, 0.3639],\n",
            "        [0.0000, 9.0000, 0.4721, 0.3019, 0.3291, 0.2718]])\n",
            "tensor([[ 0.0000,  0.0000,  0.6565,  0.2611,  0.1722,  0.1664],\n",
            "        [ 0.0000, 37.0000,  0.6094,  0.3316,  0.1290,  0.1002]])\n",
            "tensor([[0.0000e+00, 5.0000e+00, 2.2055e-01, 7.1913e-01, 4.4111e-01, 3.1175e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.0120e-01, 7.9777e-01, 5.5488e-02, 8.7791e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.6780e-01, 8.1658e-01, 3.0267e-02, 1.1684e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 2.5478e-01, 7.2527e-01, 2.5966e-02, 2.3558e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.6100e-01, 7.3552e-01, 3.2284e-02, 2.9293e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.4815e-01, 7.2105e-01, 1.5894e-02, 1.9045e-02],\n",
            "        [0.0000e+00, 7.0000e+00, 7.2838e-01, 6.0281e-01, 5.4324e-01, 5.4438e-01],\n",
            "        [0.0000e+00, 2.6000e+01, 4.6123e-01, 8.5567e-01, 2.4673e-02, 3.8657e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 1.8637e-01, 7.3994e-01, 1.8143e-02, 1.9027e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 6.0219e-02, 7.3752e-01, 2.4090e-02, 2.3630e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.9213e-01, 7.8724e-01, 6.5525e-02, 1.6156e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.8839e-02, 7.3952e-01, 2.1541e-02, 2.8515e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.1344e-01, 7.0300e-01, 1.2248e-02, 2.2744e-02],\n",
            "        [0.0000e+00, 2.6000e+01, 4.7145e-01, 8.3003e-01, 1.1823e-02, 1.3469e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.7430e-01, 7.3957e-01, 2.2071e-02, 2.9541e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.1476e-01, 7.2239e-01, 2.3169e-02, 3.0337e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 3.9274e-01, 8.0015e-01, 3.8055e-01, 1.2567e-01]])\n",
            "tensor([[ 0.0000, 16.0000,  0.6662,  0.3679,  0.3006,  0.1320],\n",
            "        [ 0.0000, 37.0000,  0.1632,  0.4635,  0.2724,  0.5692]])\n",
            "tensor([[ 0.0000,  0.0000,  0.6158,  0.4133,  0.2365,  0.2321],\n",
            "        [ 0.0000, 37.0000,  0.6021,  0.3755,  0.2522,  0.1111]])\n",
            "tensor([[0.0000e+00, 0.0000e+00, 6.2360e-01, 6.1001e-01, 7.2775e-02, 9.5072e-02],\n",
            "        [0.0000e+00, 3.0000e+01, 6.3948e-01, 6.5188e-01, 6.4596e-02, 2.3898e-02]])\n",
            "tensor([[ 0.0000, 27.0000,  0.1936,  0.5750,  0.0975,  0.3426],\n",
            "        [ 0.0000,  0.0000,  0.2342,  0.5064,  0.4684,  0.8851]])\n",
            "tensor([[ 0.0000, 48.0000,  0.2699,  0.7456,  0.5397,  0.2588],\n",
            "        [ 0.0000, 41.0000,  0.8878,  0.2639,  0.2245,  0.2778],\n",
            "        [ 0.0000, 42.0000,  0.8536,  0.6721,  0.2927,  0.4057],\n",
            "        [ 0.0000, 60.0000,  0.5000,  0.5000,  1.0000,  0.7500]])\n",
            "tensor([[ 0.0000, 74.0000,  0.5881,  0.4199,  0.0866,  0.1121]])\n",
            "tensor([[0.0000e+00, 6.0000e+01, 5.0000e-01, 9.1092e-01, 6.6250e-01, 1.7815e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.8440e-01, 4.3091e-01, 5.6515e-01, 8.5283e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 2.1922e-01, 2.5877e-01, 9.3053e-02, 2.0956e-01],\n",
            "        [0.0000e+00, 4.2000e+01, 3.0372e-01, 3.4169e-01, 2.5654e-01, 1.0263e-01],\n",
            "        [0.0000e+00, 1.3000e+01, 5.0000e-01, 5.7602e-01, 6.6250e-01, 4.5059e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 3.3215e-01, 1.6977e-01, 1.4237e-01, 2.6100e-01],\n",
            "        [0.0000e+00, 4.1000e+01, 7.9516e-01, 3.8257e-01, 5.3669e-02, 6.6520e-02],\n",
            "        [0.0000e+00, 5.5000e+01, 5.3641e-01, 9.3145e-01, 2.2285e-01, 1.3710e-01]])\n",
            "tensor([[0.0000, 2.0000, 0.1111, 0.6080, 0.2222, 0.2074],\n",
            "        [0.0000, 2.0000, 0.3130, 0.4948, 0.1269, 0.0962],\n",
            "        [0.0000, 3.0000, 0.8020, 0.4303, 0.1672, 0.1672],\n",
            "        [0.0000, 5.0000, 0.5466, 0.4014, 0.4112, 0.2943],\n",
            "        [0.0000, 5.0000, 0.8398, 0.3144, 0.3203, 0.2304],\n",
            "        [0.0000, 5.0000, 0.1517, 0.4519, 0.1693, 0.1216],\n",
            "        [0.0000, 5.0000, 0.3250, 0.4201, 0.0545, 0.0929],\n",
            "        [0.0000, 0.0000, 0.0309, 0.6101, 0.0618, 0.2652],\n",
            "        [0.0000, 0.0000, 0.2554, 0.4675, 0.0328, 0.0328],\n",
            "        [0.0000, 0.0000, 0.3445, 0.4528, 0.0145, 0.0140],\n",
            "        [0.0000, 2.0000, 0.2907, 0.4417, 0.0255, 0.0321],\n",
            "        [0.0000, 7.0000, 0.1084, 0.5143, 0.1822, 0.0955],\n",
            "        [0.0000, 5.0000, 0.0132, 0.4714, 0.0265, 0.0695]])\n",
            "tensor([[ 0.0000, 22.0000,  0.6518,  0.5411,  0.3398,  0.2912],\n",
            "        [ 0.0000, 22.0000,  0.3552,  0.5466,  0.5558,  0.3490]])\n",
            "tensor([[ 0.0000, 14.0000,  0.7185,  0.5663,  0.5630,  0.3773]])\n",
            "tensor([[ 0.0000, 31.0000,  0.5240,  0.5117,  0.1836,  0.0688],\n",
            "        [ 0.0000,  0.0000,  0.3688,  0.3626,  0.2850,  0.4127]])\n",
            "tensor([[ 0.0000, 42.0000,  0.4837,  0.7160,  0.8105,  0.3180],\n",
            "        [ 0.0000, 52.0000,  0.5454,  0.4589,  0.9092,  0.5212]])\n",
            "tensor([[0.0000e+00, 7.2000e+01, 1.7028e-01, 7.1849e-01, 9.0563e-02, 5.6302e-01],\n",
            "        [0.0000e+00, 6.9000e+01, 3.2989e-01, 8.1121e-01, 2.7357e-01, 3.7757e-01],\n",
            "        [0.0000e+00, 7.1000e+01, 7.6538e-01, 6.0434e-01, 2.1925e-01, 3.8448e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 3.1556e-01, 5.3345e-01, 3.4340e-02, 6.7223e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 3.0151e-01, 5.7745e-01, 3.2297e-02, 8.4040e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 2.8833e-01, 5.3457e-01, 1.1940e-02, 5.2958e-02],\n",
            "        [0.0000e+00, 4.5000e+01, 4.4128e-01, 5.7178e-01, 6.4046e-02, 2.3514e-02],\n",
            "        [0.0000e+00, 4.5000e+01, 4.8995e-01, 5.6558e-01, 5.2552e-02, 3.9946e-02]])\n",
            "tensor([[ 0.0000,  0.0000,  0.4067,  0.5662,  0.4789,  0.8676],\n",
            "        [ 0.0000, 13.0000,  0.6676,  0.7558,  0.3305,  0.4496],\n",
            "        [ 0.0000,  0.0000,  0.7714,  0.4265,  0.1227,  0.4578]])\n",
            "tensor([[ 0.0000, 59.0000,  0.5000,  0.5377,  1.0000,  0.8765],\n",
            "        [ 0.0000, 77.0000,  0.5001,  0.5353,  0.9538,  0.8814]])\n",
            "tensor([[0.0000e+00, 7.4000e+01, 3.3585e-01, 3.8012e-01, 7.0457e-02, 6.5619e-02]])\n",
            "tensor([[ 0.0000,  0.0000,  0.3561,  0.4267,  0.3554,  0.5503],\n",
            "        [ 0.0000, 13.0000,  0.5128,  0.5635,  0.8455,  0.5699],\n",
            "        [ 0.0000, 13.0000,  0.1262,  0.4116,  0.2524,  0.2954],\n",
            "        [ 0.0000, 26.0000,  0.2140,  0.5412,  0.1470,  0.4675]])\n",
            "tensor([[0.0000e+00, 0.0000e+00, 8.9399e-01, 5.6426e-01, 1.9208e-01, 3.2532e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.9958e-01, 6.4006e-01, 4.8177e-01, 3.2220e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.6868e-01, 4.7741e-01, 1.5800e-01, 4.1516e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 3.5026e-01, 4.8262e-01, 2.7884e-01, 6.3399e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 2.0378e-01, 4.8727e-01, 3.3593e-01, 6.4329e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.3341e-01, 4.3656e-01, 6.4202e-01, 5.4187e-01],\n",
            "        [0.0000e+00, 1.0000e+00, 4.6422e-01, 4.5012e-01, 7.5361e-02, 1.2297e-01],\n",
            "        [0.0000e+00, 5.4000e+01, 1.3074e-01, 5.4242e-01, 1.0046e-01, 6.7434e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 9.6946e-01, 6.8163e-01, 6.1082e-02, 2.3546e-01],\n",
            "        [0.0000e+00, 2.0000e+00, 4.4220e-01, 3.6427e-01, 3.7357e-02, 7.6571e-02],\n",
            "        [0.0000e+00, 5.4000e+01, 5.2369e-01, 6.1986e-01, 4.3944e-02, 4.5381e-02],\n",
            "        [0.0000e+00, 5.4000e+01, 8.4523e-01, 5.3497e-01, 3.2240e-02, 2.5056e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.9895e-01, 3.7994e-01, 2.8623e-02, 1.6475e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.1415e-01, 3.4036e-01, 3.2385e-02, 1.1107e-01]])\n",
            "tensor([[ 0.0000,  0.0000,  0.4557,  0.5220,  0.5527,  0.6185],\n",
            "        [ 0.0000, 35.0000,  0.2780,  0.5538,  0.1978,  0.1404]])\n",
            "tensor([[0.0000e+00, 6.2000e+01, 4.3130e-01, 7.1847e-02, 4.8083e-01, 1.4369e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.0000e-01, 5.0000e-01, 8.3125e-01, 1.0000e+00],\n",
            "        [0.0000e+00, 2.7000e+01, 4.8237e-01, 6.3091e-01, 2.2466e-01, 7.3818e-01],\n",
            "        [0.0000e+00, 5.7000e+01, 9.1460e-01, 7.6293e-01, 2.0477e-03, 3.7010e-01],\n",
            "        [0.0000e+00, 6.3000e+01, 8.1557e-01, 8.9121e-01, 1.5772e-01, 8.4329e-02]])\n",
            "tensor([[0.0000, 9.0000, 0.6686, 0.9014, 0.2041, 0.0909]])\n",
            "tensor([[0.0000e+00, 6.7000e+01, 6.6169e-01, 6.0179e-01, 9.2542e-02, 4.5913e-02],\n",
            "        [0.0000e+00, 6.7000e+01, 7.9670e-01, 3.7621e-01, 1.0213e-01, 8.9153e-02],\n",
            "        [0.0000e+00, 6.7000e+01, 2.2341e-01, 4.4643e-01, 3.9286e-02, 2.6772e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.4614e-01, 3.7634e-01, 1.4183e-01, 1.6160e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 2.4415e-01, 3.9248e-01, 1.5120e-01, 1.8689e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.5869e-01, 3.8974e-01, 1.2004e-01, 1.8503e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.0202e-01, 3.2798e-01, 1.2816e-01, 6.7816e-02],\n",
            "        [0.0000e+00, 6.3000e+01, 2.3150e-01, 5.7190e-01, 1.7148e-01, 1.2172e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.3664e-01, 4.2506e-01, 1.6001e-01, 1.1891e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.1906e-01, 3.9651e-01, 1.9135e-01, 1.8594e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.8148e-01, 3.7520e-01, 1.0588e-01, 1.5782e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 2.4035e-01, 6.4608e-01, 1.4803e-01, 7.1305e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.1122e-01, 5.8740e-01, 1.4171e-01, 6.5895e-02],\n",
            "        [0.0000e+00, 5.6000e+01, 2.1349e-01, 4.0384e-01, 7.3966e-02, 1.0050e-01],\n",
            "        [0.0000e+00, 5.6000e+01, 1.6136e-01, 4.2180e-01, 6.6096e-02, 8.9316e-02],\n",
            "        [0.0000e+00, 5.6000e+01, 3.0163e-01, 4.7275e-01, 3.7065e-02, 2.5668e-02],\n",
            "        [0.0000e+00, 6.7000e+01, 3.7037e-01, 5.9653e-01, 8.5162e-02, 2.8944e-02],\n",
            "        [0.0000e+00, 6.7000e+01, 1.9586e-01, 6.0515e-01, 9.9897e-02, 9.5216e-02],\n",
            "        [0.0000e+00, 6.7000e+01, 8.7056e-01, 6.0252e-01, 2.3622e-02, 1.0230e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.1996e-01, 5.8651e-01, 1.9495e-01, 1.8864e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.0129e-01, 5.8591e-01, 1.6859e-01, 1.8943e-01],\n",
            "        [0.0000e+00, 5.6000e+01, 5.8249e-01, 3.9466e-01, 9.9910e-02, 2.6798e-02],\n",
            "        [0.0000e+00, 5.6000e+01, 5.6706e-01, 3.8337e-01, 8.3995e-02, 3.2596e-02],\n",
            "        [0.0000e+00, 6.3000e+01, 4.6047e-01, 4.5118e-01, 5.4725e-02, 2.5555e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.1370e-01, 3.3548e-01, 6.4289e-02, 7.6890e-02],\n",
            "        [0.0000e+00, 6.7000e+01, 8.9484e-01, 5.8703e-01, 1.6995e-02, 1.2451e-02],\n",
            "        [0.0000e+00, 6.7000e+01, 4.0500e-01, 4.6764e-01, 4.4005e-02, 3.6010e-02]])\n",
            "tensor([[ 0.0000,  0.0000,  0.0961,  0.6289,  0.1710,  0.3202],\n",
            "        [ 0.0000, 26.0000,  0.0732,  0.6025,  0.0328,  0.0623],\n",
            "        [ 0.0000, 25.0000,  0.0970,  0.4797,  0.1558,  0.0888]])\n",
            "tensor([[ 0.0000, 64.0000,  0.5577,  0.2281,  0.1547,  0.2083],\n",
            "        [ 0.0000, 66.0000,  0.5617,  0.4708,  0.8766,  0.4810],\n",
            "        [ 0.0000, 64.0000,  0.2938,  0.2398,  0.1881,  0.1505]])\n",
            "tensor([[0.0000e+00, 1.0000e+00, 8.8728e-01, 4.3744e-01, 7.3527e-02, 4.4085e-02],\n",
            "        [0.0000e+00, 2.0000e+00, 6.8053e-01, 4.3288e-01, 1.3183e-01, 6.1873e-02],\n",
            "        [0.0000e+00, 2.0000e+00, 2.9600e-01, 4.0493e-01, 6.1643e-02, 3.7231e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.1678e-01, 4.4621e-01, 8.1608e-02, 2.1626e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.8464e-02, 3.9583e-01, 4.1524e-02, 1.1260e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.8933e-01, 4.2597e-01, 3.4946e-02, 6.8589e-02],\n",
            "        [0.0000e+00, 9.0000e+00, 2.8274e-01, 2.8605e-01, 1.2513e-02, 2.8644e-02],\n",
            "        [0.0000e+00, 9.0000e+00, 4.0982e-01, 2.2611e-01, 5.5893e-02, 5.9282e-02],\n",
            "        [0.0000e+00, 9.0000e+00, 3.4599e-01, 2.9183e-01, 1.5396e-02, 2.6436e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.7413e-01, 3.5815e-01, 1.7649e-02, 1.8155e-02],\n",
            "        [0.0000e+00, 9.0000e+00, 1.4213e-01, 3.2035e-01, 9.2770e-03, 1.5549e-02],\n",
            "        [0.0000e+00, 9.0000e+00, 7.2170e-02, 3.2258e-01, 1.2175e-02, 2.3461e-02],\n",
            "        [0.0000e+00, 2.4000e+01, 2.3132e-02, 4.0792e-01, 3.0238e-02, 5.1292e-02],\n",
            "        [0.0000e+00, 9.0000e+00, 8.6762e-01, 3.3776e-01, 2.0195e-02, 1.5088e-02],\n",
            "        [0.0000e+00, 2.4000e+01, 9.0102e-01, 4.1105e-01, 2.5455e-02, 2.2250e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.7699e-01, 4.5334e-01, 3.0116e-02, 1.7903e-01]])\n",
            "tensor([[0.0000e+00, 1.7000e+01, 6.1454e-01, 8.2482e-01, 1.0026e-01, 6.4954e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 9.9231e-01, 8.5291e-01, 1.5384e-02, 8.2189e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 9.0475e-01, 8.3766e-01, 1.5658e-02, 5.9922e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.7026e-01, 8.3289e-01, 1.5152e-02, 4.1198e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 6.7356e-01, 8.2939e-01, 2.6047e-02, 6.2899e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 3.9403e-01, 8.2650e-01, 2.3993e-02, 6.5817e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 3.2983e-01, 8.3055e-01, 1.9498e-02, 8.0403e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.0208e-02, 8.3795e-01, 4.0417e-02, 1.1211e-01],\n",
            "        [0.0000e+00, 7.0000e+00, 5.0061e-01, 8.2665e-01, 9.3500e-02, 8.5701e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 9.3157e-01, 8.3067e-01, 2.5422e-02, 6.9686e-02],\n",
            "        [0.0000e+00, 1.0000e+01, 8.4969e-01, 8.9351e-01, 4.8491e-02, 9.8334e-04],\n",
            "        [0.0000e+00, 0.0000e+00, 2.0149e-01, 8.4540e-01, 4.0008e-02, 8.5582e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 3.1065e-01, 8.3320e-01, 3.2089e-02, 8.6892e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.8975e-01, 8.3585e-01, 2.6255e-02, 8.5820e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.7518e-01, 8.4871e-01, 5.2004e-02, 9.0584e-02],\n",
            "        [0.0000e+00, 7.0000e+00, 8.5276e-01, 8.1875e-01, 6.5340e-02, 7.1442e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.6635e-01, 8.1267e-01, 9.3270e-01, 1.6265e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.6635e-01, 8.1267e-01, 9.3270e-01, 1.6265e-01]])\n",
            "tensor([[ 0.0000, 74.0000,  0.4981,  0.2935,  0.2093,  0.2102]])\n",
            "tensor([[ 0.0000, 56.0000,  0.6730,  0.8119,  0.1851,  0.1143],\n",
            "        [ 0.0000, 63.0000,  0.6004,  0.9309,  0.3331,  0.1382]])\n",
            "tensor([[0.0000e+00, 5.0000e+00, 5.1397e-01, 5.0000e-01, 8.2428e-01, 5.6250e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.0285e-01, 3.1444e-01, 6.2379e-02, 9.2306e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.2267e-03, 5.9347e-01, 4.4534e-03, 3.3092e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.5698e-01, 3.5651e-01, 7.7745e-02, 7.2367e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.0974e-01, 3.7305e-01, 4.3665e-02, 6.3110e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.9065e-01, 4.0388e-01, 2.1769e-02, 5.2153e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 1.9201e-01, 5.7074e-01, 2.3818e-02, 5.9561e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 6.5414e-01, 6.4295e-01, 5.4238e-02, 2.3589e-01]])\n",
            "tensor([[0.0000e+00, 3.2000e+01, 5.0158e-01, 5.3321e-01, 4.7871e-02, 4.2327e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 3.2217e-01, 4.4798e-01, 2.4886e-01, 3.9595e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.6501e-01, 4.9591e-01, 4.0496e-01, 4.9183e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 2.1092e-01, 6.5240e-01, 4.2184e-01, 1.9207e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.9817e-01, 5.6824e-01, 2.6855e-01, 3.6039e-01],\n",
            "        [0.0000e+00, 3.5000e+01, 5.8040e-01, 5.4219e-01, 1.0213e-01, 8.3411e-02],\n",
            "        [0.0000e+00, 3.4000e+01, 5.3770e-01, 5.2535e-01, 2.1001e-01, 1.4512e-01],\n",
            "        [0.0000e+00, 3.5000e+01, 5.8337e-01, 5.9983e-01, 4.7126e-02, 2.9117e-02]])\n",
            "tensor([[ 0.0000, 15.0000,  0.8256,  0.4986,  0.3049,  0.1498],\n",
            "        [ 0.0000, 15.0000,  0.0973,  0.4726,  0.1534,  0.2021],\n",
            "        [ 0.0000,  0.0000,  0.4077,  0.6601,  0.8153,  0.4298],\n",
            "        [ 0.0000, 59.0000,  0.5000,  0.5838,  1.0000,  0.5825]])\n",
            "tensor([[0.0000e+00, 1.4000e+01, 7.2642e-01, 1.8613e-01, 4.5258e-02, 3.1288e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.0737e-01, 6.1578e-01, 3.0734e-01, 4.3407e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.9443e-01, 5.2182e-01, 1.0610e-01, 1.2499e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.1744e-01, 5.8803e-01, 1.5570e-01, 2.4705e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.4526e-01, 6.9192e-01, 1.5107e-01, 1.4238e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.1500e-01, 5.7686e-01, 7.3850e-02, 9.1746e-02],\n",
            "        [0.0000e+00, 1.4000e+01, 4.3738e-01, 2.3127e-01, 4.7954e-02, 6.0097e-02],\n",
            "        [0.0000e+00, 1.4000e+01, 8.4264e-01, 1.9342e-01, 3.4455e-02, 3.7042e-02],\n",
            "        [0.0000e+00, 1.4000e+01, 9.8022e-01, 1.8101e-01, 3.9568e-02, 3.0770e-02],\n",
            "        [0.0000e+00, 1.4000e+01, 4.6295e-01, 2.2781e-01, 8.6462e-02, 1.2436e-01],\n",
            "        [0.0000e+00, 1.4000e+01, 1.0672e-02, 7.7369e-01, 1.5219e-02, 2.0050e-02],\n",
            "        [0.0000e+00, 1.4000e+01, 6.7998e-01, 6.2341e-01, 4.5783e-02, 5.3691e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 3.9462e-01, 5.3324e-01, 8.9032e-02, 1.0867e-01]])\n",
            "tensor([[ 0.0000, 17.0000,  0.6656,  0.4992,  0.6213,  0.6641],\n",
            "        [ 0.0000, 17.0000,  0.2684,  0.5487,  0.3188,  0.5503]])\n",
            "tensor([[0.0000e+00, 1.4000e+01, 7.2185e-01, 2.9075e-01, 7.2831e-02, 8.1729e-02],\n",
            "        [0.0000e+00, 2.0000e+00, 3.5197e-01, 6.0466e-01, 7.0393e-01, 4.8752e-01],\n",
            "        [0.0000e+00, 2.0000e+00, 6.6410e-01, 5.3577e-01, 6.7181e-01, 4.5105e-01],\n",
            "        [0.0000e+00, 2.0000e+00, 2.9862e-01, 3.3986e-01, 1.8004e-01, 5.5111e-02],\n",
            "        [0.0000e+00, 1.4000e+01, 2.1447e-01, 3.1093e-01, 9.6174e-02, 1.2667e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.0872e-03, 3.4675e-01, 1.6174e-02, 3.4437e-02],\n",
            "        [0.0000e+00, 1.4000e+01, 2.6690e-01, 3.4331e-01, 6.0525e-02, 3.7315e-02],\n",
            "        [0.0000e+00, 7.0000e+00, 7.8457e-02, 3.3148e-01, 1.5691e-01, 5.6057e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 3.4394e-01, 3.4672e-01, 2.0220e-02, 3.0991e-02],\n",
            "        [0.0000e+00, 2.0000e+00, 9.9260e-01, 3.7054e-01, 1.4797e-02, 2.0749e-02],\n",
            "        [0.0000e+00, 6.0000e+01, 9.4677e-01, 3.7729e-01, 5.1646e-02, 4.7145e-03]])\n",
            "tensor([[0.0000, 3.0000, 0.5362, 0.5538, 0.9275, 0.5611],\n",
            "        [0.0000, 0.0000, 0.5082, 0.5000, 0.3518, 0.6687],\n",
            "        [0.0000, 2.0000, 0.0304, 0.3404, 0.0609, 0.0218]])\n",
            "tensor([[ 0.0000, 51.0000,  0.4006,  0.5766,  0.3483,  0.3447],\n",
            "        [ 0.0000, 50.0000,  0.3071,  0.5344,  0.2730,  0.2773],\n",
            "        [ 0.0000, 60.0000,  0.8009,  0.4747,  0.3004,  0.6995]])\n",
            "tensor([[ 0.0000,  0.0000,  0.4992,  0.5542,  0.6672,  0.8916],\n",
            "        [ 0.0000, 77.0000,  0.6752,  0.5623,  0.3153,  0.8582]])\n",
            "tensor([[ 0.0000,  0.0000,  0.4575,  0.7130,  0.1118,  0.3482],\n",
            "        [ 0.0000,  0.0000,  0.5180,  0.7552,  0.1301,  0.2712],\n",
            "        [ 0.0000,  0.0000,  0.7790,  0.7841,  0.0396,  0.0828],\n",
            "        [ 0.0000, 33.0000,  0.5630,  0.1826,  0.0839,  0.0571]])\n",
            "tensor([[0.0000, 3.0000, 0.7147, 0.4927, 0.2197, 0.1333],\n",
            "        [0.0000, 3.0000, 0.1981, 0.5124, 0.0977, 0.0668],\n",
            "        [0.0000, 0.0000, 0.2983, 0.4112, 0.0694, 0.1117],\n",
            "        [0.0000, 0.0000, 0.2785, 0.4466, 0.0500, 0.2174],\n",
            "        [0.0000, 0.0000, 0.6887, 0.4382, 0.1212, 0.1790],\n",
            "        [0.0000, 0.0000, 0.1886, 0.4752, 0.0764, 0.1108],\n",
            "        [0.0000, 3.0000, 0.3654, 0.4970, 0.2408, 0.2674]])\n",
            "tensor([[0.0000e+00, 0.0000e+00, 1.2724e-01, 4.9941e-01, 2.3189e-01, 6.5176e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.0347e-01, 4.4519e-01, 9.4646e-02, 1.3560e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 3.4748e-01, 4.9580e-01, 8.4842e-02, 2.3677e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 2.2302e-01, 5.2197e-01, 8.3984e-02, 3.1412e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.1846e-01, 4.6645e-01, 6.8875e-02, 1.4148e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.9316e-01, 4.5190e-01, 1.3341e-01, 1.9432e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.2122e-01, 5.0239e-01, 7.6106e-02, 9.8306e-02],\n",
            "        [0.0000e+00, 2.4000e+01, 3.5262e-01, 4.4313e-01, 5.0404e-02, 6.9068e-02],\n",
            "        [0.0000e+00, 2.4000e+01, 3.0252e-01, 4.6334e-01, 2.1184e-02, 5.1017e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.2451e-01, 4.7947e-01, 6.5461e-02, 1.6378e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 2.4640e-01, 4.3241e-01, 4.2911e-02, 7.3900e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.5580e-01, 4.2307e-01, 1.7315e-02, 3.1654e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.7087e-01, 4.5046e-01, 4.3944e-02, 7.4285e-02],\n",
            "        [0.0000e+00, 2.4000e+01, 8.5108e-01, 4.6003e-01, 4.7306e-02, 6.0331e-02],\n",
            "        [0.0000e+00, 2.4000e+01, 4.8868e-01, 5.4794e-01, 7.0031e-02, 5.7635e-02],\n",
            "        [0.0000e+00, 2.8000e+01, 3.3821e-01, 7.7349e-01, 1.1175e-01, 1.0214e-01],\n",
            "        [0.0000e+00, 2.8000e+01, 3.8315e-01, 6.7411e-01, 1.0871e-01, 1.2767e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.7790e-01, 5.8730e-01, 8.2146e-02, 3.1025e-01],\n",
            "        [0.0000e+00, 2.8000e+01, 4.5249e-01, 7.7357e-01, 1.2294e-01, 1.0345e-01],\n",
            "        [0.0000e+00, 2.8000e+01, 2.8428e-01, 5.7283e-01, 9.6975e-02, 1.3069e-01],\n",
            "        [0.0000e+00, 2.8000e+01, 6.0549e-01, 8.0328e-01, 2.3779e-01, 4.3857e-02],\n",
            "        [0.0000e+00, 2.8000e+01, 5.8484e-01, 7.1502e-01, 1.2537e-01, 1.6259e-01],\n",
            "        [0.0000e+00, 2.8000e+01, 4.1297e-01, 4.8137e-01, 2.5438e-02, 5.3696e-02],\n",
            "        [0.0000e+00, 2.8000e+01, 4.3314e-01, 7.0622e-01, 7.9975e-02, 5.8790e-02],\n",
            "        [0.0000e+00, 2.8000e+01, 4.5385e-01, 6.7031e-01, 3.1706e-02, 8.6786e-02],\n",
            "        [0.0000e+00, 2.8000e+01, 3.6418e-01, 5.7349e-01, 3.3369e-02, 8.5665e-02],\n",
            "        [0.0000e+00, 2.8000e+01, 4.5514e-01, 5.4075e-01, 5.5202e-02, 7.0293e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.2262e-01, 5.0802e-01, 1.1194e-01, 1.7119e-01]])\n",
            "tensor([[ 0.0000, 16.0000,  0.6288,  0.4992,  0.7299,  0.6672]])\n",
            "tensor([[0.0000, 1.0000, 0.9399, 0.5727, 0.0828, 0.1079],\n",
            "        [0.0000, 5.0000, 0.1688, 0.4335, 0.3376, 0.3599],\n",
            "        [0.0000, 0.0000, 0.9297, 0.5125, 0.1057, 0.1373]])\n",
            "tensor([[ 0.0000, 46.0000,  0.5122,  0.3892,  0.4978,  0.3626],\n",
            "        [ 0.0000, 60.0000,  0.5021,  0.4954,  0.6229,  0.7955]])\n",
            "tensor([[ 0.0000, 11.0000,  0.8425,  0.4275,  0.3149,  0.5857]])\n",
            "tensor([[0.0000e+00, 0.0000e+00, 9.6612e-01, 4.0840e-01, 6.7769e-02, 1.5729e-01],\n",
            "        [0.0000e+00, 6.3000e+01, 8.9523e-01, 4.6547e-01, 9.7840e-02, 5.5831e-02],\n",
            "        [0.0000e+00, 3.9000e+01, 8.8724e-01, 6.8791e-01, 4.6946e-02, 1.6343e-01],\n",
            "        [0.0000e+00, 6.3000e+01, 6.9521e-01, 4.5494e-01, 9.8072e-02, 6.6269e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.0085e-01, 4.9988e-01, 1.2704e-01, 3.7985e-01],\n",
            "        [0.0000e+00, 6.3000e+01, 1.4195e-01, 4.5161e-01, 7.0663e-02, 5.7969e-02]])\n",
            "tensor([[0.0000e+00, 0.0000e+00, 3.7382e-01, 5.7509e-01, 4.7932e-02, 1.3244e-01],\n",
            "        [0.0000e+00, 2.9000e+01, 3.1000e-01, 4.1157e-01, 2.6451e-02, 2.7250e-02]])\n",
            "tensor([[ 0.0000,  0.0000,  0.2827,  0.5425,  0.5654,  0.3539],\n",
            "        [ 0.0000, 37.0000,  0.6630,  0.6748,  0.6740,  0.1251]])\n",
            "tensor([[0.0000e+00, 3.7000e+01, 4.3840e-01, 7.7917e-01, 2.4643e-01, 2.4823e-02],\n",
            "        [0.0000e+00, 3.7000e+01, 6.1992e-01, 6.8832e-01, 9.3634e-02, 1.7775e-02],\n",
            "        [0.0000e+00, 3.7000e+01, 9.3644e-01, 5.6251e-01, 2.2668e-02, 1.5778e-02],\n",
            "        [0.0000e+00, 3.7000e+01, 4.2280e-01, 5.8434e-01, 4.4438e-02, 1.8135e-02],\n",
            "        [0.0000e+00, 3.7000e+01, 1.7999e-01, 7.5216e-01, 8.1223e-02, 1.4925e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.5875e-01, 4.0873e-01, 2.1905e-02, 3.5887e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.0313e-01, 3.9882e-01, 2.9603e-02, 5.6513e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 6.6980e-01, 4.0730e-01, 3.2162e-02, 4.5606e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.5110e-01, 3.8707e-01, 6.9059e-02, 6.3537e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 9.9539e-01, 4.2860e-01, 9.2252e-03, 5.3326e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.2957e-01, 7.3531e-01, 1.6514e-01, 8.7462e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.4516e-01, 6.7296e-01, 2.3862e-01, 5.4381e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 3.6579e-01, 5.6962e-01, 1.6092e-01, 3.0388e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.4360e-01, 5.3190e-01, 8.6026e-02, 4.0421e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 1.0053e-01, 7.3167e-01, 8.7394e-02, 4.3967e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.0210e-05, 7.1789e-01, 1.4042e-04, 5.7972e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 7.3331e-01, 3.8652e-01, 1.7484e-02, 3.2005e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.3776e-01, 5.8120e-01, 8.7553e-01, 3.7032e-01]])\n",
            "tensor([[ 0.0000, 16.0000,  0.8146,  0.7268,  0.3708,  0.2963],\n",
            "        [ 0.0000, 16.0000,  0.2956,  0.3203,  0.0910,  0.2557],\n",
            "        [ 0.0000, 62.0000,  0.2928,  0.3078,  0.4499,  0.3628]])\n",
            "tensor([[ 0.0000,  0.0000,  0.2337,  0.6144,  0.4674,  0.4368],\n",
            "        [ 0.0000, 36.0000,  0.1632,  0.8042,  0.2136,  0.0573]])\n",
            "tensor([[0.0000e+00, 5.6000e+01, 8.7248e-01, 8.0134e-01, 2.5504e-01, 1.3174e-01],\n",
            "        [0.0000e+00, 5.6000e+01, 2.5733e-01, 3.1324e-01, 3.6179e-01, 3.7648e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.7405e-01, 4.0327e-01, 6.0696e-01, 5.5655e-01],\n",
            "        [0.0000e+00, 6.3000e+01, 5.5987e-01, 5.5931e-01, 3.8835e-01, 2.4301e-01],\n",
            "        [0.0000e+00, 5.6000e+01, 9.9931e-01, 7.6458e-01, 1.3867e-03, 2.1433e-01],\n",
            "        [0.0000e+00, 5.6000e+01, 3.5732e-02, 2.7669e-01, 7.1464e-02, 3.0339e-01],\n",
            "        [0.0000e+00, 6.0000e+01, 4.3835e-01, 7.0461e-01, 8.7670e-01, 3.1853e-01],\n",
            "        [0.0000e+00, 4.7000e+01, 5.6873e-01, 5.5585e-01, 4.0906e-02, 3.9611e-02]])\n",
            "tensor([[0.0000e+00, 6.2000e+01, 2.0490e-01, 4.9781e-01, 2.8774e-01, 2.6892e-01],\n",
            "        [0.0000e+00, 6.3000e+01, 5.5727e-01, 5.0793e-01, 1.9690e-01, 1.8424e-01],\n",
            "        [0.0000e+00, 6.4000e+01, 4.0428e-01, 6.8153e-01, 4.7898e-02, 3.7037e-02],\n",
            "        [0.0000e+00, 6.6000e+01, 5.6912e-01, 6.7284e-01, 1.9769e-01, 4.8613e-02],\n",
            "        [0.0000e+00, 6.6000e+01, 5.5344e-01, 5.4101e-01, 1.4815e-01, 4.4140e-02],\n",
            "        [0.0000e+00, 6.2000e+01, 6.4766e-01, 3.4164e-01, 4.2977e-02, 4.0025e-02]])\n",
            "tensor([[0.0000e+00, 7.9000e+01, 2.9711e-01, 6.0299e-01, 3.8331e-01, 3.4641e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.1285e-01, 5.0000e-01, 9.7430e-01, 1.0000e+00],\n",
            "        [0.0000e+00, 4.1000e+01, 1.1800e-01, 4.7567e-01, 1.0750e-01, 1.3065e-01],\n",
            "        [0.0000e+00, 4.1000e+01, 1.3530e-01, 4.7127e-01, 9.8334e-02, 1.4226e-01],\n",
            "        [0.0000e+00, 7.9000e+01, 1.3209e-01, 3.7282e-01, 3.1229e-02, 1.0493e-01]])\n",
            "tensor([[0.0000e+00, 3.9000e+01, 5.8362e-01, 3.6092e-01, 4.0201e-01, 1.2922e-01],\n",
            "        [0.0000e+00, 6.9000e+01, 7.1163e-01, 7.2462e-01, 5.7675e-01, 2.5389e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.0516e-01, 5.1070e-01, 6.0593e-01, 6.8173e-01],\n",
            "        [0.0000e+00, 4.5000e+01, 4.7335e-01, 8.3211e-01, 1.1029e-01, 3.8897e-02],\n",
            "        [0.0000e+00, 6.8000e+01, 8.0886e-01, 3.2369e-01, 3.8228e-01, 3.3741e-01],\n",
            "        [0.0000e+00, 7.4000e+01, 9.7748e-01, 1.9287e-01, 4.5039e-02, 3.7524e-02],\n",
            "        [0.0000e+00, 4.5000e+01, 7.8528e-01, 8.0136e-01, 2.6626e-01, 1.0040e-01]])\n",
            "tensor([[ 0.0000,  0.0000,  0.5163,  0.5000,  0.9675,  1.0000],\n",
            "        [ 0.0000, 34.0000,  0.4970,  0.5011,  0.9939,  0.8934]])\n",
            "tensor([[ 0.0000,  0.0000,  0.5271,  0.6367,  0.3629,  0.7266],\n",
            "        [ 0.0000, 30.0000,  0.2279,  0.6526,  0.1246,  0.6316]])\n",
            "tensor([[ 0.0000,  0.0000,  0.7924,  0.4544,  0.0958,  0.2499],\n",
            "        [ 0.0000, 20.0000,  0.9354,  0.4933,  0.1291,  0.4650],\n",
            "        [ 0.0000, 20.0000,  0.2631,  0.5590,  0.5263,  0.6288],\n",
            "        [ 0.0000, 26.0000,  0.8009,  0.4338,  0.0577,  0.1143]])\n",
            "tensor([[ 0.0000, 59.0000,  0.5613,  0.6085,  0.8774,  0.5329]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lqzdO6DtHMa",
        "outputId": "c48751d0-b795-424a-f8b7-d81af948934b"
      },
      "source": [
        "!mkdir images\n",
        "!cd images\n",
        "\n",
        "# Download Images\n",
        "!wget -c \"https://pjreddie.com/media/files/train2014.zip\" --header \"Referer: pjreddie.com\"\n",
        "!unzip -q train2014.zip && rm train2014.zip\n",
        "!wget -c \"https://pjreddie.com/media/files/val2014.zip\" --header \"Referer: pjreddie.com\"\n",
        "!unzip -q val2014.zip && rm val2014.zip\n",
        "\n",
        "\n",
        "# Download COCO Metadata\n",
        "!wget -c \"https://pjreddie.com/media/files/instances_train-val2014.zip\" --header \"Referer: pjreddie.com\"\n",
        "!wget -c \"https://pjreddie.com/media/files/coco/5k.part\" --header \"Referer: pjreddie.com\"\n",
        "!wget -c \"https://pjreddie.com/media/files/coco/trainvalno5k.part\" --header \"Referer: pjreddie.com\"\n",
        "!wget -c \"https://pjreddie.com/media/files/coco/labels.tgz\" --header \"Referer: pjreddie.com\"\n",
        "!tar xzf labels.tgz\n",
        "!unzip -q instances_train-val2014.zip\n",
        "\n",
        "# Set Up Image Lists\n",
        "!paste <(awk \"{print \\\"$PWD\\\"}\" <5k.part) 5k.part | tr -d '\\t' > 5k.txt\n",
        "!paste <(awk \"{print \\\"$PWD\\\"}\" <trainvalno5k.part) trainvalno5k.part | tr -d '\\t' > trainvalno5k.txt\n",
        "\n",
        "!rm instances_train-val2014.zip 5k.part trainvalno5k.part labels.tgz\n",
        "!rm -rf sample_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-08 10:44:45--  https://pjreddie.com/media/files/train2014.zip\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13510435630 (13G) [application/zip]\n",
            "Saving to: ‘train2014.zip’\n",
            "\n",
            "train2014.zip       100%[===================>]  12.58G  20.3MB/s    in 10m 51s \n",
            "\n",
            "2021-11-08 10:55:36 (19.8 MB/s) - ‘train2014.zip’ saved [13510435630/13510435630]\n",
            "\n",
            "--2021-11-08 10:59:55--  https://pjreddie.com/media/files/val2014.zip\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645013297 (6.2G) [application/zip]\n",
            "Saving to: ‘val2014.zip’\n",
            "\n",
            "val2014.zip         100%[===================>]   6.19G  20.5MB/s    in 5m 14s  \n",
            "\n",
            "2021-11-08 11:05:10 (20.2 MB/s) - ‘val2014.zip’ saved [6645013297/6645013297]\n",
            "\n",
            "--2021-11-08 11:07:18--  https://pjreddie.com/media/files/instances_train-val2014.zip\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 165168220 (158M) [application/zip]\n",
            "Saving to: ‘instances_train-val2014.zip’\n",
            "\n",
            "instances_train-val 100%[===================>] 157.52M  20.7MB/s    in 8.5s    \n",
            "\n",
            "2021-11-08 11:07:28 (18.5 MB/s) - ‘instances_train-val2014.zip’ saved [165168220/165168220]\n",
            "\n",
            "--2021-11-08 11:07:28--  https://pjreddie.com/media/files/coco/5k.part\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 230000 (225K) [application/octet-stream]\n",
            "Saving to: ‘5k.part’\n",
            "\n",
            "5k.part             100%[===================>] 224.61K   528KB/s    in 0.4s    \n",
            "\n",
            "2021-11-08 11:07:29 (528 KB/s) - ‘5k.part’ saved [230000/230000]\n",
            "\n",
            "--2021-11-08 11:07:29--  https://pjreddie.com/media/files/coco/trainvalno5k.part\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5722468 (5.5M) [application/octet-stream]\n",
            "Saving to: ‘trainvalno5k.part’\n",
            "\n",
            "trainvalno5k.part   100%[===================>]   5.46M  5.43MB/s    in 1.0s    \n",
            "\n",
            "2021-11-08 11:07:31 (5.43 MB/s) - ‘trainvalno5k.part’ saved [5722468/5722468]\n",
            "\n",
            "--2021-11-08 11:07:31--  https://pjreddie.com/media/files/coco/labels.tgz\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17940023 (17M) [application/octet-stream]\n",
            "Saving to: ‘labels.tgz’\n",
            "\n",
            "labels.tgz          100%[===================>]  17.11M  10.6MB/s    in 1.6s    \n",
            "\n",
            "2021-11-08 11:07:33 (10.6 MB/s) - ‘labels.tgz’ saved [17940023/17940023]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}